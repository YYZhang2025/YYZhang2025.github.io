[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Yuyang",
    "section": "",
    "text": "I am a recent graduate from Singapore Management University in MITB (AI Track). With a strong foundation in computer science and big data, I have hands-on experience in machine learning and deep learning, and Iâ€™m passionate about building impactful AI-driven solutions. I am currently seeking full-time opportunities in the AI field where I can contribute, grow, and make a meaningful impact. Open to Work â€“ feel free to reach out at zhangyuyang1211@gmail.com!"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About Yuyang",
    "section": "Education",
    "text": "Education\n\n  \n    \n    \n      2023 - 2025\n      Master of IT in Business\n      Singapore Management University\n    \n  \n  \n    \n    \n      2020 - 2023\n      Bachelor of Computer Science\n      University of Wollongong"
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "About Yuyang",
    "section": "Work Experience",
    "text": "Work Experience\n\n  \n    \n    \n      Feb 2025  -  Apr 2025\n      Research Assistant Intern\n      SMART"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "About Yuyang",
    "section": "Projects",
    "text": "Projects\n\n  \n    \n    \n       File-Tuning Large Visual Language Model\n          [GitHub]\n          [Blog]\n      \n        Wrting the"
  },
  {
    "objectID": "index.html#technical-skills",
    "href": "index.html#technical-skills",
    "title": "About Yuyang",
    "section": "Technical Skills",
    "text": "Technical Skills\n\n  \n    Python Development\n    Algorithm Design\n    Deep Learning\n    Machine Learning\n    Computer Vision\n    Natural Language Processing\n    Deep Reinforcement Learning\n    Graph Neural Networks\n    Large Language Model\n    Model Compression\n    Convex Optimization\n    Probabilistic Graph Model\n    Meta Learning\n    Deep Generative Model"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture01/lec01.html",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture01/lec01.html",
    "title": "Lecture 01: Introduction & Tokenization",
    "section": "",
    "text": "1 Introduction\nLecture 01 çš„å‰åŠæ®µä»‹ç»äº†ä»€ä¹ˆæ˜¯LLMä»¥åŠè¯¾ç¨‹çš„å¤§çº²å®‰æ’ã€‚æ€»ä½“æ¥è¯´ï¼Œè¯¾ç¨‹çš„åŸºæœ¬çš„å†…å®¹ä¼šç”±å¦‚ä¸‹çš„å‡ ä¸ªéƒ¨åˆ†ç»„æˆï¼š\n\n\n\n\n\n\nFigureÂ 1: è¯¾ç¨‹çš„5å¤§å—å†…å®¹\n\n\n\nè¿™äº›éƒ¨åˆ†ä¼šåœ¨åç»­çš„Lectureä¸­é€æ­¥å±•å¼€ä»‹ç»ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¸åšè¿‡å¤šèµ˜è¿°ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬é‡ç‚¹ä»‹ç»è¯¾ç¨‹çš„ååŠéƒ¨åˆ†å†…å®¹ï¼Œå³Tokenizationã€‚å­¦ä¹ å®Œæœ¬ç« èŠ‚åï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹å°è¯•Assignment01çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œå³BPE-Tokenizationçš„å®ç°ã€‚\n\n\n\n\n\n\nWarning\n\n\n\nä¸ªäººæ„Ÿè§‰BPEçš„éƒ¨åˆ†æ˜¯æ•´ä¸ªè¯¾ç¨‹ä¸­æœ€éš¾ç†è§£çš„éƒ¨åˆ†ï¼Œå»ºè®®å¤šèŠ±æ—¶é—´ç†è§£æ¸…æ¥šã€‚åŒæ—¶è¿™éƒ¨åˆ†ä¹Ÿå¯èƒ½æ˜¯åœ¨Assignmentä¸­èŠ±è´¹æ—¶é—´æœ€å¤šçš„éƒ¨åˆ†ï¼ˆå¯èƒ½æ˜¯å› ä¸ºæˆ‘ä¸ªäººå¯¹è¿™éƒ¨åˆ†ç†è§£ä¸å¤Ÿé€å½»çš„åŸå› ï¼‰ã€‚ æ€»ä¹‹ï¼Œé‡åˆ°å›°éš¾ä¸è¦æ°”é¦ï¼Œæ…¢æ…¢æ¥ï¼Œå¤§å®¶éƒ½æ˜¯è¿™æ ·è¿‡æ¥çš„ï¼ï¼\n\n\n\n\n2 Tokenization\nTokenizationæ˜¯Language Modelingçš„ç¬¬ä¸€æ­¥ã€‚å®ƒçš„ä½œç”¨æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£å’Œå¤„ç†çš„å½¢å¼ã€‚Tokenizationçš„è¿‡ç¨‹å°±æ˜¯å°†æ–‡æœ¬æ‹†åˆ†æˆæ›´å°çš„å•ä½ï¼ˆç§°ä¸ºtokensï¼‰ï¼Œå¹¶ä¸ºæ¯ä¸ªtokenåˆ†é…ä¸€ä¸ªå”¯ä¸€çš„æ•´æ•°IDã€‚\nTokenizationçš„è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n\n\n\n\n\n\nFigureÂ 2: Tokenizationçš„è¿‡ç¨‹ç¤ºæ„å›¾ï¼Œä¸»è¦ç”±ä¸¤ä¸ªæ­¥éª¤ç»„æˆ: encodeå’Œdecodeã€‚\n\n\n\n\n\n\n\n\n\nFigureÂ 3: Tokenization ç¤ºä¾‹. è®¿é—®è¿™ä¸ªé“¾æ¥å¯ä»¥åœ¨çº¿ä½“éªŒä¸åŒçš„tokenizerçš„æ•ˆæœã€‚\n\n\n\n\n\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ä¸åŒçš„ tokenization ä¹‹é—´çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä»¥åŠä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦ä¸åŒçš„ tokenization æ–¹æ³•ã€‚ é¦–å…ˆï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹å¸¸è§çš„å‡ ç§ tokenization æ–¹æ³•ï¼š\n\nCharacter-level Tokenization: å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå•ä¸ªå­—ç¬¦ã€‚\nWord-level Tokenization: å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå•è¯ã€‚\nSubword-level Tokenization: å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå­è¯å•å…ƒï¼ˆå¦‚BPE, WordPiece, Unigramç­‰ï¼‰ã€‚\nByte-level Tokenization: å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå­—èŠ‚å•å…ƒï¼ˆå¦‚GPT-2çš„tokenizerï¼‰ã€‚\n\næˆ‘ä»¬ç”±ä»ä¸åŒçš„åˆ†è¯çš„ä¾‹å­æ¥çœ‹ä¸€ä¸‹ä¸åŒçš„tokenizationæ–¹æ³•çš„åŒºåˆ«ï¼Œ é¦–å…ˆä»‹ç»æœ€ç®€å•çš„Character-level Tokenizationï¼š\n\n2.0.1 Character-level Tokenization\nCharacter-level Tokenizationæ˜¯å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå•ä¸ªå­—ç¬¦ã€‚ ä¾‹å¦‚ï¼Œå¥å­ â€œHello, world!â€ ä¼šè¢«æ‹†åˆ†ä¸ºä»¥ä¸‹tokensï¼š\n['H', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/LearningNotes/CS336/index.html",
    "href": "posts/LearningNotes/CS336/index.html",
    "title": "CS336: LLM from Scratch Lecture Notes and Assignments",
    "section": "",
    "text": "Note\n\n\n\n Due to the time constraint, these notes may contain errors or omissions. And to speed up the writing process, I only wrote the notes in CHINESES. If you are interested in collaborating to improve these notes,or transalting to different languages, please feel free to contact me. \n\n\n\n\n\nRelated Resources:\n\nLecture Website: CS336 LLM from Scratch\nLecture Recordings: YouTube Playlist\nMy Solution Repo: GitHub\n\n\n\nAbout this Course:\n\nThis course has 17 Lectures and 5 Assignments in total.\nIt might take around 200 hours to finish all the lectures and assignments.\n\n\n\n\n\n\nLecture Notes for CS336\n\n\n\n\n\n\n\n\n\n\nLecture 01: Introduction & Tokenization\n\n\nLecture01ä»‹ç»äº†è¯¾ç¨‹çš„å¤§çº²ä»¥åŠLLMçš„ç°çŠ¶å’ŒåŸºæœ¬æ¦‚å¿µã€‚åœ¨è¯¾ç¨‹çš„ååŠæ®µä»‹ç»äº†Language Modelingçš„ç¬¬ä¸€æ­¥ï¼Œå³Tokenizationçš„åŸºæœ¬æ¦‚å¿µå’Œå¸¸ç”¨æ–¹æ³•ã€‚ç€é‡ä»‹ç»äº†Byte Pair Encoding (BPE)ç®—æ³•çš„åŸç†å’Œå®ç°ã€‚åœ¨å­¦ä¹ å®ŒLecture 01åï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹å°è¯•Assignment 01çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œå³BPE-Tokenizationçš„å®ç°\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 02: PyTorch Basics & Resource Accounts\n\n\nLecture02 ä»‹ç»äº†PyTorchçš„åŸºæœ¬æ¦‚å¿µå’Œä½¿ç”¨æ–¹æ³•ã€‚å¯ä»¥å°†è¿™èŠ‚è¯¾å½“ä½œä¸€ä¸ªreviewï¼Œå¤ä¹ ä¸€ä¸‹ä¹‹å‰å­¦è¿‡çš„PyTorchçŸ¥è¯†ç‚¹ã€‚åŒæ—¶ï¼Œè¯¾ç¨‹ä¸­ä»‹ç»äº†ä¸€ä¸ª einops çš„åº“ï¼Œå¯ä»¥ç®€åŒ–å¼ é‡æ“ä½œçš„ä»£ç ç¼–å†™ã€‚è¿™èŠ‚è¯¾ä¹Ÿä»‹ç»äº†ä¸åŒæ•°æ®ç±»å‹ï¼ˆå¦‚FP32, FP16, BF16ç­‰ï¼‰åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨å’Œä¼˜ç¼ºç‚¹ã€‚å¹¶ä¸”é€šè¿‡è®¡ç®—è¿™äº›æ•°æ®ç±»å‹åœ¨å†…å­˜ä¸­çš„å ç”¨ï¼Œå¸®åŠ©æˆ‘ä»¬ç†è§£ä¸ºä»€ä¹ˆæœ‰äº›æ•°æ®ç±»å‹æ›´é€‚åˆåœ¨æœ‰é™èµ„æºä¸‹è¿›è¡Œè®­ç»ƒ, å¹¶ä¸”åœ¨ä»€ä¹ˆæƒ…å†µä¸‹éœ€è¦åº”ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆMixed Precision Trainingï¼‰ï¼Œä»€ä¹ˆæƒ…å†µä¸‹éœ€è¦ç”¨é«˜ç²¾åº¦çš„æ•°æ®ç±»å‹ã€‚åœ¨è¯¾ç¨‹çš„æœ€åï¼Œè¿˜ä»‹ç»äº†ä¸åŒçš„Optimizerï¼ˆå¦‚SGD, Adamç­‰ï¼‰çš„åŸºæœ¬åŸç†å’Œä½¿ç”¨åœºæ™¯\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 03: LM Model Architecture & Hyperparameters\n\n\nLecture03 ä»‹ç»äº†ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒæ¶æ„ä¸è¶…å‚æ•°è®¾è®¡ã€‚è¯¾ç¨‹å¯¹æ¯”äº†åŸå§‹ Transformer ä¸ä¸»æµ LLaMA-like æ¶æ„ï¼Œæ€»ç»“äº† pre-normã€RMSNormã€SwiGLUã€RoPE ç­‰å…³é”®è®¾è®¡çš„ç»éªŒå…±è¯†ï¼Œå¹¶ç»“åˆå¤§é‡è¿‘æœŸæ¨¡å‹å®è·µï¼Œè®²è§£äº† MLP å®½åº¦æ¯”ä¾‹ã€æ³¨æ„åŠ›å¤´é…ç½®ã€æ¨¡å‹æ·±å®½æ¯”ä¸è¯è¡¨è§„æ¨¡ç­‰è¶…å‚æ•°é€‰æ‹©åŸåˆ™ã€‚åŒæ—¶è¿˜ä»‹ç»äº† z-lossã€QK-Normã€MQA/GQA ç­‰ç”¨äºæå‡è®­ç»ƒç¨³å®šæ€§å’Œæ¨ç†æ•ˆç‡çš„å…³é”®æŠ€å·§ã€‚\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\nNo matching items\n\n\n\nAssignments\n\n\n\n\n\n\n\n\n\n\nAssignment 01: Tokenization & Language Modeling\n\n\nAssignment 01 è¦æ±‚æˆ‘ä»¬å®ç°ä¸€ä¸ªç®€å•çš„è¯­è¨€æ¨¡å‹è®­ç»ƒæµç¨‹ï¼Œæ¶µç›–æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹å®šä¹‰ã€è®­ç»ƒå’Œè¯„ä¼°ç­‰æ­¥éª¤ã€‚é€šè¿‡è¿™ä¸ªä½œä¸šï¼Œæˆ‘ä»¬å°†å·©å›ºå¯¹è‡ªç„¶è¯­è¨€å¤„ç†åŸºç¡€æ¦‚å¿µçš„ç†è§£ï¼Œå¹¶æŒæ¡ä½¿ç”¨PyTorchè¿›è¡Œæ·±åº¦å­¦ä¹ æ¨¡å‹å¼€å‘çš„åŸºæœ¬æŠ€èƒ½\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/index.html",
    "href": "posts/LearningNotes/LLM-Series/index.html",
    "title": "LLM Model Series Learning Notes",
    "section": "",
    "text": "Qwen Model Series\n\n\nåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢ä¸€ç³»åˆ—çš„Qwenæ¨¡å‹ï¼Œæ²¿ç€Qwenæ¨¡å‹çš„å‘å±•æ—¶ï¼Œæ¥çœ‹çœ‹ä¸åŒæ—¶æœŸçš„Qwenæ¨¡å‹è¿ç”¨äº†æ€ä¹ˆæ ·çš„ä¸åŒçš„æŠ€æœ¯\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "posts/LearningNotes/DLFaC/Chapter01/chatper01.html",
    "href": "posts/LearningNotes/DLFaC/Chapter01/chatper01.html",
    "title": "Chapter 01: Introduction",
    "section": "",
    "text": "THis is the Chatper 01\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html",
    "title": "01: Attention is all you need (Transformer)",
    "section": "",
    "text": "1 Preliminary\n  \n  1.1 Softmax Function\n  1.2 Vector Similarity\n  1.3 Word Embedding Layer\n  1.4 Position Embedding Layer\n  1.5 Attention Layer\n  \n  1.5.1 Self-Attention Layer\n  1.5.2 Cross Attention Layer\n  \n  1.6 Normalization Layer\n  1.7 Feed Forward Layer\n  1.8 Residual Connection\n  1.9 Output Layer\n  1.10 Others\n  1.11 Experiment\n  1.12 Weight Initialization\n  \n  1.12.1 Optimizer\n  \n  \n  2 Summary\n  3 Key Concepts\n  4 Q & A\n  5 Related resource & Further Reading\næˆ‘ä»¬å¼€å§‹ç¬¬ä¸€ç¯‡è®ºæ–‡çš„å­¦ä¹ ï¼š ã€ŠAttention is All You Needã€‹ (Vaswani et al. 2023)ï¼Œä¹Ÿå°±æ˜¯ä¼ è¯´ä¸­çš„Transformeræ¨¡å‹ã€‚Transformeræ¨¡å‹çš„æå‡ºï¼Œå½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»¥åŠæ›´å¹¿æ³›çš„é¢†åŸŸã€‚è¯¥æ¶æ„å®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶(Attention)ï¼Œä¸å†ä¾èµ–å¾ªç¯ï¼ˆRNNï¼‰æˆ–å·ç§¯ï¼ˆCNNï¼‰ï¼Œå› æ­¤åœ¨è®­ç»ƒæ—¶æ›´æ˜“å¹¶è¡ŒåŒ–ã€æ•ˆç‡æ›´é«˜ã€‚Transformer å·²æˆä¸ºä¼—å¤šå‰æ²¿æ¨¡å‹çš„åŸºç¡€ï¼Œä¸ä»…åœ¨ NLP ä¸­è¡¨ç°çªå‡ºï¼Œä¹Ÿæ‰©å±•åˆ°è®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸã€‚æ¯”å¦‚ ChatGPTã€DeepSeek ç­‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éƒ½ä»¥ Transformer ä¸ºæ ¸å¿ƒæ¶æ„ã€‚æ‰€ä»¥æˆ‘ä»¬è‡ªç„¶å°±æŠŠå®ƒå½“ä½œæˆ‘ä»¬ç¬¬ä¸€ç¯‡æ–‡ç« çš„é¦–é€‰ã€‚"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#softmax-function",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#softmax-function",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.1 Softmax Function",
    "text": "1.1 Softmax Function"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#vector-similarity",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#vector-similarity",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.2 Vector Similarity",
    "text": "1.2 Vector Similarity\nç®€å•å›é¡¾äº†ä¸€ä¸‹è¿™äº›æ•°å­¦çŸ¥è¯†ï¼Œæ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹Transformeråˆ°åº•æ˜¯ä¸ªä»€ä¹ˆä¸œè¥¿ã€‚ # Transformer Transformer æ˜¯ Google åœ¨2017å¹´æå‡ºçš„æ–°çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå®ƒçš„æå‡ºä¸»è¦æ˜¯ä¸ºäº†è§£å†³ï¼Œ\n\nåºåˆ—å»ºæ¨¡(Sequence Modeling)çš„æ•ˆç‡é—®é¢˜:\n\nåœ¨ Transformer å‡ºç°ä¹‹å‰ï¼Œä¸»æµæ–¹æ³•æ˜¯ RNNï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œï¼‰å’Œ CNNï¼ˆå·ç§¯ç¥ç»ç½‘ç»œï¼‰ã€‚\nRNN éœ€è¦æŒ‰é¡ºåºé€æ­¥å¤„ç†åºåˆ—ï¼Œæ— æ³•å¹¶è¡ŒåŒ–ï¼Œè®­ç»ƒå’Œæ¨ç†æ•ˆç‡ä½ä¸‹ã€‚\nCNN è™½ç„¶æœ‰ä¸€å®šçš„å¹¶è¡Œæ€§ï¼Œä½†æ•æ‰é•¿è·ç¦»ä¾èµ–éœ€è¦å †å å¾ˆå¤šå±‚ï¼Œè®¡ç®—å¼€é”€å¤§ã€‚\n\né•¿è·ç¦»ä¾èµ–å»ºæ¨¡é—®é¢˜:\n\nRNN åœ¨æ•æ‰é•¿è·ç¦»ä¾èµ–æ—¶å®¹æ˜“å‡ºç°æ¢¯åº¦æ¶ˆå¤±(Gradient Vanish) æˆ–æ¢¯åº¦çˆ†ç‚¸(Gradient Explosion)ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥å­¦ä¹ è¿œè·ç¦»çš„ä¿¡æ¯ã€‚\n\n\n\nThis inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. â€¦ In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions  Attention is all you need, p.Â \n\næ¨¡å‹çš„åŸºæœ¬æ¶æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n\nç”¨ä»£ç æ˜¾ç¤ºå°±æ˜¯:\nclass Transformer(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.encoder = Encoder(config)\n        self.decoder = Decoder(config)\n        self.output_layer = nn.Linear(config.d_model, config.tgt_vocab_size)\n        ...\n    \n    def forward(self, original, target):\n        ...\næ¥ä¸‹æ¥è®©æˆ‘ä»¬ä»ä¸‹è‡³ä¸Šï¼Œæ¥æ·±åº¦è§£åˆ¨Transformerçš„æ¨¡å‹ç»“æ„åˆ†åˆ«æ˜¯: 1. Word Embedding Layer 2. Position Embedding Layer 3. Attention Layer: 1. Self-Attention Layer 2. Cross-Attention Layer 4. Normalization Layer 5. Feed Forward Layer 6. Output Layer"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#word-embedding-layer",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#word-embedding-layer",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.3 Word Embedding Layer",
    "text": "1.3 Word Embedding Layer\nWord Embedding åŸºæœ¬æ˜¯æ‰€æœ‰è¯­è¨€æ¨¡å‹çš„ç¬¬ä¸€æ­¥ï¼Œ"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#position-embedding-layer",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#position-embedding-layer",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.4 Position Embedding Layer",
    "text": "1.4 Position Embedding Layer\n\\[\n\\begin{split}\nPE_{(pos, 2i)} & = \\sin (pos / 10,000^{2i / d_{model}}) \\\\\nPE_{(pos, 2i+1)} & = \\cos (pos / 10,000^{2i+1 / d_{model}})\n\\end{split}\n\\]"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#attention-layer",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#attention-layer",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.5 Attention Layer",
    "text": "1.5 Attention Layer\n\n1.5.1 Self-Attention Layer\n\n\n1.5.2 Cross Attention Layer\n\\[\n\\begin{array}{|l|l|}\n\\hline\n\\textbf{Step} & \\textbf{Time Complexity} \\\\\n\\hline\nQK^\\top & \\mathcal{O}(n^2 d) \\\\\n\\text{softmax}(QK^\\top) & \\mathcal{O}(n^2) \\\\\n\\text{attention} \\times V & \\mathcal{O}(n^2 d) \\\\\n\\hline\n\\textbf{Total} & \\mathcal{O}(n^2 d) \\\\\n\\hline\n\\end{array}\n\\]"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#normalization-layer",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#normalization-layer",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.6 Normalization Layer",
    "text": "1.6 Normalization Layer\nLayer Normalization"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#feed-forward-layer",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#feed-forward-layer",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.7 Feed Forward Layer",
    "text": "1.7 Feed Forward Layer\n\\[\n\\text{FFN}(\\mathrm{x}) = \\underset{}{\\max} (0, \\mathrm{x} W_{1} + b_{1}) W_{2} + b_{2}\n\\]"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#residual-connection",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#residual-connection",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.8 Residual Connection",
    "text": "1.8 Residual Connection\n\\[\n\\mathbf{y} = \\text{LayerNorm}(\\mathbf{x} + \\mathrm{Sublayer}(\\mathbf{x}))\n\\]\n\\[\n\\begin{split}\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}}\n&= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\cdot \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\cdot \\left( \\mathbf{I} + \\frac{\\partial \\mathrm{Sublayer}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right) \\\\\n&= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}}}_{\\text{straight path}} +\n\\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\cdot\n\\frac{\\partial\\,\\mathrm{Sublayer}(\\mathbf{x})}{\\partial \\mathbf{x}}}_{\\text{through the sub-layer}}\n\\end{split}\n\\]"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#output-layer",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#output-layer",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.9 Output Layer",
    "text": "1.9 Output Layer"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#others",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#others",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.10 Others",
    "text": "1.10 Others\nå½“ç„¶ï¼Œé™¤äº†ä»¥ä¸Šçš„ä¸€ä¸ªéƒ¨åˆ†ï¼ŒTransformerä¸­è¿˜æœ‰å‡ ä¸ªå€¼å¾—ä¸€æçš„éƒ¨åˆ†ï¼Œä¸è¿‡å¤„äºç¯‡å¹…çš„å…³ç³»ï¼Œåœ¨è¿™é‡Œå°±ä¸è¿‡å¤šçš„èµ˜è¿°äº†ï¼Œå…¶ä¸­åŒ…æ‹¬Tokenizationç­‰ã€‚"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#experiment",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#experiment",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.11 Experiment",
    "text": "1.11 Experiment"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#weight-initialization",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#weight-initialization",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.12 Weight Initialization",
    "text": "1.12 Weight Initialization\n\n1.12.1 Optimizer\nTransformerçš„è®ºæ–‡ä¸­ï¼Œç”¨çš„æ˜¯Adam Optimizer,\n\n\n\n\n\n\nNote\n\n\n\nå¯¹äºä¸äº†çš„Adamçš„åŒå­¦ï¼Œä¹Ÿä¸ç”¨å¤ªæ‹…å¿ƒï¼Œä¹‹åæˆ‘ä»¬ä¼šæœ‰ä¸€ç³»åˆ—çš„æ–‡ç« ï¼Œä¸“é—¨ä»‹ç»è¿™äº›ä¼˜åŒ–å™¨çš„ï¼ŒåŒ…æ‹¬Adamï¼ŒAdamWï¼Œä»¥åŠæœ€è¿‘æ¯”è¾ƒç«çš„Moun"
  },
  {
    "objectID": "posts/PapersWithCode/08-dit/DiT.html",
    "href": "posts/PapersWithCode/08-dit/DiT.html",
    "title": "08: Scalable Diffusion Models with Transformers (DiT)",
    "section": "",
    "text": "On this page\n   \n  \n  1 Preliminary\n  2 DiT\n  3 Summary\n  4 Key Concepts\n  5 Q & A\n  6 Related resource & Further Reading\n  \n\n\n1 Preliminary\n\n\n2 DiT\n\n\n3 Summary\n\n\n4 Key Concepts\n\n\n5 Q & A\n\n\n6 Related resource & Further Reading\n\nCode Reference: Meta DiT\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/PapersWithCode/04-clip/CLIP.html",
    "href": "posts/PapersWithCode/04-clip/CLIP.html",
    "title": "04: Learning Transferable Visual Models From Natural Language Supervision(CLIP)",
    "section": "",
    "text": "On this page\n   \n  \n  1 CLIP\n  2 Key Concepts\n  3 Q & A\n  4 æ‰©å±•\n  \n\n\n1 CLIP\n\nTraining procedure Preprocessing The exact details of preprocessing of images during training/validation can be found here.\n\nImages are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\n # Summary\n\n\n2 Key Concepts\n\n\n3 Q & A\n\n\n4 æ‰©å±•\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/PapersWithCode/07-ddpm/DDPM.html",
    "href": "posts/PapersWithCode/07-ddpm/DDPM.html",
    "title": "07: Denoising Diffusion Probabilistic Models(DDPM)",
    "section": "",
    "text": "On this page\n   \n  \n  1 Preliminary\n  2 DDPM\n  3 Summary\n  4 Key Concepts\n  5 Q & A\n  6 Related resource & Further Reading\n  \n\n\n1 Preliminary\n\n\n2 DDPM\n\n\n3 Summary\n\n\n4 Key Concepts\n\n\n5 Q & A\n\n\n6 Related resource & Further Reading\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/PapersWithCode/100_Papers_index.html",
    "href": "posts/PapersWithCode/100_Papers_index.html",
    "title": "100 Papers with Code",
    "section": "",
    "text": "Note\n\n\n\n Due to the large number of papers included in this series. I will mainly write in Chinese to speed up the writing process. If you have any questions or suggestions, please feel free to contact me. Or if you would like to contribute translations of any of the articles into English or other languages, please let me know! \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01: Attention is all you need (Transformer)\n\n\n\nTransformer\n\nAttention\n\nNLP\n\nArchitecture\n\n\n\nTransformer æ˜¯ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œèƒ½å¤Ÿå¹¶è¡Œå¤„ç†åºåˆ—ï¼Œåœ¨è¯­è¨€ã€è§†è§‰å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”ä½œä¸º GPTã€BERT ç­‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ¸å¿ƒåŸºç¡€ï¼Œæ¨åŠ¨äº†å½“ä»Šç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å¿«é€Ÿå‘å±•ã€‚\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)\n\n\n\nComputer-Vision\n\n\n\nSwin Transformer æ˜¯ä¸€ç§ä½¿ç”¨å±‚æ¬¡åŒ–ç»“æ„å’Œæ»‘åŠ¨çª—å£è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ViTæ¨¡å‹ï¼Œæ—¢ä¿ç•™äº†å±€éƒ¨å»ºæ¨¡çš„é«˜æ•ˆæ€§ï¼Œåˆé€šè¿‡çª—å£åç§»å®ç°è·¨åŒºåŸŸä¿¡æ¯äº¤äº’ï¼Œå¯ä½œä¸ºé€šç”¨è§†è§‰éª¨å¹²ç½‘ç»œï¼Œé€‚ç”¨äºå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ç­‰å¤šç§è§†è§‰ä»»åŠ¡ã€‚\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n04: Learning Transferable Visual Models From Natural Language Supervision(CLIP)\n\n\n\nMulti-Modality\n\nMustReadPaper\n\n\n\nCLIPï¼ˆå¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒæ¨¡å‹ï¼‰æ˜¯ ä¸€ä¸ªè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡åœ¨æ•°äº¿å¯¹å›¾æ–‡æ•°æ®ä¸Šè¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œå­¦å¾—è”åˆè¡¨å¾ï¼Œä»è€Œèƒ½å¤Ÿåœ¨é›¶æ ·æœ¬æ¡ä»¶ä¸‹å®Œæˆå¤šç§è§†è§‰ä»»åŠ¡ã€‚\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n05: Emerging Properties in Self-Supervised Vision Transformers(DINO)\n\n\n\nSelf-Supervised-Learning\n\nRepresentation-Learning\n\n\n\nThis is the DINO article\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n06: Auto-Encoding Variational Bayes(VAE)\n\n\n\nSelf-Supervised-Learning\n\nGenerative-Model\n\nRepresentation-Learning\n\n\n\nVAEï¼ˆå˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼‰æ˜¯ä¸€ç±»ç»“åˆæ¦‚ç‡å›¾æ¨¡å‹ä¸ç¥ç»ç½‘ç»œçš„ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒé€šè¿‡å¼•å…¥å¯å‚æ•°åŒ–çš„è¿‘ä¼¼åéªŒ \\(q_\\phi(z|x)\\) æ¥æ‘Šé”€æ¨æ–­æˆæœ¬ï¼Œå¹¶ç”¨æœ€å¤§åŒ– ELBO çš„æ–¹å¼åŒæ—¶å­¦ä¹ æ•°æ®çš„æ½œåœ¨è¡¨ç¤ºä¸ç”Ÿæˆè¿‡ç¨‹ï¼šå…¶ä¸­é‡å»ºé¡¹ç¡®ä¿æ¨¡å‹èƒ½ä»æ½œå˜é‡è¿˜åŸæ•°æ®ï¼ŒKL é¡¹åˆ™å°†æ½œç©ºé—´çº¦æŸä¸ºæ¥è¿‘å…ˆéªŒçš„è¿ç»­ç»“æ„ã€‚å€ŸåŠ©é‡å‚æ•°åŒ–æŠ€å·§ï¼ŒVAE èƒ½åœ¨ç«¯åˆ°ç«¯è®­ç»ƒä¸­é«˜æ•ˆåœ°å­¦ä¹ ä¸€ä¸ªå¹³æ»‘ã€å¯é‡‡æ ·çš„æ½œç©ºé—´ï¼Œä»è€Œå®ç°è¡¨ç¤ºå­¦ä¹ ã€æ’å€¼ã€ç”Ÿæˆç­‰åŠŸèƒ½ï¼Œæ˜¯ç°ä»£æ·±åº¦ç”Ÿæˆæ¨¡å‹çš„é‡è¦åŸºç¡€ã€‚\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n07: Denoising Diffusion Probabilistic Models(DDPM)\n\n\nA comprehensive overview of Denoising Diffusion Probabilistic Models (DDPM) and their applications in generative modeling.\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n08: Scalable Diffusion Models with Transformers (DiT)\n\n\nA comprehensive overview of the DiT model architecture and its applications in scalable diffusion processes.\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nVision-Transformer\n\n\n\nComputer Vision\n\nTransformer\n\n\n\nVision Transformer (ViT) é€šè¿‡å°†å›¾åƒåˆ‡åˆ†ä¸º patch å¹¶ç›´æ¥åº”ç”¨æ ‡å‡† Transformerï¼Œå®ç°äº†åœ¨å¤§è§„æ¨¡æ•°æ®ä¸‹è¶…è¶Š CNN çš„å›¾åƒåˆ†ç±»æ€§èƒ½ã€‚\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nVQ-VAE\n\n\n\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "posts/Projects/PaliGemma/pali-gemma.html",
    "href": "posts/Projects/PaliGemma/pali-gemma.html",
    "title": "PaliGemma Inference and Fine Tuning",
    "section": "",
    "text": "Here is the full process of the Pali-Gemma\n\n\n\n\n\n\n\nFigureÂ 1: The Pali-Gemma model is a multi-modal large language model that integrates vision and language tasks. The full process includes data collection, model training, and fine-tuning for specific applications.\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Projects/Qwen3-VL/Qwen3-VL.html",
    "href": "posts/Projects/Qwen3-VL/Qwen3-VL.html",
    "title": "Qwen3-VL Inference",
    "section": "",
    "text": "1 Qwen3-VL Architecture\n  \n  1.1 Image Input Processing\n  1.2 Vision Encoder\n  \n  1.2.1 2D RoPE\n  1.2.2 Position Interpolation\n  1.2.3 Patch Merger\n  \n  1.3 Vision Language Fusion\n  \n  1.3.1 DeepStack\n  \n  \n  2 Related Resources"
  },
  {
    "objectID": "posts/Projects/Qwen3-VL/Qwen3-VL.html#image-input-processing",
    "href": "posts/Projects/Qwen3-VL/Qwen3-VL.html#image-input-processing",
    "title": "Qwen3-VL Inference",
    "section": "1.1 Image Input Processing",
    "text": "1.1 Image Input Processing"
  },
  {
    "objectID": "posts/Projects/Qwen3-VL/Qwen3-VL.html#vision-encoder",
    "href": "posts/Projects/Qwen3-VL/Qwen3-VL.html#vision-encoder",
    "title": "Qwen3-VL Inference",
    "section": "1.2 Vision Encoder",
    "text": "1.2 Vision Encoder\n\nWe utilize the SigLIP-2 architecture as our vision encoder and continue training it with dynamic input resolutions, initialized from official pretrained checkpoints. To accommodate dynamic resolutions effectively, we employ 2D-RoPE and interpolate absolute position embeddings based on input size, following the methodology of CoMP.  Qwen3-VL Technical Report (Bai et al. 2025), P3\n\nThe vision encoder is the SigLip-2 architecture (Tschannen et al. 2025). As mentioned in the Qwen3-VL technical report (Bai et al. 2025), the weight is loaded from the SigLip-2 pretrained checkpoint, but with the continuous training to fit the dynamic input resolutions. One of the key techniques is change the position encoding. It use:\n\n2D-RoPE (Rotary Position Embedding)(Su et al. 2023) to replace the original position encoding in ViT.\nInterpolate the absolute position embeddings based on input size.\n\n\n1.2.1 2D RoPE\nThe 2D RoPe is an extension of the original RoPE (Su et al. 2023) to 2D inputs. The original RoPE is designed for 1D sequences, such as text. It encodes the position information by rotating the query and key vectors in the self-attention mechanism. The 2D RoPE extends this idea to 2D inputs, such as images. It encodes the position information by rotating the query and key vectors in both height and width dimensions. The mathematical formulation of 2D RoPE is as follows:\n\\[\n\\text{RoPE}_{2D}(Q, K, pos_h, pos_w) = Q \\cdot R(pos_h) \\cdot R(pos_w), K \\cdot R(pos_h) \\cdot R(pos_w)\n\\tag{1}\\]\nwhere \\(R(pos_h)\\) and \\(R(pos_w)\\) are the rotation matrices for height and width positions, respectively. By applying 2D RoPE, the model can effectively capture the spatial relationships in images, which is crucial for vision tasks.\n\n\n1.2.2 Position Interpolation\nIn addition to 2D RoPE, the Qwen3-VL model also uses position interpolation to handle dynamic input resolutions. The original ViT model uses absolute position embeddings, which are fixed for a specific input size. To accommodate dynamic input sizes, the Qwen3-VL model interpolates the absolute position embeddings based on the input size. The interpolation is done using bilinear interpolation, which allows the model to adapt to different input sizes without losing the positional information. The mathematical formulation of position interpolation is as follows:\n\\[\nPE_{interp}(x, y) = \\text{BilinearInterpolate}(PE, x, y)\n\\tag{2}\\]\nwhere \\(PE\\) is the original position embedding matrix, and \\((x, y)\\) are the coordinates in the interpolated space. By using position interpolation, the model can effectively handle images of varying sizes while maintaining the positional information necessary for accurate vision tasks.\n\n\n\n\n\n\nFigureÂ 2: The illustration of CoMP\n\n\n\n\\[\n\\mathbf{R}_{x,y} =\n\\left(\\begin{array}{cc:cc}\n    \\cos x\\theta & -\\sin x\\theta & 0 & 0 \\\\\n    \\sin x\\theta & \\cos x\\theta & 0 & 0 \\\\\n    \\hdashline\n    0 & 0 & \\cos y\\theta & -\\sin y\\theta \\\\\n    0 & 0 & \\sin y\\theta & \\cos y\\theta\n\\end{array} \\right)\n\\tag{3}\\]\nwhere \\(x\\) and \\(y\\) are the position indices in height and width dimensions, respectively, and \\(\\theta\\) is a predefined angle.\n\n\n1.2.3 Patch Merger\n\n\n\n\n\n\nFigureÂ 3: The function of Patch Merger. patch merging process, a down-sampling technique used in transformer architectures like the Swin Transformer. This operation reduces the spatial dimensions of the feature map while increasing the channel dimension.\n\n\n\nHowever, unlike other patch merger such as the one in Swin Transformer (Liu et al. 2021), the Patch Merger in Qwen3-VL is implemented as a linear layer that projects the concatenated patch embeddings into a lower-dimensional space. The adjcant patch embeddings are processed during the pre-processing step, and the Patch Merger layer simply reduces the dimensionality of the concatenated embeddings. Wewill see more"
  },
  {
    "objectID": "posts/Projects/Qwen3-VL/Qwen3-VL.html#vision-language-fusion",
    "href": "posts/Projects/Qwen3-VL/Qwen3-VL.html#vision-language-fusion",
    "title": "Qwen3-VL Inference",
    "section": "1.3 Vision Language Fusion",
    "text": "1.3 Vision Language Fusion\n\n1.3.1 DeepStack\n\n\n\n\n\n\nFigureÂ 4: The architecture of DeepStack\n\n\n\n(Meng et al. 2024)\nThe DeepStack module is designed to effectively merge visual and textual information. It consists of multiple layers of cross-attention and feed-forward networks. The cross-attention mechanism allows the model to attend to relevant visual features based on the textual input, enabling a more comprehensive understanding of the multi-modal data. The feed-forward networks further process the combined features to enhance their representation."
  },
  {
    "objectID": "posts/Blogs/blogs_index.html",
    "href": "posts/Blogs/blogs_index.html",
    "title": "ğŸ‘‹ğŸ»Welcome to Yuyangâ€™s Blog",
    "section": "",
    "text": "From Entropy to KL Divergence: A Comprehensive Guide\n\n\n\nMathematics\n\n\n\nKL Divergence, also known as Kullback-Leibler Divergence, is a fundamental concept in information theory and statistics. In this blog post, we will explore the concept of KL Divergence, its mathematical formulation, and its applications in various fields such as machine learning, data science, and artificial intelligence. By the end of this post, you will have a solid understanding of KL Divergence and how to apply it in your own projects.\n\n\n\n\n\n2025-12-02\n\n\nYuyang Zhang\n\n5 min\n\n951 words\n\n\n\n\n\n\n\nAll kinds of â€œLearningsâ€ in Deep Learning\n\n\n\nDeep Learning\n\nSummary\n\n\n\nIn the blog, I will cover all kind of learnings in Deep Learning, including supervised learning, unsupervised learning, self-supervised learning, reinforcement learning, and more. By the end, you will have a solid understanding of different learning paradigms in Deep Learning.\n\n\n\n\n\n2025-12-02\n\n\nYuyang Zhang\n\n14 min\n\n2,766 words\n\n\n\n\n\n\n\nThe Evolution of Position Encoding in the Transformer\n\n\n\nTransformer\n\nLarge Language Model\n\n\n\nIn the blog, I will go through different position encoding machnism used in the Transformer-based neural network architecture. We will explore why the position encoding is important and different types of position encoding. In the end, we will extend the position encoding to visual field, such as in the Image and Video to unify the position encoding.\n\n\n\n\n\n2025-11-25\n\n\nYuyang Zhang\n\n16 min\n\n3,174 words\n\n\n\n\n\n\n\nAll About Diffusion & Flow Models\n\n\n\nGenerative-Model\n\nDiffusion-Model\n\n\n\nThis article offers a comprehensive overview of diffusion models from multiple perspectives. We begin with the foundationsâ€”DDPM, DDIM, and Score Matchingâ€”and explore their relationships. From there, we introduce the ODE/SDE framework, showing how DDPM can be derived from stochastic differential equations and how this connects to Flow Matching. We then highlight key model variants such as Stable Diffusion and Movie Gen, discussing their architectures and applications. Finally, we broaden the scope to examine how diffusion models are being adapted beyond image generation, including diffusion policies in reinforcement learning and their emerging role in large language models (LLMs).\n\n\n\n\n\n2025-10-15\n\n\nYuyang Zhang\n\n26 min\n\n5,001 words\n\n\n\n\n\n\n\nLLM Part3: Alignment\n\n\n\nTransformer\n\nLarge Language Model\n\n\n\nIn this blog, we will going through different alignment techniques for LLMs, including how to effectively align them with human values and intentions. We will explore techniques such as reinforcement learning from human feedback (RLHF), and more. By the end, you will have a solid understanding of how to align LLMs for your own applications.\n\n\n\n\n\n2025-10-10\n\n\nYuyang Zhang\n\n2 min\n\n380 words\n\n\n\n\n\n\n\nLLM Part2: Inference\n\n\n\nLarge Language Model\n\nInference\n\n\n\nIn this blog, we will going through the inference process of LLMs, including how to effectively use them for various tasks. We will explore techniques such as prompt engineering, few-shot learning, and more. By the end, you will have a solid understanding of how to leverage LLMs for your own applications.\n\n\n\n\n\n2025-10-01\n\n\nYuyang Zhang\n\n1 min\n\n88 words\n\n\n\n\n\n\n\nLLM Part1: Architecture\n\n\n\nTransformer\n\nLarge Language Model\n\n\n\nIn this blog, we will going through different transformer architectures that power LLMs. Such as position encoding, attention mechanisms, and more. In the end, we will explore some SOTA LLM architectures, and study how to they combine those techniques to achieve better performance.\n\n\n\n\n\n2025-09-25\n\n\nYuyang Zhang\n\n28 min\n\n5,428 words\n\n\n\n\n\n\n\nSpeed Up Training for Neural Networks\n\n\n\nTraining Tricks\n\n\n\nTraining large neural networks can be time-consuming and resource-intensive. This blog post explores various techniques to speed up the training process, including optimizing calculating in the single GPUs, and parallelizing the training across multiple GPUs.\n\n\n\n\n\n2025-09-10\n\n\nYuyang Zhang\n\n14 min\n\n2,648 words\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html",
    "title": "LLM Part1: Architecture",
    "section": "",
    "text": "1 The overview of transformer model\n  2 Position Encoding\n  \n  2.1 Learned Position Encoding\n  2.2 Absolute Position Encoding\n  2.3 Relative Position Encoding\n  2.4 RoPE (Rotary Position Embedding)\n  2.5 ALIBI\n  2.6 Extend to longer context\n  \n  2.6.1 Linear Position Interpolation\n  2.6.2 NTK-Aware Position Interpolation\n  2.6.3 YaRN\n  \n  \n  3 Normalization\n  \n  3.1 Layer Normalization vs.Â RMS Normalization\n  3.2 Pre-Layer Normalization vs.Â Post-Layer Normalization\n  3.3 QK Norm\n  \n  4 Attention Mechanism\n  \n  4.1 Multi Headed Attention\n  \n  4.1.1 Time Complexity of Scaled Dot-Product Attention\n  \n  4.2 Grouped Query Attention / Multi Query Attention\n  4.3 Sparse Attention\n  \n  4.3.1 Sliding window attention\n  4.3.2 Dilated Attention\n  \n  4.4 Multi Latent Attention\n  4.5 Flash Attention\n  \n  4.5.1 Flash Attention V1 vs.Â V2 vs.Â V3\n  \n  4.6 Native Sparse Attention\n  4.7 Attention Sink\n  \n  5 Activations\n  \n  5.1 Swish\n  5.2 Gated Linear Unit (GLU)\n  \n  6 Feed Forward Network & Mixture of Experts\n  \n  6.1 Multi Layer Perceptron (MLP)\n  6.2 Gated Linear Unit (GLU)\n  6.3 Mixture of Experts (MoE)\n  \n  7 Model Initialization\n  \n  7.1 Weight Initialization\n  7.2 Layer Initialization\n  \n  8 Case Study\n  \n  8.1 LLaMA\n  8.2 Qwen\n  8.3 DeepSeek\n  8.4 GPT-Oss\n  \n  9 Other Architectures\n  \n  9.1 Diffusion Language Models\n  9.2 State Space Model (SSM)\n  \n  10 Conclusion\nThis is the part 1 of the LLM series, architecture. In this blog, we will going through different transformer architectures that power LLMs. Such as position encoding, attention mechanisms, and more. In the end, we will explore some SOTA LLM architectures, and study how to they combine those techniques to achieve better performance. We are going from bottom to up:\nBesides that, we will also explore different normalization techniques, such as Layer Normalization and RMS Normalization, and the different position of the normalization layers within the architecture. Then, we will going through the training process and how to effectively train these architectures."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#learned-position-encoding",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#learned-position-encoding",
    "title": "LLM Part1: Architecture",
    "section": "2.1 Learned Position Encoding",
    "text": "2.1 Learned Position Encoding\nIn the absolute position encoding, we put our position information into fixed sinusoidal functions. It is hand-crafted for specific tasks and does not adapt to the data. So, is it possible to learn position encodings from the data itself? It is possible, and this leads us to the concept of learned position encodings. The learned position encodings are typically implemented as additional trainable parameters in the model. Instead of using fixed sinusoidal functions, the model learns to generate position embeddings that are optimized for the specific task and dataset.\nThis method is used in such as Vision Transformers (Dosovitskiy et al. 2021)."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#absolute-position-encoding",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#absolute-position-encoding",
    "title": "LLM Part1: Architecture",
    "section": "2.2 Absolute Position Encoding",
    "text": "2.2 Absolute Position Encoding\nAs used in the (Vaswani et al. 2023), absolute position encoding assigns a unique position embedding to each position in the input sequence, regardless of the content of the tokens. One common approach is to use a fixed sinusoidal function to generate the position embeddings. For example: for each position \\(pos\\), the position embedding \\(PE(pos)\\) can be defined as:\n\\[\n\\begin{aligned}\n\\text{PE}(pos, 2i) &= \\sin\\!\\left(pos \\times \\frac{1}{10,000^{2i/d_{\\text{model}}}}\\right) \\\\\n\\text{PE}(pos, 2i+1) &= \\cos\\!\\left(pos \\times \\frac{1}{10,000^{2i/d_{\\text{model}}}}\\right)\n\\end{aligned}\n\\tag{1}\\]\nwhere\n\n\\(d_{\\text{model}}\\) is the dimensionality of the embeddings.\n\\(i\\) is the index of the embedding dimension. The sin function is applied to the even indices \\(2i\\), while the cos function is applied to the odd indices \\(2i+1\\).\n\nWe can illustrate the encoding as following:\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Display of the position with different sequence lengths\n\n\n\n\n\n\n\n\n\n\n\n(b) Display of the position with different d_model\n\n\n\n\n\n\n\nFigureÂ 2: Illustration of Absolute Position Encoding\n\n\n\n\nThere are several properties can be read from the FigureÂ 2 and EquationÂ 1:\n\nPeriodicity: The sine and cosine functions used in the position encoding have a periodic nature, which allows the model to easily learn to attend to relative positions. This is evident in FigureÂ 2 (a), where the position encodings exhibit similar patterns for different sequence lengths.\nDimensionality: The choice of \\(d_{\\text{model}}\\) affects the granularity of the position encodings. As shown in FigureÂ 2 (b), increasing the dimensionality results in more fine-grained position encodings, which can help the model capture more subtle positional information.\nThe low-dimension part of the dmodel is change more frequently than the high-dimension part, allowing the model to adapt more easily to different input lengths.\n\n\nWe chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset \\(k\\), \\(PE_{pos+k}\\) can be represented as a linear function of \\(PE_{pos}\\).  Attention Is All You Need \n\nHow to understand this sentence. Letâ€™s first redefine the position encoding. \\[\n\\begin{aligned}\n\\mathrm{PE}(pos,2i) &= \\sin\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big), \\\\\n\\mathrm{PE}(pos,2i+1) &= \\cos\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big).\n\\end{aligned}\n\\] where \\(\\alpha_i = 10,000^{2i/d_{\\text{model}}}\\). And we consider \\((2i, 2i+1)\\) as one pair. Now, consider the same pair at position \\(pos + k\\). We can write the position encoding as:\n\\[\n\\begin{align}\n\\mathrm{PE}(pos+k,2i)\n&= \\sin\\!\\Big(\\tfrac{pos+k}{\\alpha_i}\\Big)\n    = \\sin\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big)\\cos\\!\\Big(\\tfrac{k}{\\alpha_i}\\Big)\n    + \\cos\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big)\\sin\\!\\Big(\\tfrac{k}{\\alpha_i}\\Big) \\\\\n\\mathrm{PE}(pos+k,2i+1)\n&= \\cos\\!\\Big(\\tfrac{pos+k}{\\alpha_i}\\Big)\n    =\\cos\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big)\\cos\\!\\Big(\\tfrac{k}{\\alpha_i}\\Big)\n    - \\sin\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big)\\sin\\!\\Big(\\tfrac{k}{\\alpha_i}\\Big)\n\\end{align}\n\\tag{2}\\]\n\n\nAngle addition formulas: \\[\n\\begin{align*}\n&\\sin(a+b) = \\sin(a)\\cos(b) + \\cos(a)\\sin(b) \\\\\n&\\cos(a+b) = \\cos(a)\\cos(b) - \\sin(a)\\sin(b)\n\\end{align*}\n\\]\nWrite this as vector form:\n\\[\n\\mathbf{p}_{pos}^{(i)} =\n\\begin{bmatrix}\n\\sin(pos/\\alpha_i) \\\\\n\\cos(pos/\\alpha_i)\n\\end{bmatrix}\n\\]\nThen \\(\\mathbf{p}_{pos+k}^{(i)}\\) equal to: \\[\n\\mathbf{p}_{pos+k}^{(i)} =\n\\underbrace{\n\\begin{bmatrix}\n\\cos(\\tfrac{k}{\\alpha_i}) & \\ \\ \\sin(\\tfrac{k}{\\alpha_i}) \\\\\n-\\sin(\\tfrac{k}{\\alpha_i}) & \\ \\ \\cos(\\tfrac{k}{\\alpha_i})\n\\end{bmatrix}\n}_{\\displaystyle R_i(k)}\n\\ \\mathbf{p}_{pos}^{(i)}\n\\]\nNotice that \\(R_i(k)\\) is known as rotation matrix which only depends on the relative position \\(k\\) and not on the absolute position \\(pos\\). This is the key insight that allows the model to generalize to different positions.\nStacking all pairs, \\[\n\\mathrm{PE}(pos+k) =\n\\underbrace{\n    \\begin{pmatrix}\n\\cos\\!\\big(\\tfrac{k}{\\alpha_1}\\big) & \\sin\\!\\big(\\tfrac{k}{\\alpha_1}\\big) & 0 & 0 & \\cdots & 0 & 0 \\\\\n-\\sin\\!\\big(\\tfrac{k}{\\alpha_1}\\big) & \\ \\cos\\!\\big(\\tfrac{k}{\\alpha_1}\\big) & 0 & 0 & \\cdots & 0 & 0 \\\\\n0 & 0 & \\cos\\!\\big(\\tfrac{k}{\\alpha_2}\\big) &\\sin\\!\\big(\\tfrac{k}{\\alpha_2}\\big) & \\cdots & 0 & 0 \\\\\n0 & 0 &  -\\sin\\!\\big(\\tfrac{k}{\\alpha_2}\\big) & \\ \\cos\\!\\big(\\tfrac{k}{\\alpha_2}\\big) & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & 0 & \\cdots & \\cos\\!\\big(\\tfrac{k}{\\alpha_{d/2}}\\big) & \\sin\\!\\big(\\tfrac{k}{\\alpha_{d/2}}\\big) \\\\\n0 & 0 & 0 & 0 & \\cdots & -\\sin\\!\\big(\\tfrac{k}{\\alpha_{d/2}}\\big) & \\ \\cos\\!\\big(\\tfrac{k}{\\alpha_{d/2}}\\big)\n\\end{pmatrix}\n}_{R(k)}\n\\cdot\n\\underbrace{\n    \\begin{pmatrix}\n\\sin(\\tfrac{k}{\\alpha_1}) \\\\\n\\cos(\\tfrac{k}{\\alpha_1}) \\\\\n\\sin(\\tfrac{k}{\\alpha_2}) \\\\\n\\cos(\\tfrac{k}{\\alpha_2}) \\\\\n\\vdots \\\\\n\\sin(\\tfrac{k}{\\alpha_{d/2}}) \\\\\n\\cos(\\tfrac{k}{\\alpha_{d/2}})\n\\end{pmatrix}\n}_{\\mathrm{PE}(pos)}\n\\]\nwhere \\(R(k)\\) is block-diagonal with those \\(2\\times2\\) rotations, \\(R(k)\\) depends on \\(k\\) but not on \\(pos\\) â†’ a linear map of \\(\\mathrm{PE}(pos)\\)."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#relative-position-encoding",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#relative-position-encoding",
    "title": "LLM Part1: Architecture",
    "section": "2.3 Relative Position Encoding",
    "text": "2.3 Relative Position Encoding\nRelative Position Encoding, first proposed in Transformer-XL (Dai et al. 2019), then adaptive in different language model.\n\\[\nA_{i,j} =\n\\underbrace{Q_i^\\top K_j}_{\\text{content-based addressing}}\n+\n\\underbrace{Q_i^\\top R_{i-j}}_{\\text{content-dependent positional bias}}\n+\n\\underbrace{u^\\top K_j}_{\\text{global content bias}}\n+\n\\underbrace{v^\\top R_{i-j}}_{\\text{global positional bias}}\n\\]\nwhere:\n\n\\(Q_i \\in \\mathbb{R}^d\\): query vector at position \\(i\\)\n\n\\(K_j \\in \\mathbb{R}^d\\): key vector at position \\(j\\)\n\n\\(R_{i-j} \\in \\mathbb{R}^d\\): embedding of the relative distance \\((i-j)\\)\n\n\\(u, v \\in \\mathbb{R}^d\\): learnable global bias vectors\n\n\\(A_{i,j}\\): unnormalized attention score between position \\(i\\) and \\(j\\)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass RelPositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        self.d_model = d_model\n\n        # relative positions: range [-max_len, max_len]\n        range_len = max_len * 2 + 1\n        self.rel_emb = nn.Embedding(range_len, d_model)\n\n        # trainable biases u, v (Transformer-XL)\n        self.u = nn.Parameter(torch.Tensor(d_model))\n        self.v = nn.Parameter(torch.Tensor(d_model))\n\n    def forward(self, q, k, seq_len):\n        B, H, L, Dh = q.size()\n\n        # (L, L): relative position indices\n        pos_idx = torch.arange(L, dtype=torch.long, device=q.device)\n        rel_idx = pos_idx[None, :] - pos_idx[:, None]  # i-j\n        rel_idx = rel_idx + seq_len  # shift to [0, 2*max_len]\n        rel_pos_emb = self.rel_emb(rel_idx)  # (L, L, d_model)\n\n        # compute QK^T (content-based)\n        content_score = torch.matmul(q, k.transpose(-2, -1))  # (B, H, L, L)\n\n        # project queries with R\n        rel_q = q + self.v.view(1, 1, 1, -1)  # add bias v\n        rel_score = torch.einsum('bhld,lrd-&gt;bhlr', rel_q, rel_pos_emb)\n\n        # add global content bias (u)\n        content_bias = torch.einsum('d,bhjd-&gt;bhj', self.u, k).unsqueeze(2)\n\n        # total score\n        logits = content_score + rel_score + content_bias\n        return logits / (Dh ** 0.5)  # scale as in attention"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#rope-rotary-position-embedding",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#rope-rotary-position-embedding",
    "title": "LLM Part1: Architecture",
    "section": "2.4 RoPE (Rotary Position Embedding)",
    "text": "2.4 RoPE (Rotary Position Embedding)\nSo far we have see the absolute position encoding and relative position encoding. However, there is an problem with absolute position encoding. For example, for two sentence:\n\nEvery Day I will go to gym\nI will go to gym every day\n\nThe absolute position encoding is totally different from two sentences, even though they have the same words and means. On the other hand, the problem of the relative position encoding is that it does not capture the absolute position information, which is crucial for understanding the meaning of the sentences for some task such as text summarization.\nRoPE (Su et al. 2023) is a method combine those two. The vector rotated certain degree according to the absolute position in the sentence. On the other hand, it relative position information is preserved. According to EquationÂ 2, the relative position is not related to the position.\n\n\n\n\n\n\nFigureÂ 3: Illustration of RoPE\n\n\n\n\\[\nR_{\\Theta,m}^{d} \\mathbf{x}\n=\n\\begin{pmatrix}\nx_1\\\\\nx_2\\\\\nx_3\\\\\nx_4\\\\\n\\vdots\\\\\nx_{d-1}\\\\\nx_d\n\\end{pmatrix}\n\\otimes\n\\begin{pmatrix}\n\\cos(m\\theta_{1})\\\\\n\\cos(m\\theta_{1})\\\\\n\\cos(m\\theta_{2})\\\\\n\\cos(m\\theta_{2})\\\\\n\\vdots\\\\\n\\cos\\!\\big(m\\theta_{d/2}\\big)\\\\\n\\cos\\!\\big(m\\theta_{d/2}\\big)\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n- x_2\\\\\nx_1\\\\\n- x_4\\\\\nx_3\\\\\n\\vdots\\\\\n- x_d\\\\\nx_{d-1}\n\\end{pmatrix}\n\\otimes\n\\begin{pmatrix}\n\\sin(m\\theta_{1})\\\\\n\\sin(m\\theta_{1})\\\\\n\\sin(m\\theta_{2})\\\\\n\\sin(m\\theta_{2})\\\\\n\\vdots\\\\\n\\sin\\!\\big(m\\theta_{d/2}\\big)\\\\\n\\sin\\!\\big(m\\theta_{d/2}\\big)\n\\end{pmatrix}\n\\tag{3}\\]\nwhere\n\\[\n\\theta_{i,d} = \\frac{1}{10,000^{2(i - 1) / d}} ,\\quad i \\in [1, 2, \\dots, d / 2 ]\n\\]\nAs we can see, to implement the RoPE in code, we can:\n\nConstruct cos and sin matrices for the given input dimensions and maximum position.\nApply the rotation to the input embeddings using the constructed matrices.\n\n\n\n\n\n\n\nNote\n\n\n\nOne thing always bother me about this implementation is that the rotate_half function actually swap pair of the last dimension as mentioned in the paper. For example:\n&gt;&gt;&gt; x = torch.arange(0, 24).reshape(3, 8) # (B, D)\n&gt;&gt;&gt; x\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [16, 17, 18, 19, 20, 21, 22, 23]])\n&gt;&gt;&gt; x1 = rotate_half(x)\n&gt;&gt;&gt; x1\ntensor([[ -4,  -5,  -6,  -7,   0,   1,   2,   3],\n        [-12, -13, -14, -15,   8,   9,  10,  11],\n        [-20, -21, -22, -23,  16,  17,  18,  19]])\nThe above function just change the x to [-x_{d//2}, ..., -x_{d}, x_0, ..., x_{d//2-1}]\nCheck this linkif you are interested.\nIf this really bother you, you can just implement like this:\ndef rotate_half_v2(x):\n    # Assume x is (B, D)\n    x = x.reshape(x.shape[0], -1, 2)\n    x1, x2 = x.unbind(dim = -1)\n    x = torch.stack((-x2, x1), dim = -1)\n    return x.reshape(x.shape[0], -1)\nwhich is same as they mentioned in the paper.\n&gt;&gt;&gt; x\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [16, 17, 18, 19, 20, 21, 22, 23]])\n&gt;&gt;&gt; x2 = rotate_half_v2(x)\n&gt;&gt;&gt; x2\ntensor([[ -1,   0,  -3,   2,  -5,   4,  -7,   6],\n        [ -9,   8, -11,  10, -13,  12, -15,  14],\n        [-17,  16, -19,  18, -21,  20, -23,  22]])\nOr\ndef rotate_half_v3(x: torch.Tensor) -&gt; torch.Tensor:\n    y = torch.empty_like(x)\n    y[..., ::2] = -x[..., 1::2]  # even positions get -odd\n    y[..., 1::2] =  x[..., ::2]  # odd positions get even\n    return y\n\n&gt;&gt;&gt; x3 = rotate_half_v3(x)\n&gt;&gt;&gt; x3\ntensor([[ -1,   0,  -3,   2,  -5,   4,  -7,   6],\n        [ -9,   8, -11,  10, -13,  12, -15,  14],\n        [-17,  16, -19,  18, -21,  20, -23,  22]])\n\n\nThe other implementation of the RoPE is reply on the complex number. We can treat 2D vector \\((x, y)\\) as a complex number \\(z = x + iy\\), and the rotation can be done by multiplying with a complex exponential:\n\\[\nz' = z \\cdot e^{i\\theta} = (x + iy) \\cdot (\\cos \\theta + i \\sin \\theta) = (x \\cos \\theta - y \\sin \\theta) + i (x \\sin \\theta + y \\cos \\theta)\n\\]\nCode adapted from LLaMA model"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#alibi",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#alibi",
    "title": "LLM Part1: Architecture",
    "section": "2.5 ALIBI",
    "text": "2.5 ALIBI\n(Press, Smith, and Lewis 2022)\n\n\n\n\n\n\nFigureÂ 4\n\n\n\nAliBi: simple, monotonic bias â†’ strong extrapolation, lower overhead.\nSo far, for the position embedding, we modify the Q, K to add the position information for attention to calculate. However, is it possible to directly modify the attention score? To notify them the position information? This is exactly what ALIBI does. The ALIBI (Attention with Linear Biases) method introduces a linear bias to the attention scores based on the distance between tokens. The bias is added directly to the attention score before applying the softmax function. Mathematically:\n\\[\n\\operatorname{softmax}\\!\\Big( q_i k_j^\\top \\;+\\; m \\cdot (-(i-j)) \\Big)\n\\]\nwhere:\n\n\\(q_i \\in \\mathbb{R}^d\\): query vector at position \\(i\\)\n\\(k_j \\in \\mathbb{R}^d\\): key vector at position \\(j\\)\n\\(m \\in \\mathbb{R}\\): slop (head-dependent constant)\n\\((i - j) \\geq 0\\): relative distance\n\nFor example, for query \\(i\\), the logits against all keys \\([0, 1, \\dots, i ]\\) become: \\[\n\\ell_i \\;=\\;\n\\Big[\\, q_i k_0^\\top - m(i-0),\\;\n       q_i k_1^\\top - m(i-1),\\;\n       \\ldots,\\;\n       q_i k_{i-1}^\\top - m(1),\\;\n       q_i k_i^\\top \\,\\Big]\n\\]"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#extend-to-longer-context",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#extend-to-longer-context",
    "title": "LLM Part1: Architecture",
    "section": "2.6 Extend to longer context",
    "text": "2.6 Extend to longer context\nSo far, for all the position encoding we discussed, it has one main drawback: it fixed maximum context length during training.\n\n\n\n\n\n\nFigureÂ 5: (Image Source Video: Long-Context LLM Extension)\n\n\n\nWhen we are training on the fixed context length, the model learns to attend to the positions within that context. However, during inference, we may want to extend the context length beyond what the model was trained on. This is where the challenge lies. One way is to train a longer context length model from the beginning. However this requires more computational resources and may not be feasible for all applications. So, we need to find a way to adapt the model to longer contexts without retraining it from scratch.\nThere are several approaches to address this issue. Letâ€™s discuss a few of them.\n\n2.6.1 Linear Position Interpolation\n\n\n\n\n\n\nFigureÂ 6\n\n\n\ndecreases the frequencies of the basis functions so that more tokens fit within each period. The position interpolation (Chen et al. 2023) mentioned that we can just linearly interpolate the position embeddings for the extended context. This allows the model to generate position embeddings for longer sequences without requiring additional training. It just rescale the \\(m\\) base in the RoPE by: \\[\nf'(\\mathbf{x}, m) = f(\\mathbf{x}, m\\frac{L}{L'})\n\\] where \\(L\\) is the original context length and \\(L'\\) is the new context length.\n\n\n2.6.2 NTK-Aware Position Interpolation\nThe Linear Position Interpolation, if it was possible to pick the correct scale parameter dynamically based on the sequence length rather than having to settle for the fixed tradeoff of maximum sequence length vs.Â performance on shorter sequences. The NTK-Aware Position Interpolation method leverages the Neural Tangent Kernel (NTK) framework to adaptively adjust the position embeddings during inference. By analyzing the modelâ€™s behavior in the NTK regime, we can identify the optimal scaling factors for different sequence lengths, allowing for more effective extrapolation of position information.\n\\[\n\\alpha^{\\text{NTK-RoPE}}_{j} = \\kappa^{-\\frac{2j}{d_k}}\n\\]\n\n\n\n\n\n\nNeural Tangent Kernel (NTK)\n\n\n\n\n\n\n\n\n2.6.3 YaRN\nanother RoPE extension method, uses â€œNTK-by-partsâ€ interpolation strategies across different dimensions of the embedding space and introduces a temperature factor to adjust the attention distribution for long inputs. But RoPE cannot extrapolate well to sequences longer than training (e.g., a model trained on 2K tokens struggles at 8K).\n\\[\n\\alpha^{\\mathrm{YaRN}}_{j}\n= \\frac{(1-\\gamma_j)\\,\\tfrac{1}{t} + \\gamma_j}{\\sqrt{T}} \\,.\n\\]\nYaRN modifies RoPE to support longer context windows while preserving model stability. (Peng et al. 2023)"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#layer-normalization-vs.-rms-normalization",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#layer-normalization-vs.-rms-normalization",
    "title": "LLM Part1: Architecture",
    "section": "3.1 Layer Normalization vs.Â RMS Normalization",
    "text": "3.1 Layer Normalization vs.Â RMS Normalization\nThe Layer Normalization (Ba, Kiros, and Hinton 2016) is a technique to normalize the inputs across the features for each training example. It is defined as:\n\\[\n\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma + \\epsilon} \\cdot \\gamma + \\beta\n\\tag{4}\\] where:\n\n\\(\\mu(x)\\): the mean of the input features.\n\\(\\sigma(x)\\): the standard deviation of the input features.\n\\(\\gamma\\): a learnable scale parameter.\n\\(\\beta\\): a learnable shift parameter.\n\nThere are two learnable parameters in Layer Normalization: \\(\\gamma\\) and \\(\\beta\\), which have the same shape as the input features \\(d_{\\text{model}}\\).\nHowever, in the Root Mean Square(RMS) Normalization, proposed in (Zhang and Sennrich 2019), that we remove the mean from the normalization process. The RMS Normalization is defined as:\n\\[\n\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\cdot \\gamma\n\\tag{5}\\] where:\n\n\\(\\epsilon\\): a small constant to prevent division by zero.\n\\(\\gamma\\): a learnable scale parameter, which has the same shape as the input features \\(d_{\\text{model}}\\).\n\nAs we can see, the main difference between Layer Normalization and RMS Normalization is the removal of the mean from the normalization process, and remove the learnable shift parameter \\(\\beta\\). There are several advantage of that:\n\nSimplicity: RMS Normalization is simpler and requires fewer parameters, making it easier to implement and faster to compute. For model with \\(d_{\\text{model}} = 512\\), 8 layers, each layer has 2 normalization. Than the reduction number of parameter is up to \\(512 \\times 8 \\times 2 = 8192\\) parameters.\nFewer operations: no mean subtraction, no bias addition.\nSaves memory bandwidth, which is often the bottleneck in GPUs (not FLOPs).\nWhile reduce the number of parameters, it also maintains similar performance."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#pre-layer-normalization-vs.-post-layer-normalization",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#pre-layer-normalization-vs.-post-layer-normalization",
    "title": "LLM Part1: Architecture",
    "section": "3.2 Pre-Layer Normalization vs.Â Post-Layer Normalization",
    "text": "3.2 Pre-Layer Normalization vs.Â Post-Layer Normalization\n\n\n\n\n\n\nFigureÂ 7: The figure illustrates the three different position of the normalization layer in the transformer architecture.\n\n\n\nOne main good reason why Pre-Layer Normalization is perform bettern than the Post-Layer Normalization is that, according to (Xiong et al. 2020), the help the gradient flow back through the network without disrupting the residual connections. When using the Pre-Layer Normalization, it tends to converge faster and more stably. And the initlization methods is become less sensitive to the scale of the inputs.\nThere are third type of the normalization position by (Ding et al. 2021) called sandwiched normalization, which is a combination of pre-layer and post-layer normalization. Is is proposed to improve the training for the text-image pair.\n\n\n\n\n\n\nGradient Flow\n\n\n\nOne generalizable lesson is that we should keep residual connections â€œcleanâ€ identity paths."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#qk-norm",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#qk-norm",
    "title": "LLM Part1: Architecture",
    "section": "3.3 QK Norm",
    "text": "3.3 QK Norm\nThere is another normalization method called Query-Key Normalization (QK Norm), which is designed to improve the attention mechanism by normalizing the query and key vectors before computing the attention scores."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#multi-headed-attention",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#multi-headed-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.1 Multi Headed Attention",
    "text": "4.1 Multi Headed Attention\nThe standard multi headed attention is defined as:\n\\[\n\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O\n\\tag{6}\\]\nwhere each head is computed as:\n\\[\n\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n\\tag{7}\\] with \\(W_i^Q\\), \\(W_i^K\\), and \\(W_i^V\\) being learned projection matrices.\nAnd the attention function, usually is scaled dot-product attention, is defined as: \\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\\tag{8}\\]\nwhere \\(d_k\\) is the dimension of the keys. The reason for the scaling factor \\(\\sqrt{d_k}\\) is to counteract the effect of large dot-product values by the large dimension of \\(K\\), which can push the softmax function into regions with very small gradients.\n\n4.1.1 Time Complexity of Scaled Dot-Product Attention\nThe time complexity of the scaled dot-product attention mechanism can be analyzed as follows:\n\nQuery-Key Dot Product: The computation of the dot product between the query and key matrices has a time complexity of \\(O(n^2 d_k)\\), where \\(n\\) is the sequence length and \\(d_k\\) is the dimension of the keys.\nSoftmax Computation: The softmax function is applied to the dot product results, which has a time complexity of \\(O(n^2)\\).\nValue Weighting: The final step involves multiplying the softmax output with the value matrix, which has a time complexity of \\(O(n^2 d_v)\\), where \\(d_v\\) is the dimension of the values.\n\nOverall, the time complexity of the scaled dot-product attention is dominated by the query-key dot product and can be expressed as:\n\\[\nO(n^2 (d_k + d_v))\n\\tag{9}\\]\nAs we can see, the time complexity is quadratic in the sequence length, which can be a bottleneck for long sequences / contexts. Letâ€™s see how we can improve it."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#grouped-query-attention-multi-query-attention",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#grouped-query-attention-multi-query-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.2 Grouped Query Attention / Multi Query Attention",
    "text": "4.2 Grouped Query Attention / Multi Query Attention\n\n\n\n\n\n\nFigureÂ 8: Overview of Grouped Query Attention & Multi Query Attention (Image Source: (Ainslie et al. 2023))\n\n\n\nProposed by the (Ainslie et al. 2023), Grouped Query Attention (GQA) and Multi Query Attention (MQA) are designed to reduce the computational burden of the attention mechanism by grouping queries and sharing keys and values across multiple queries."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#sparse-attention",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#sparse-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.3 Sparse Attention",
    "text": "4.3 Sparse Attention\n\nFixed Pattern Attention: for example, sliding window attention\nStrided Attention: for example, dilated attention, every m-th token\nBlock Sparse Attention: for example, BigBird (BigBirdSparseAttention2020zaheer?), Longformer (Beltagy, Peters, and Cohan 2020)\nGlobal Attention:\nLearned / Dynamic Spare Attention\n\n\n4.3.1 Sliding window attention\n\n\n\n\n\n\nFigureÂ 9\n\n\n\n(Beltagy, Peters, and Cohan 2020)\n\n\n4.3.2 Dilated Attention\n\n\n\n\n\n\nFigureÂ 10: The illustration of Sparse Attention. (Image Source: Generating Long Sequences with Sparse Transformers)\n\n\n\n(Child et al. 2019)"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#multi-latent-attention",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#multi-latent-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.4 Multi Latent Attention",
    "text": "4.4 Multi Latent Attention\n\n\n\n\n\n\nFigureÂ 11: Compare between MHA, Grouped Query Attention, Multi Query Attention and Multi Latent Attention.\n\n\n\nMulti Latent Attention (MLA), proposed in (DeepSeek-AI et al. 2024) is a proposed extension to the attention mechanism that aims to capture more complex relationships within the input data by introducing multiple latent spaces. Each latent space can learn different aspects of the data, allowing for a more nuanced understanding of the input.\n\n\n\n\n\n\nFigureÂ 12: The detail of Multi Latent Attention (MLA)."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#flash-attention",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#flash-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.5 Flash Attention",
    "text": "4.5 Flash Attention\nSo far, we see different attention mechanisms that aim to improve the efficiency and effectiveness of the standard attention mechanism. However, all aforementioned methods improve the attention mechanism by approximating the attention calculation. On the other hand, Flash Attention, proposed in (Dao 2023), takes a different approach by optimizing the attention computation itself.\n\n\n\n\n\n\nFigureÂ 13: The illustration of Flash Attention\n\n\n\n\n4.5.1 Flash Attention V1 vs.Â V2 vs.Â V3"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#native-sparse-attention",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#native-sparse-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.6 Native Sparse Attention",
    "text": "4.6 Native Sparse Attention\n\n\n\n\n\n\nFigureÂ 14: Illustration of Native Sparse Attention (Image Source: Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention)\n\n\n\nProposed in (Yuan et al. 2025), this is a novel approach to sparse attention that aligns with hardware capabilities and allows for efficient training of sparse attention mechanisms."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#attention-sink",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#attention-sink",
    "title": "LLM Part1: Architecture",
    "section": "4.7 Attention Sink",
    "text": "4.7 Attention Sink"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#swish",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#swish",
    "title": "LLM Part1: Architecture",
    "section": "5.1 Swish",
    "text": "5.1 Swish"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#gated-linear-unit-glu",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#gated-linear-unit-glu",
    "title": "LLM Part1: Architecture",
    "section": "5.2 Gated Linear Unit (GLU)",
    "text": "5.2 Gated Linear Unit (GLU)"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#multi-layer-perceptron-mlp",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#multi-layer-perceptron-mlp",
    "title": "LLM Part1: Architecture",
    "section": "6.1 Multi Layer Perceptron (MLP)",
    "text": "6.1 Multi Layer Perceptron (MLP)"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#gated-linear-unit-glu-1",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#gated-linear-unit-glu-1",
    "title": "LLM Part1: Architecture",
    "section": "6.2 Gated Linear Unit (GLU)",
    "text": "6.2 Gated Linear Unit (GLU)"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#mixture-of-experts-moe",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#mixture-of-experts-moe",
    "title": "LLM Part1: Architecture",
    "section": "6.3 Mixture of Experts (MoE)",
    "text": "6.3 Mixture of Experts (MoE)"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#weight-initialization",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#weight-initialization",
    "title": "LLM Part1: Architecture",
    "section": "7.1 Weight Initialization",
    "text": "7.1 Weight Initialization"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#layer-initialization",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#layer-initialization",
    "title": "LLM Part1: Architecture",
    "section": "7.2 Layer Initialization",
    "text": "7.2 Layer Initialization"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#llama",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#llama",
    "title": "LLM Part1: Architecture",
    "section": "8.1 LLaMA",
    "text": "8.1 LLaMA"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#qwen",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#qwen",
    "title": "LLM Part1: Architecture",
    "section": "8.2 Qwen",
    "text": "8.2 Qwen"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#deepseek",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#deepseek",
    "title": "LLM Part1: Architecture",
    "section": "8.3 DeepSeek",
    "text": "8.3 DeepSeek"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#gpt-oss",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#gpt-oss",
    "title": "LLM Part1: Architecture",
    "section": "8.4 GPT-Oss",
    "text": "8.4 GPT-Oss"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#diffusion-language-models",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#diffusion-language-models",
    "title": "LLM Part1: Architecture",
    "section": "9.1 Diffusion Language Models",
    "text": "9.1 Diffusion Language Models\n\n\n\n\n\n\nFigureÂ 15: Illustration of Diffusion Language Model. (Video Source: Inception Lab)\n\n\n\nLLaDA (Nie et al. 2025) is a diffusion-based language model that leverages the principles of diffusion models to generate text. By modeling the text generation process as a diffusion process, LLaDA aims to improve the quality and diversity of generated text.\n\n\n\n\n\n\nFigureÂ 16: Example of LLaDA generation process. Prompt: â€œExplain what artificial intelligence is.â€ (Image Source: LLaDA demo)\n\n\n\n\n\n\n\n\n\nFigureÂ 17: The training process and sampling process of LLaDA. (Image Source: LLaDA)"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#state-space-model-ssm",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#state-space-model-ssm",
    "title": "LLM Part1: Architecture",
    "section": "9.2 State Space Model (SSM)",
    "text": "9.2 State Space Model (SSM)"
  },
  {
    "objectID": "posts/Blogs/LLM-Alignment/post.html",
    "href": "posts/Blogs/LLM-Alignment/post.html",
    "title": "LLM Part3: Alignment",
    "section": "",
    "text": "1 Supervised Fine-Tuning (SFT)\n  2 Review of Reinforcement Learning\n  3 Format LLM Alignment as a Reinforcement Learning Problem\n  4 RLHF Algorithms\n  \n  4.1 Proximal Policy Optimization (PPO)\n  4.2 Direct Preference Optimization (DPO)\n  4.3 Group Relative Policy Optimization (GRPO)\n  4.4 Group Sequence Policy Optimization(GSPO)\n  4.5 Soft Adaptive Policy Optimization(SAPO)\n  \n  5 Conclusion\nIn the part 1 and part 2 of the LLM series, we covered the architecture and inference techniques for LLMs. In this part 3, we will focus on alignment techniques, which are crucial for ensuring that LLMs behave in ways that are consistent with human values and intentions. We will explore various methods for aligning LLMs, including reinforcement learning from human feedback (RLHF), and discuss their implications for the development and deployment of these models. We will first explore the simple Supervised Fine-Tuning (SFT) approach, which involves fine-tuning LLMs on curated datasets that reflect human values and preferences. Than we will explore different RLHF techniques, which involve training LLMs using feedback from human evaluators to improve their alignment with human intentions. We will explore algorithms from PPO, DPO to GRPO and"
  },
  {
    "objectID": "posts/Blogs/LLM-Alignment/post.html#proximal-policy-optimization-ppo",
    "href": "posts/Blogs/LLM-Alignment/post.html#proximal-policy-optimization-ppo",
    "title": "LLM Part3: Alignment",
    "section": "4.1 Proximal Policy Optimization (PPO)",
    "text": "4.1 Proximal Policy Optimization (PPO)"
  },
  {
    "objectID": "posts/Blogs/LLM-Alignment/post.html#direct-preference-optimization-dpo",
    "href": "posts/Blogs/LLM-Alignment/post.html#direct-preference-optimization-dpo",
    "title": "LLM Part3: Alignment",
    "section": "4.2 Direct Preference Optimization (DPO)",
    "text": "4.2 Direct Preference Optimization (DPO)"
  },
  {
    "objectID": "posts/Blogs/LLM-Alignment/post.html#group-relative-policy-optimization-grpo",
    "href": "posts/Blogs/LLM-Alignment/post.html#group-relative-policy-optimization-grpo",
    "title": "LLM Part3: Alignment",
    "section": "4.3 Group Relative Policy Optimization (GRPO)",
    "text": "4.3 Group Relative Policy Optimization (GRPO)\nhttps://arxiv.org/abs/2402.03300"
  },
  {
    "objectID": "posts/Blogs/LLM-Alignment/post.html#group-sequence-policy-optimizationgspo",
    "href": "posts/Blogs/LLM-Alignment/post.html#group-sequence-policy-optimizationgspo",
    "title": "LLM Part3: Alignment",
    "section": "4.4 Group Sequence Policy Optimization(GSPO)",
    "text": "4.4 Group Sequence Policy Optimization(GSPO)\nhttps://arxiv.org/abs/2507.18071"
  },
  {
    "objectID": "posts/Blogs/LLM-Alignment/post.html#soft-adaptive-policy-optimizationsapo",
    "href": "posts/Blogs/LLM-Alignment/post.html#soft-adaptive-policy-optimizationsapo",
    "title": "LLM Part3: Alignment",
    "section": "4.5 Soft Adaptive Policy Optimization(SAPO)",
    "text": "4.5 Soft Adaptive Policy Optimization(SAPO)\nUsed in the Qwen3-VL (Bai et al. 2025) model."
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html",
    "title": "All About Diffusion & Flow Models",
    "section": "",
    "text": "1 Preliminary\n  \n  1.1 Multivariate Gaussian Distribution\n  \n  1.1.1 Linear Gaussian\n  \n  1.2 KL-Divergence & Fisher Divergence\n  1.3 ELBO\n  1.4 Score function & Langevin Dynamics\n  \n  2 DDPM\n  \n  2.1 Forward and Backward Diffusion Process\n  2.2 Loss Function\n  2.3 Sampling from DDPM\n  2.4 Time Embedding\n  2.5 Sampling\n  \n  3 Score Matching\n  4 Conditioned Generation\n  \n  4.1 Classifier Generation\n  4.2 Classifier-Free Generation\n  \n  5 Speed Up Diffusion Models\n  \n  5.1 DDIM\n  5.2 Progressive Distillation\n  5.3 Consistency Models\n  5.4 Latent Diffusion Model\n  5.5 Score Matching\n  \n  6 From ODE and SDE view point\n  \n  6.1 ODE vs.Â SDE\n  \n  6.1.1 Vector Field\n  \n  6.2 Conditional Vector Field & Marginal Vector Field\n  6.3 Mean Flow\n  \n  7 Model Architecture\n  \n  7.1 U-Net\n  7.2 Control Net\n  7.3 Diffusion Transformer (DiT)\n  \n  8 Applications\n  \n  8.1 Text-Image Generation\n  \n  8.1.1 Imagen\n  8.1.2 DALLÂ·E\n  8.1.3 Stable Diffusion\n  \n  8.2 Text-Video Generation\n  \n  8.2.1 Meta Movie Gen Video\n  8.2.2 Veo\n  \n  8.3 Language Modeling\n  8.4 Diffusion Policy\n  \n  9 Learning Resource\nThis article offers a comprehensive overview of diffusion models from multiple perspectives. We begin with the foundationsâ€”DDPM, DDIM, and Score Matchingâ€”and explore their relationships. From there, we introduce the ODE/SDE framework, showing how DDPM can be derived from stochastic differential equations and how this connects to Flow Matching.\nWe then highlight key model variants such as Stable Diffusion and Movie Gen, discussing their architectures and applications. Finally, we broaden the scope to examine how diffusion models are being adapted beyond image generation, including diffusion policies in reinforcement learning and their emerging role in large language models (LLMs)."
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#multivariate-gaussian-distribution",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#multivariate-gaussian-distribution",
    "title": "All About Diffusion & Flow Models",
    "section": "1.1 Multivariate Gaussian Distribution",
    "text": "1.1 Multivariate Gaussian Distribution\nThe probability density function of a random vector \\(x \\in \\mathbb{R}^d\\) that follows a multivariate Gaussian distribution with mean vector \\(\\mu \\in \\mathbb{R}^d\\) and covariance matrix \\(\\Sigma \\in \\mathbb{R}^{d \\times d}\\) is given by:\n\\[\np(x) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}}\n\\exp\\left( -\\tfrac{1}{2}(x - \\mu)^{\\top}\\Sigma^{-1}(x - \\mu) \\right)\n\\tag{1}\\]\nA special case arises when the covariance matrix is the identity, \\(\\Sigma = \\mathbf{I}_{d} \\in \\mathbb{R}^{d \\times d}\\). This is known as the isotropic Gaussian. In deep learning practice, it is common to only predict the mean of the Gaussian, denoted \\(\\mu_{\\theta}\\), while assuming an isotropic covariance:\n\\[\np(x) = \\frac{1}{(2\\pi)^{d/2}}\n\\exp\\left( -\\tfrac{1}{2}(x - \\mu_{\\theta})^{\\top}(x - \\mu_{\\theta}) \\right)\n\\tag{2}\\]\nA fundamental property of Gaussian distributions is that the sum of independent Gaussians is itself Gaussian:\n\\[\nx + y \\sim \\mathcal{N}(\\mu_1 + \\mu_2,\\ \\Sigma_1 + \\Sigma_2)\n\\tag{3}\\]\nAs a simple example, consider two independent random Gaussian variables \\(\\varepsilon_1, \\varepsilon_2 \\sim \\mathcal{N}(0, \\mathbf{I}_d)\\). Define: \\[\n\\mathrm{x}_1 = \\sigma_1 \\varepsilon_1, \\quad \\mathrm{x}_2 = \\sigma_2 \\varepsilon_2\n\\] Then, since \\(\\mathrm{x}_1\\) and \\(\\mathrm{x}_2\\) are independent, their sum satisfies:\n\\[\n\\begin{split}\n\\mathrm{x}_1 + \\mathrm{x}_2 &\\sim \\mathcal{N}(0, (\\sigma_1^2 + \\sigma_2^2)\\mathbf{I}_d) \\\\\n\\mathrm{x}_1 + \\mathrm{x}_2 &= \\sqrt{\\sigma_1^2 + \\sigma_2^2},\\varepsilon,\n\\quad \\varepsilon \\sim \\mathcal{N}(0, \\mathbf{I}_d)\n\\end{split}\n\\tag{4}\\]\n\n1.1.1 Linear Gaussian\nA linear Gaussian model specifies the conditional distribution of \\(\\mathbf{y}\\) given \\(\\mathbf{x}\\) as: \\[\nq(\\mathbf{y}\\mid \\mathbf{x}) = \\mathcal{N}\\big(\\mathbf{A}\\mathbf{x} + \\mathbf{b}, \\ \\boldsymbol{\\Sigma}\\big)\n\\tag{5}\\]\nwhere the mean of the \\(\\mathbf{y}\\) is depend on the \\(\\mathbf{x}\\). One simple case is: \\[\nq(\\mathbf{y}\\mid \\mathbf{x})\n=\\mathcal{N} \\big(\\alpha \\mathbf{x},\\beta\\mathbf{I}_{d}\\big)\n\\]\nAn important point to note is that when \\(\\beta\\) is large, the posterior distribution \\(q(\\mathbf{x}\\mid \\mathbf{y})\\) deviates significantly from being Gaussian. However, in the regime where \\(\\beta \\ll 1\\), the posterior can be well approximated by a Gaussian.This is the one important property we need to understand when are implementing the DDPM, where inference relies on approximating posterior distributions during the reverse diffusion process.\n\n\n\n\n\n\nFigureÂ 1\n\n\n\n\n\n\n\n\n\nFigureÂ 2\n\n\n\nCode Generated above graph: GitHub"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#kl-divergence-fisher-divergence",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#kl-divergence-fisher-divergence",
    "title": "All About Diffusion & Flow Models",
    "section": "1.2 KL-Divergence & Fisher Divergence",
    "text": "1.2 KL-Divergence & Fisher Divergence\nThe Kullbackâ€“Leibler (KL) divergence is a measure of how one probability distribution \\(Q\\) diverges from a reference distribution \\(P\\). It is defined as: \\[\nD_{\\text{KL}}(Q \\| P) = \\int Q(z) \\log \\frac{Q(z)}{P(z)}  dz = \\mathbb{E}_{Q}\\left[ \\log \\frac{Q}{P} \\right]\n\\tag{6}\\]\nKey properties:\n\n\\(D_{\\text{KL}} \\geq 0,\\) with equality if and only if \\(Q = P\\) almost everywhere.\nIt is asymmetric: \\(D_{\\text{KL}}(Q \\| P) \\neq D_{\\text{KL}}(P \\| Q)\\).\n\nThe Fisher divergence provides another way to measure discrepancy between two distributions \\(Q\\) and m\\(P\\), focusing on their score functions (the gradients of log densities, we will introduce score function later.). It is defined as: \\[\nD_{F}(Q \\| P) = \\frac{1}{2}  \\mathbb{E}_{z \\sim Q} \\Big[ \\big| \\nabla_z \\log Q(z) - \\nabla_z \\log P(z) \\big|^2 \\Big]\n\\tag{7}\\]\nFor example, for the gaussian distribution EquationÂ 1, the KL-Divergence is:\n\\[\nD_{\\text{KL}}(Q \\| P) = \\frac{1}{2} \\Big(\n\\mathrm{tr}(\\Sigma_p^{-1}\\Sigma_q)(\\mu_p - \\mu_q)^{\\top}\\Sigma_p^{-1}(\\mu_p - \\mu_q)d + \\ln \\frac{\\det \\Sigma_p}{\\det \\Sigma_q}\n\\Big)\n\\tag{8}\\]"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#elbo",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#elbo",
    "title": "All About Diffusion & Flow Models",
    "section": "1.3 ELBO",
    "text": "1.3 ELBO\nIn probabilistic modeling and variational inference, we often want to compute the marginal likelihood of observed data \\(x\\):\n\\[\np(x) = \\int p(x, z),dz = \\int p(x \\mid z),p(z),dz\n\\tag{9}\\]\nwhere:\n\n\\(z\\): is the latent variable.\n\\(p(z)\\): is the prior distribution of the latent variable (we often assume Gaussian for the continuous variable).\n\\(p(x | z)\\): is the likelihood of the data point \\(x\\).\n\nHowever, directly computing \\(p(x)\\) is usually intractable.\n\n\n\n\n\n\nWhy \\(p(x)\\) is intractable?\n\n\n\nDirect computation of \\(p(x)\\) is generally intractable, since the integral is both high-dimensional \\(z \\in \\mathbb{R}^{d}\\) and involves nonlinear functions (e.g., neural networks in generative models).\n\n\nTo address this, we will introduce an tractable approximate distribution(also known as variational distribution) \\(Q_{\\phi}(z |x)\\) to approximate the true posterior \\(P(z |x)\\). Now, letâ€™s re-write the log-likelihood, and insert \\(Q_{\\phi}(z | x)\\) in the equation:\n\\[\n\\begin{split}\n\\log_{\\theta}P(\\mathrm{x})   \n&= \\log \\int P_{\\theta}(\\mathrm{x} | \\mathrm{z}) \\, d\\mathrm{z} \\\\\n&=  \\log \\int P_{\\theta}(\\mathrm{x, z}) \\frac{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\, dx   \\\\\n&=  \\log \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}} \\left[ \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\right] \\\\\n&\\geq   \\boxed{\\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}} \\left[ \\log  \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\right] } \\\\\n&=  \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}} \\left[  \\log\\frac{P_{\\theta}(\\mathrm{x} | \\mathrm{z}) P(\\mathrm{z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}  \\right]    \\\\\n& = \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}}[\\log P_{\\theta}(\\mathrm{x} | \\mathrm{z})] - D_{KL}[Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\| P(\\mathrm{z})]\n\\end{split}\n\\tag{10}\\]\nThe inequality follows from Jensenâ€™s inequality (\\(\\log \\mathbb{E}[f] \\geq \\mathbb{E}[\\log f]\\), since \\(\\log\\) is concave).\nThe boxed expectation is the Evidence Lower Bound (ELBO):\n\\[\n\\text{ELBO} = \\underbrace{ \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}}[\\log P_{\\theta}(\\mathrm{x} | \\mathrm{z})]  }_{ \\text{Reconstruction term} }- \\underbrace{ D_{KL}[Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\| P(\\mathrm{z})] }_{ \\text{Regularization term} }\n\\tag{11}\\]\n\nThe first term encourages the model to reconstruct the data well.\nThe second term regularizes the approximate posterior q_{}(z x) to stay close to the prior p(z).\n\nMaximizing the ELBO therefore makes \\(Q_{\\phi}(z \\mid x)\\) approximate the true posterior, while also maximizing the likelihood of the observed data.\nNow, letâ€™s derive the ELBO from the another perspective, letâ€™s measure how different \\(Q_{\\phi}(\\mathrm{z} | \\mathrm{x})\\) and \\(P(\\mathrm{z}|\\mathrm{x})\\) through KL-divergence:\n\\[\n\\begin{align}\nD_{KL}[Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\| P(\\mathrm{z} | \\mathrm{x})]   \n& = \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\left[ \\log \\frac{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}{P(\\mathrm{z} | \\mathrm{x})} \\right] \\\\\n& = \\int Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\log \\frac{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}{P(\\mathrm{z} | \\mathrm{x})} \\, d\\mathrm{z}  \\\\\n& = - \\int Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\log \\frac{P(\\mathrm{z} | \\mathrm{x})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\, d\\mathrm{z}  \\\\\n& = - \\int Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\log \\frac{P(\\mathrm{z} | \\mathrm{x}) P_{\\theta}(\\mathrm{x})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) P_{\\theta}(\\mathrm{x})}  \\, d\\mathrm{z}  \\\\\n& = - \\int Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\log \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) P_{\\theta}(\\mathrm{x})}  \\, d\\mathrm{z}  \\\\\n& = - \\int Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\log \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}  \\, d\\mathrm{z}   + \\int Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\log P_{\\theta}(\\mathrm{x})  \\, d\\mathrm{z}  \\\\\n& = - \\boxed{\\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}\\left[\\log \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\right]}  + \\log P_{\\theta}(\\mathrm{x})\n\\end{align}\n\\tag{12}\\]\nThat lead to: \\[\n\\log P_{\\theta}(\\mathrm{x}) = \\underbrace{ \\boxed{\\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}\\left[\\log \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\right] } }_{ ELBO }+ D_{KL}[Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\| P(\\mathrm{z} | \\mathrm{x})]    \n\\tag{13}\\]\nThe KN-Divergence is greater than 0, so, the log-likelihood is greater or equal ELBO. When the variational distribution \\(Q_{\\phi}(\\mathrm{z} | \\mathrm{x})\\) is same as the true distribution \\(P(\\mathrm{z} | \\mathrm{x})\\), the ELBO is equal to the log-likelihood.\nSo, in summary, the ELBO is, which is defined as: \\[\nEBLO =  \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}\\left[\\log \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\right]  =  \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}}[\\log P_{\\theta}(\\mathrm{x} | \\mathrm{z})] - D_{KL}[Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\| P(\\mathrm{z})]\n\\tag{14}\\]\nOne of the most well-known applications of the ELBO in deep learning is the Variational AutoEncoder (Kingma and Welling 2022). The VAE is a generative model that combines probabilistic latent variable modeling with neural networks. It introdce an encoder network to parameterize the variational distribution \\(q_{\\phi}(z \\mid x)\\) and a decoder network to model the likelihood \\(p_{\\theta}(x \\mid z)\\). Training the VAE corresponds to maximizing the ELBO, which balances two objectives: (1) accurately reconstructing the input data from latent codes, and (2) regularizing the latent distribution to remain close to a simple prior (typically Gaussian). This makes VAEs powerful tools for both representation learning and generative modeling. For those who are interested in the implementation of the VAE, and deep dive in to VAE, please to check:\n\nVAE Code\nVAE Blog"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#score-function-langevin-dynamics",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#score-function-langevin-dynamics",
    "title": "All About Diffusion & Flow Models",
    "section": "1.4 Score function & Langevin Dynamics",
    "text": "1.4 Score function & Langevin Dynamics\nThe score function of a probability distribution \\(p(x)\\) is defined as the gradient of its log-density with respect to the variable \\(x\\): \\[\ns(x) = \\nabla_x \\log p(x)\n\\tag{15}\\]\nthe score function points toward regions of higher probability mass. In high-dimensional spaces, where the explicit density \\(p(x)\\) may be intractable to compute, the score function provides a powerful alternative representation: instead of knowing the density itself, we only need to know the direction in which probability increases.\nLangevin dynamics originates from statistical physics and describes the motion of particles subject to both deterministic forces and random noise. In the context of sampling from a distribution p(x), Langevin dynamics provides a stochastic iterative update rule: \\[\nx_{t+1} = x_t + \\frac{\\eta}{2} \\nabla_x \\log p(x_t) + \\sqrt{\\eta}\\varepsilon_t\n\\quad \\varepsilon_t \\sim \\mathcal{N}(0, I)\n\\tag{16}\\]\nHere:\n\n\\(\\eta &gt; 0\\) is the step size\nthe gradient term drives samples toward high-probability regions,\nthe noise term ensures proper exploration of the space.\n\nThis stochastic process converges to the target distribution \\(p(x)\\) under suitable conditions, making it a foundational method for Markov Chain Monte Carlo (MCMC) sampling.\nFor example, the score of the Gaussian Distribution is: \\[\ns(x) = \\nabla_x \\log p(x) = -\\Sigma^{-1}(x - \\mu)\n\\tag{17}\\]\nSo, we can run the langevin dynamics as following: \\[\nx_{t+1} = x_t + \\frac{\\eta_t}{2}\\left(-\\frac{x_t-\\mu}{\\sigma^2}\\right) + \\sqrt{\\eta_t}\\,\\varepsilon_t\n\\tag{18}\\]\ndef langevin_dynamics_update(x, step_size, score):\n    noise = np.random.randn()\n    x = x + (step_size / 2.0) * score + np.sqrt(step_size) * noise\n    return x\nBelow are two plot showing the Langevin Dynamics on 1-d Gaussian Distribution, \n\n\n\n\n\n\nFigureÂ 3: langevin_step_vary\n\n\n\n\n\n\n\n\n\nLangevin Dynamics vs.Â Gradient Descient\n\n\n\nThe Langevin Dynamics is very similary as the algorithm we used to update the parameters in the neural network, which if defined as: \\[\nx_{t+1} = x_t - \\eta \\,\\nabla f(x_t)\n\\] where \\(\\eta\\) is the learning rate. However, there are several different: - The Graident Descient is Determinsitc while Langevin Dynamics is storchasic becuase of \\(\\sqrt{ \\eta } \\varepsilon_t\\) - Gradient Descent minimizes an explicit function f(x) while Langevin Dynamics simulates a Markov chain whose stationary distribution is p(x) with constant \\(\\eta\\), it generates samples from that distribution."
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#forward-and-backward-diffusion-process",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#forward-and-backward-diffusion-process",
    "title": "All About Diffusion & Flow Models",
    "section": "2.1 Forward and Backward Diffusion Process",
    "text": "2.1 Forward and Backward Diffusion Process\nFor the diffusion process, we gradually add standard normal distribution, until it become the pure gaussian, mathematically, it can be express as:\n\\[\np(\\mathrm{x}_{t} | \\mathrm{x}_{t- 1}) = \\mathcal{N}(\\mathrm{x}_{t}; \\sqrt{ 1 - \\beta_{t} }\\mathrm{x}_{t -1}, \\beta_{t} \\mathbf{I}_{d})\n\\]\nwhere \\(\\{ \\beta_{t} \\in (0, 1) \\}_{t = 1}^{T}\\) and \\(\\beta_{1} \\leq \\beta_{2} \\leq \\dots \\leq \\beta_{T}\\). The \\(\\mathrm{x}_{t}\\) can be expressed as: \\[\n\\mathrm{x}_{t} =  \\sqrt{ 1 - \\beta_{t} }\\mathrm{x}_{t -1} +\\sqrt{ \\beta_{t} } \\epsilon_{t}\n\\]\nThere are many different choice of \\(\\beta\\):\n\nLearned\nConstant\nLinearly or quadratically increased\nFollows a cosine function\n\nOne thing to notice is that \\(\\beta_{t} \\ll 1\\) to make sure that \\(p_{\\theta}(\\mathrm{x}_{t-1} | \\mathrm{x}_{t})\\) can be approximated to the Gaussian Distribution. Look at the expression of the \\(\\mathrm{x}_{t}\\), we can see that it depends on the \\(\\mathrm{x}_{t-1}\\), while \\(\\mathrm{x}_{t -1 }\\) depends on the \\(\\mathrm{x}_{t - 2}\\), and so on, so, the \\(\\mathrm{x}_{t}\\) can also be expressed as:\n\\[\n\\mathrm{x}_{t} = \\sqrt{ \\alpha_{t} }\\mathrm{x}_{0}+ \\sqrt{ 1-\\alpha_{t} } \\epsilon_{t} \\quad \\text{where}\\ \\alpha_{t} = \\prod_{\\tau=1}^{t}(1 - \\beta_{\\tau})\n\\]\nThis is called the forward process,\nAnd the whole forward process format a Markov Chain:\n\\[\np(\\mathrm{x}_{0}, \\mathrm{x}_{1: T}) = p(\\mathrm{x}_{0}) \\prod_{t = 1}^{T}p(\\mathrm{x}_{t} | \\mathrm{x}_{t  -1 })\n\\]\nLetâ€™s quick summary the forward process, and get familiar with following equations:\n\\[\n\\begin{split}\n\\mathrm{x}_{t} & =  \\sqrt{ 1 - \\beta_{t} }\\mathrm{x}_{t -1} +\\sqrt{ \\beta_{t} } \\epsilon_{t} \\\\\n\\mathrm{x}_{t} & = \\sqrt{ \\alpha_{t} }\\mathrm{x}_{0}+ \\sqrt{ 1-\\alpha_{t} } \\epsilon_{t}  \\\\\n\\mathrm{x}_{0} & = \\frac{\\mathrm{x}_{t} - \\sqrt{ 1-\\alpha_{t} } \\epsilon_{t}}{\\sqrt{ \\alpha_{t} }} \\\\\n\\epsilon_t & = \\frac{\\mathrm{x}_t - \\sqrt{\\alpha_t} \\mathrm{x}_0 }{\\sqrt{1 - \\alpha_t}}\n\\end{split}\n\\]\nFrom the last three equation, we can conclude that, as long as we know two of three \\(\\mathrm{x}_{0}, \\mathrm{x}_{t}, \\mathrm{\\epsilon}_{t}\\), we can get other three, this is very useful when we are training the DDPM. \nBackward Process, backward process is from \\(\\mathrm{x}_{t}\\) to \\(\\mathrm{x}_{t -1}\\), it can be expression as: \\[\np(\\mathrm{x}_{t - 1} |  \\mathrm{ x}_{t}) = \\int p(\\mathrm{x}_{t - 1} | \\mathrm{x}_{t}, \\mathrm{x}_{0})p(\\mathrm{x}_{0} | \\mathrm{x}_{t}) d\\mathrm{x}_{0}\n\\]\nThis is intractable if we marginal over \\(\\mathrm{x}_{0}\\). However, if we also conditioned on the \\(\\mathrm{x}_{0}\\), we will get: \\[\np(\\mathrm{x}_{t - 1} | \\mathrm{x}_{t}, \\mathrm{x}_{0}) = \\frac{q(\\mathrm{x}_{t} | \\mathrm{x}_{t - 1}, \\mathrm{x}_{0}) q(\\mathrm{x}_{t - 1} | \\mathrm{x}_{0})}{q(\\mathrm{x}_{t} | \\mathrm{x}_{0})}\n\\]\nWe now that the Markov property of the forward process, we have: \\[\nq(\\mathrm{x}_{t} | \\mathrm{x}_{t - 1}, \\mathrm{x}_{0})  = q(\\mathrm{x}_{t} | \\mathrm{x}_{t - 1})\n\\] which we know, the exact equation is, and we also know what \\(q(\\mathrm{x}_{t} | \\mathrm{x}_{0})\\), so, we know extact what the \\(p(\\mathrm{x}_{t - 1} | \\mathrm{x}_{t}, \\mathrm{x}_{0})\\) is: \\[\n\\begin{split}\np(\\mathrm{x}_{t - 1} | \\mathrm{x}_{t}, \\mathrm{x}_{0})  & = \\mathcal{N}(\\mathrm{x}_{t -1} | \\mu_{t}(\\mathrm{x}_{0}, \\mathrm{x}_{t}), \\sigma_{t}^{2}\\mathbf{I}_{d}) \\\\ \\\\\n\\quad \\text{where}\\   \\mu_{t}(\\mathrm{x}_{0}, \\mathrm{x}_{t}) & = \\frac{(1 - \\alpha_{t - 1})\\sqrt{ 1- \\beta _{t}}\\mathrm{x}_{t} + \\sqrt{ \\alpha_{t - 1} }\\beta_{t}\\mathrm{x}_{0}}{1 -\\alpha_{t}} \\\\\n\\sigma_{t}^{2} & = \\frac{\\beta_{t}( 1- \\alpha_{t - 1})}{1 -\\alpha_{t}}\n\\end{split}\n\\]\nHowever, when we are generating the example, we donâ€™t know what \\(\\mathrm{x}_{0}\\) is. That why we need train a neural network to approximate it.\nNext, derive the loss function we needed to train the neural network."
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#loss-function",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#loss-function",
    "title": "All About Diffusion & Flow Models",
    "section": "2.2 Loss Function",
    "text": "2.2 Loss Function\nLetâ€™s first derive the loss function of the DDPM. DDPM can be view as the hierarchical VAE. So, we can derive the loss function using the ELBO EquationÂ 14. Recall, the log-likelihood with ELBO is defined as following:\n\\[\n\\begin{align}\n\\log P_{\\theta}(\\mathrm{x}) & =  ELBO + D_{KL}[Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\| P(\\mathrm{z} | \\mathrm{x})]     \\\\\n& \\geq   \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}\\left[\\log \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\right]    \\\\\n& = \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q_{\\phi}(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})}\\left[\\log \\frac{P_{\\theta}(\\mathrm{x}_{0}, \\mathrm{x}_{1:T})}{Q_{\\phi}(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\right]  \n\\end{align}\n\\] where \\(\\mathrm{x}_{1:T}\\) is the latent variable.\nOne thing good about DDPM is that, we know what is the posteriror distribution \\(Q_{\\phi}(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})\\) exactly: \\[\nQ_{\\phi}(\\mathrm{x}_{1:T} | \\mathrm{x}_{0}) = \\prod_{t=1}^{T}P(\\mathrm{x}_{t} | \\mathrm{x}_{t - 1})\n\\]\nSo, the ELBO become: \\[\n\\begin{align}\nEBLO   \n& = \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q_{\\phi}(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})}\\left[\\log \\frac{P_{\\theta}(\\mathrm{x}_{0}, \\mathrm{x}_{1:T})}{Q_{\\phi}(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\right]   \\\\\n& =  \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\frac{P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}}) \\prod_{t = 2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})P(\\mathrm{x}_{T})}\n{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})\\prod_{t=2}^{T}Q(\\mathrm{x}_{t} | \\mathrm{x}_{t - 1})}  \\right] \\\\\n& = \\mathbb{E}_{ Q(\\mathrm{x}_{T} | \\mathrm{x}_{0})} [\\log  P(\\mathrm{x}_{T})] +  \\mathbb{E}_{ Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\frac{\\prod_{t=2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}{\\prod_{t=2}^{T}Q(\\mathrm{x}_{t} | \\mathrm{x}_{t - 1})} \\right] + \\mathbb{E}_{ Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})} \\left[ \\log \\frac{P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x}_{1})}{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})}  \\right] \\\\\n& = \\mathbb{E}_{ Q(\\mathrm{x}_{T} | \\mathrm{x}_{0})} [\\log  P(\\mathrm{x}_{T})]  \n+ \\log  \\sum_{t=2}^{T}\\mathbb{E}_{ Q(\\mathrm{x}_{t-1}, \\mathrm{x_{t}} | \\mathrm{x}_{0})}\\left[ \\log \\frac{P_{\\theta}(\\mathrm{x}_{t-1} | \\mathrm{x}_{t})}{Q(\\mathrm{x}_{t} | \\mathrm{x}_{t-1})}  \\right]\n+ \\mathbb{E}_{ Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})} \\left[ \\log \\frac{P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x}_{1})}{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})}  \\right]\n\\end{align}\n\\]\nAs we can see, to calculate the second term, we need to sample from two random distribution, to get \\(\\mathrm{x_{t}}, \\mathrm{x_{t-1}}\\). This will create very noisy estimate with high variance. So, we need to re-write the ELBO, to make it better low variance, using the Bayesian Rule, we get:\n\\[\n\\begin{align}\nELBO &=   \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\left( P(\\mathrm{x}_{T})\\frac{\\prod_{t = 2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}\n{\\prod_{t=2}^{T}Q(\\mathrm{x}_{t} | \\mathrm{x}_{t - 1})} \\frac{P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}})}{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})} \\right)  \\right]    \\\\\n&=   \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\left( P(\\mathrm{x}_{T})\\frac{\\prod_{t = 2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}\n{\\prod_{t=2}^{T}Q(\\textcolor{green}{\\mathrm{x}_{t} | \\mathrm{x}_{t - 1}, \\mathrm{x}_{0}})} \\frac{P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}})}{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})} \\right)  \\right]  \\\\\n&=   \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\left( P(\\mathrm{x}_{T})\\frac{\\prod_{t = 2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}\n{\\prod_{t=2}^{T}Q(\\textcolor{green}{\\mathrm{x}_{t-1} | \\mathrm{x}_{t}, \\mathrm{x}_{0}})} \\frac{Q(\\mathrm{x}_{t-1} | \\mathrm{x}_{0})}{Q(\\mathrm{x}_{t}|\\mathrm{x}_{0})}\\frac{P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}})}{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})} \\right)  \\right]  \\\\\n&=   \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\left( P(\\mathrm{x}_{T})\\frac{\\prod_{t = 2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}\n{\\prod_{t=2}^{T}Q(\\textcolor{green}{\\mathrm{x}_{t-1} | \\mathrm{x}_{t}, \\mathrm{x}_{0}})} \\frac{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})}{Q(\\mathrm{x}_{T}|\\mathrm{x}_{0})}\\frac{P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}})}{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})} \\right)  \\right]  \\\\\n&=  \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\left( \\frac{P(\\mathrm{x}_{T})}{Q(\\mathrm{x}_{T}|\\mathrm{x}_{0})} \\frac{\\prod_{t = 2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}\n{\\prod_{t=2}^{T}Q(\\textcolor{green}{\\mathrm{x}_{t-1} | \\mathrm{x}_{t}, \\mathrm{x}_{0}})}P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}}) \\right)  \\right]   \\\\\n&=  \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\left( \\frac{P(\\mathrm{x}_{T})}{Q(\\mathrm{x}_{T}|\\mathrm{x}_{0})} \\frac{\\prod_{t = 2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}\n{\\prod_{t=2}^{T}Q(\\textcolor{green}{\\mathrm{x}_{t-1} | \\mathrm{x}_{t}, \\mathrm{x}_{0}})}P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}}) \\right)  \\right]   \\\\\n& = \\mathbb{E}_{\\mathrm{x}_{T} \\sim Q(\\mathrm{x}_{T} | \\mathrm{x}_{0})} \\left[\\log \\frac{P(\\mathrm{x}_{T})}{Q(\\mathrm{x}_{T}|\\mathrm{x}_{0})} \\right] + \\sum_{t=2}^{T} \\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} |\\mathrm{x}_{0} )} \\left[\\log \\frac{P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}\n{Q(\\textcolor{green}{\\mathrm{x}_{t-1} | \\mathrm{x}_{t}, \\mathrm{x}_{0}})} \\right]\n+ \\mathbb{E}_{\\mathrm{x}_{1} \\sim Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})}[P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}}) ]  \\\\\n& = -D_{KL}[Q(\\mathrm{x}_{T} | \\mathrm{x}_{0}) \\| P(\\mathrm{x}_{T})]  \\\\ &\n\\quad - \\sum_{t=2}^{T}\\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} |\\mathrm{x}_{0} )} [D_{KL}[Q(\\mathrm{x}_{t-1} |\\mathrm{x}_{t} \\mathrm{x}_{0}) \\| P_{\\theta}(\\mathrm{x}_{t-1} | \\mathrm{x}_{t})]  ]\\\\&\n\\quad +  \\mathbb{E}_{\\mathrm{x}_{1} \\sim Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})}[P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}}) ]\n\\end{align}\n\\]\nThe first term is the prior matching term, which is the constant, no need to optimize. The third term is the reconstruction term, which is the negilibale, because the variance schedule make it almost constant and its learning signal is weak compared to the denoising terms. Now, letâ€™s check the most complex and horriable term, the second term is the consistent term, which the KL Divergence EquationÂ 6 between two gaussian distribution, which has close form: \\[\n\\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} |\\mathrm{x}_{0} )} [D_{KL}[Q(\\mathrm{x}_{t-1} |\\mathrm{x}_{t} \\mathrm{x}_{0}) \\| P_{\\theta}(\\mathrm{x}_{t-1} | \\mathrm{x}_{t})]  ] = \\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} | \\mathrm{x}_{0})} \\left [\\frac{1}{2\\tilde{\\sigma}_{t}^{2}}\\| \\mu_{\\theta}(\\mathrm{x}_{t}, t)  - \\tilde{\\mu}(\\mathrm{x}_{t} | \\mathrm{x}_{0})\\|^{2} \\right]\n\\]\nSince we know that the \\(\\tilde{\\mu}(\\mathrm{x}_{t} | \\mathrm{x}_{0})\\) exact it, we can optimize this by:\n\nSample \\(\\mathrm{x}_{0}\\), \\(t\\)\nSample \\(\\mathrm{x}_{t}\\) from \\(\\mathcal{N}(\\tilde{\\mu}(\\mathrm{x}_{t} | \\mathrm{x}_{0}), \\beta_{t} \\varepsilon_{t})\\)\nPass the \\(\\mathrm{x}_{t}\\) and \\(t\\) to the neural network \\(\\mu_{\\theta}\\)\nCalculate the mean square loss and update the parameters \\(\\theta\\)\nRepeat\n\nLetâ€™s see how can we get \\(\\mathrm{x}_{0}\\) deriectly from the neural network, let rewrite the ELBO:\n\\[\n\\begin{align}\n\\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} |\\mathrm{x}_{0} )} [D_{KL}[Q(\\mathrm{x}_{t-1} |\\mathrm{x}_{t} \\mathrm{x}_{0}) \\| P_{\\theta}(\\mathrm{x}_{t-1} | \\mathrm{x}_{t})]  ] &= \\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} | \\mathrm{x}_{0})} \\left [\\frac{1}{2\\tilde{\\sigma}_{t}^{2}}\\| \\mu_{\\theta}(\\mathrm{x}_{t}, t)  - \\tilde{\\mu}(\\mathrm{x}_{t}, \\mathrm{x}_{0})\\|^{2} \\right] \\\\\n& = \\frac{1}{2 \\tilde{\\sigma}_t^2}Â \\cdotÂ \n\\frac{\\bar{\\alpha}_{t-1} \\beta_t^2}{(1-\\bar{\\alpha}_t)^2}Â \n\\mathbb{E}_{\\mathrm{x}_{t}\\sim Q(\\mathrm{x}_t|\\mathrm{x}_0)}Â \n\\left[ \\| \\hat{x}_\\theta(x_t, t) - x_0 \\|^2 \\right] \\\\\n& =\\omega_t\n\\mathbb{E}_{\\mathrm{x}_{t}\\sim Q(\\mathrm{x}_t|\\mathrm{x}_0)}Â \n\\left[ \\| \\hat{\\mathrm{x}}_\\theta(\\mathrm{x}_t, t) - \\mathrm{x}_0 \\|^2 \\right]\n\\end{align}\n\\]\nAs we can see, the \\(\\mathrm{x}_{\\theta}\\) can be write as the \\(\\mu_{\\theta}\\) to some constant.\nFinally, letâ€™s get our noise predictor \\(\\varepsilon_t\\)-predictor: \\[\n\\begin{align}\n\\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} |\\mathrm{x}_{0} )} [D_{KL}[Q(\\mathrm{x}_{t-1} |\\mathrm{x}_{t} \\mathrm{x}_{0}) \\| P_{\\theta}(\\mathrm{x}_{t-1} | \\mathrm{x}_{t})]  ] &= \\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} | \\mathrm{x}_{0})} \\left [\\frac{1}{2\\tilde{\\sigma}_{t}^{2}}\\| \\mu_{\\theta}(\\mathrm{x}_{t}, t)  - \\tilde{\\mu}(\\mathrm{x}_{t}, \\mathrm{x}_{0})\\|^{2} \\right] \\\\\n& = \\frac{1}{2 \\tilde{\\sigma}_t^2}\n\\cdot\n\\frac{(1-\\bar{\\alpha}_t)^2}{\\bar{\\alpha}_t (1-\\bar{\\alpha}_t)}\n\\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_t|\\mathrm{x}_0)}\n\\left[ \\| \\hat{\\varepsilon}_\\theta(x_t, t) - \\varepsilon_t \\|^2 \\right]\n\\\\\n& =\\omega_{t}'\n\\mathbb{E}_{\\mathrm{x}_{t}\\sim Q(\\mathrm{x}_t|\\mathrm{x}_0)}Â \n\\left[ \\| \\hat{\\varepsilon}_\\theta(x_t, t) - \\varepsilon_t \\|^2 \\right]\n\\end{align}\n\\]\nSummary, from the DDPM, we have derive 3 different predictor:\n\nMean Predictor\n\\(x_{0}\\) Predictor\nNoise Predictor \n\n\nIn practice, we can simply drop the weight term in training: and use noise predictor\nIn this blog, we will first introduce what is the diffusion models, than we will introduce how to implement the DDPM from scratch using PyTorch. After that, we will explore the flow matching and score matching model through the ODE/SDE. By the end of the blog, I believe you will gain a comprehensive understanding of the diffusion model, and SOTA generative models."
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#sampling-from-ddpm",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#sampling-from-ddpm",
    "title": "All About Diffusion & Flow Models",
    "section": "2.3 Sampling from DDPM",
    "text": "2.3 Sampling from DDPM\nDiffusion Model\n\n\nForward Diffusion Process: \\[\nq(\\mathrm{x}_{t} | \\mathrm{x}_{t - 1}) =\\mathcal{N}(\n\\mathrm{x}_{t};\n\\sqrt{ 1 - \\beta_{t} }\\mathrm{x}_{t}, \\beta_{t}\\mathbf{I}\n)\n\\]\n\\[\n\\mathrm{x}_{t} =  \\sqrt{ 1 - \\beta_{t} }\\mathrm{x}_{t} + \\beta_{t}\\epsilon_{t}, \\quad \\text{where} \\ \\epsilon_{t} \\sim \\mathcal{N}(0, \\mathbf{I}_{})\n\\]\n\\[\n\\mathrm{x}_{t} = \\sqrt{ \\bar{\\alpha}_{t} }\\mathrm{x_{0}} +  \\sqrt{ 1 - \\bar{\\alpha}_{t} }\\epsilon\n\\]\nÂ Langevin dynamics: $$ t = {t-1} + {} p({t-1}) + ,_t, _t (0, )\n$$\nBackward Diffusion Process: $$ \\[\\begin{align}\n& p_{\\theta}(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T) \\prod_{t=1}^T p_{\\theta}(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)  \\\\\n\n&  p_{\\theta}(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) = \\mathcal{N}\\!\\left(\\mathbf{x}_{t-1}; \\mu_{\\theta}(\\mathbf{x}_t, t), \\Sigma_{\\theta}(\\mathbf{x}_t, t)\\right)\n\n\\end{align}\\] $$\nThe above content is intractable, one thing to notice that is is tractable when we conditioned on the \\(\\mathrm{x}_{0}\\) \\[\nq(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\n= \\mathcal{N}\\!\\left(\\mathbf{x}_{t-1};\n\\textcolor{blue}{\\tilde{\\mu}(\\mathbf{x}_t, \\mathbf{x}_0)}, \\,\n\\textcolor{red}{\\tilde{\\beta}_t \\mathbf{I}}\\right)\n\\] where : \\[\n\\begin{align}\n\\tilde{\\mu}_t\n& = \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t}\\mathbf{x}_t\n+ \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t}\n   \\frac{1}{\\sqrt{\\alpha_t}} \\left(\\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_t}\\,\\epsilon_t\\right) \\\\\n& = \\textcolor{cyan}{\\frac{1}{\\sqrt{\\alpha_t}}\n   \\left(\\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\,\\epsilon_t\\right)}\n\\end{align}\n\\]\nSo, the loss function become: $$\n\\[\\begin{align}\n\\mathcal{L}_t^{\\text{simple}}\n& = \\mathbb{E}_{t \\sim [1,T], \\mathbf{x}_0, \\epsilon_t}\n  \\left[ \\left\\| \\epsilon_t - \\epsilon_\\theta(\\mathbf{x}_t, t) \\right\\|^2 \\right] \\\\\n& = \\mathbb{E}_{t \\sim [1,T], \\mathbf{x}_0, \\epsilon_t}\n  \\left[ \\left\\| \\epsilon_t - \\epsilon_\\theta\\!\\left(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0\n  + \\sqrt{1 - \\bar{\\alpha}_t}\\,\\epsilon_t,\\, t \\right) \\right\\|^2 \\right]\n\\end{align}\\] $$\nand the loss is: \\[\n\\mathcal{L} = \\mathcal{L}_{t} + C\n\\] where \\(C\\) is some constant not depend on \\(\\theta\\)"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#time-embedding",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#time-embedding",
    "title": "All About Diffusion & Flow Models",
    "section": "2.4 Time Embedding",
    "text": "2.4 Time Embedding\ndef get_timestep_embedding(timesteps, embedding_dim):\n    \"\"\"\n    This matches the implementation in Denoising Diffusion Probabilistic Models:\n    From Fairseq.\n    Build sinusoidal embeddings.\n    This matches the implementation in tensor2tensor, but differs slightly\n    from the description in Section 3.5 of \"Attention Is All You Need\".\n    \"\"\"\n    assert len(timesteps.shape) == 1\n\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#sampling",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#sampling",
    "title": "All About Diffusion & Flow Models",
    "section": "2.5 Sampling",
    "text": "2.5 Sampling\nAfter training a noise denoiser, we can sample from the \\(p_{\\text{init}}\\), and convert it to the \\(p_{\\text{data}}\\)\nThis is relatively simple,"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#classifier-generation",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#classifier-generation",
    "title": "All About Diffusion & Flow Models",
    "section": "4.1 Classifier Generation",
    "text": "4.1 Classifier Generation"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#classifier-free-generation",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#classifier-free-generation",
    "title": "All About Diffusion & Flow Models",
    "section": "4.2 Classifier-Free Generation",
    "text": "4.2 Classifier-Free Generation"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#ddim",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#ddim",
    "title": "All About Diffusion & Flow Models",
    "section": "5.1 DDIM",
    "text": "5.1 DDIM\nDDIM is determinstic"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#progressive-distillation",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#progressive-distillation",
    "title": "All About Diffusion & Flow Models",
    "section": "5.2 Progressive Distillation",
    "text": "5.2 Progressive Distillation\nAs proposed in (ProgressiveDistillationFast2022salimans?)"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#consistency-models",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#consistency-models",
    "title": "All About Diffusion & Flow Models",
    "section": "5.3 Consistency Models",
    "text": "5.3 Consistency Models\nAs proposed in the (Song et al. 2023)"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#latent-diffusion-model",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#latent-diffusion-model",
    "title": "All About Diffusion & Flow Models",
    "section": "5.4 Latent Diffusion Model",
    "text": "5.4 Latent Diffusion Model\nVariance Autoencoder"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#score-matching-1",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#score-matching-1",
    "title": "All About Diffusion & Flow Models",
    "section": "5.5 Score Matching",
    "text": "5.5 Score Matching\n\\[\n\\nabla_{x_t} \\log q(x_t|x_0)\n= \\nabla_x \\left( - \\frac{\\| x_t - \\sqrt{\\bar{\\alpha}_t} x_0 \\|^2}{2(1-\\bar{\\alpha}_t)} \\right)\n= - \\frac{x_t - \\sqrt{\\bar{\\alpha}_t} x_0}{1-\\bar{\\alpha}_t}\n\\]\n$$ _{x_t} q(x_t|x_0) = - = - \n$$\nSo, can be interpreted as predicting the score \\(\\nabla_{x_t} \\log q(x_t|x_0)\\) up to a scaling factor \\(- \\frac{1}{\\sqrt{1-\\bar{\\alpha}_t}}\\)\nAccording to the Tweedieâ€™s formula, we have: \\[\n\\nabla_{x_t} \\log q(x_t)\n= - \\frac{x_t - \\sqrt{\\bar{\\alpha}_t}\\,\\mathbb{E}[x_0 \\mid x_t]}{1-\\bar{\\alpha}_t}\n\\]\n \n\nSo, this is the Noise-Conditional Score-Based Models \n\nSo, the solution is the Annealed Langevin Dynamics  At the beginning (when \\(\\sigma_{t}\\) is large), As time progresses (and \\(\\sigma_{t}\\) decreases),"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#ode-vs.-sde",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#ode-vs.-sde",
    "title": "All About Diffusion & Flow Models",
    "section": "6.1 ODE vs.Â SDE",
    "text": "6.1 ODE vs.Â SDE\nBefore talk about the ODE and SDE, letâ€™s first understand some concepts to solid our understanding. DDPM can be viewed as the discreted version of the SDE, and SDE can be viewed as continuous version of DDPM.\n\n6.1.1 Vector Field\nVector Field is a function that assign a vector to every point in space. For example: imagine a weather map, at each location, an arrow shows the windâ€™s direction and strength. That arrow map is a vector field.\n\n\\[\nF: \\mathbb{R}^{n} \\to \\mathbb{R}^{n}\n\\]\nAnd every ODE \\(u\\) is defined by a vector field, and take in two variable \\(\\mathrm{x}\\) and \\(t\\) \\[\nu: \\mathbb{R}^{d} \\times [0, 1] \\to \\mathbb{R}^{d}, \\quad (x, t) \\to u_{t}(x)\n\\] that for every time \\(t\\) and location \\(\\mathrm{x}\\), we get a vector \\(u_{t}(\\mathrm{x})  \\in \\mathbb{R}^{d}\\) that point to some direction. Image a point in the weather map, \\(x\\) is a point in the map, and \\(u(x)\\) tell \\(x\\), which direction should go next.\n\n\n\n\n\n\n\n\n\n\n\n\nWhy we need \\(t\\) in the ODE?\n\n\n\nBecause for every location \\(\\mathrm{x}\\), we might arrive same location at different time, due to the random start point \\(\\mathrm{x}_{0}\\)\n\n\n\n\n\nlearned_marginals_norm\n\n\n\n\n\nlearned_marginals_norm\n\n\n\n\n\nlearned_marginals_circle\n\n\n\n \\[\n\\begin{align}\n\\frac{d}{dt}\\mathrm{x}_{t } &= u_{t}(\\mathrm{x}_{t}) \\\\\n\\mathrm{x_{0}}&=x_{0}\n\\end{align}\n\\]\nSo, another question we want to ask it: when we start at \\(x_{0}\\), where are we at \\(t\\). This can be solved by flow, which is a solution to the ODE:\n$$\n\\[\\begin{align}\n\\psi : \\mathbb{R}^d \\times [0,1] \\mapsto \\mathbb{R}^d &,\n\\quad (x_0, t) \\mapsto \\psi_t(x_0) \\\\\n\n\\frac{d}{dt} \\psi_t(x_0) & = u_t(\\psi_t(x_0)) \\\\\n\n\\psi_0(x_0)& = x_0\\\\\n\\end{align}\\] $$\n\\[\n\\mathrm{x}_{1} \\sim  p_{\\text{data}}  \n\\] However, we can not solve the problem. But we can use the numerical analysis. One of the simplest and intuitive methods is Euler method:\n\\[\n\\mathrm{x}_{t + h} = \\mathrm{x}_{t} +  h u_{t}(\\mathrm{x}_{t}) \\quad (t = 0, h, 2h, 3h, \\dots,  1- h)\n\\]\nStochastic Differential Equations extend the ODEs with stochastic(random) trajectories, which is also known as stochastic process. The stochastic is add through a Brownian motion. A Brownain motion \\(W = (W_{t})_{0\\leq t \\leq 1}\\) is a stochastic process such that: \\(W_{0} = 0\\): - Normal Increments: \\(W_{t} - W_{s} \\sim \\mathcal{N}(0, (t - s)\\mathbf{I}_{d})\\) for all \\(0 \\leq s \\leq t\\) - Independent Increments\nBrownian Motion is also known as Wiener Process: \\[\nW_{t + h} = W_{t} + \\sqrt{ h }\\epsilon_{t}, \\quad \\text{where} \\ \\epsilon_{t} \\sim \\mathcal{N}(0, \\mathbf{I}_{d})\n\\]\nOrnstein-Unlenbeck(OU) process\nEuler-Maruyama Method is a numerical method.\n\n\n\nforward_panel_with_side_strips_full\n\n\n\n\n\nreverse_panel_with_side_strips_full"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#conditional-vector-field-marginal-vector-field",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#conditional-vector-field-marginal-vector-field",
    "title": "All About Diffusion & Flow Models",
    "section": "6.2 Conditional Vector Field & Marginal Vector Field",
    "text": "6.2 Conditional Vector Field & Marginal Vector Field\nGiven data point \\(\\mathrm{z}\\), we can construct conditional vector field that: \\[\n\\frac{d}{dt}\\mathrm{x}_{t} = u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z})\n\\] where, by following the ODE \\(u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z})\\), the \\(\\mathrm{x}_{t}\\) will end in the data point \\(\\mathrm{z}\\). However, what we actually want is the marginal vector field: \\[\n\\begin{split}\nu_{t}^{\\text{target}}(\\mathrm{x}_{t} )\n&= \\int u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z}) p(\\mathrm{z} | \\mathrm{x}) \\, d\\mathrm{z} \\\\\n&=  \\int u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z}) \\frac{p_{t}(\\mathrm{x}|\\mathrm{z})p_{\\text{data}}(\\mathrm{z})}{p_{t}(\\mathrm{x})} \\, d\\mathrm{z}  \\\\\n\\end{split}\n\\]\nThis is statisfy the property we want. We can derive is through continuity equation.\n\\[\n\\begin{split}\n\\mathcal{L}_{FM}(\\theta) = \\mathbb{E}_{t \\sim [0,1], \\mathrm{z} \\sim p_{data}, \\mathrm{x_{t}} \\sim p_{t}(\\mathrm{x}_{t} | \\mathrm{z})} \\left[\\| u_{t}^{\\theta}(\\mathrm{x}_{t}) - u_{t}^{\\text{target}}(\\mathrm{x}_{t}) \\|^{2} \\right]\n\\end{split}\n\\]\nOne problem of this is that \\(u_{t}^{\\text{target}}(\\mathrm{x}_{t} )\\) is intractable due to the marginal over high-dimensional.\nLetâ€™s rewrite the \\(\\mathcal{L}_{FM}\\), by using the factor \\(\\|a- b \\|^{2} = \\|a\\|^{2} - 2a^{T}b + \\|b\\|^{2}\\):\n\\[\n\\begin{split}\n\\mathcal{L}_{FM}(\\theta)\n&= \\mathbb{E}_{t\\sim[0,1],\\, z\\sim p_{\\rm data},\\, x_t\\sim p_t(x_t|z)}\n\\left[\\|u^\\theta_t(x_t)-u^{\\rm target}_t(x_t)\\|^2\\right] \\\\\n&= \\mathbb{E}_{t \\sim \\text{Unif},\\, z \\sim p_{\\text{data}},\\, x \\sim p_t(\\cdot|z)}\\left[\\|u^\\theta_t(x_t)\\|^2\\right]\n-2\\,\\mathbb{E}_{t \\sim \\text{Unif},\\,  x \\sim p_t(\\cdot|z)}\\left[u^\\theta_t(x_t)^{T} u^{\\rm target}_t(x_t)\\right]\n+\\underbrace{ \\mathbb{E}_{t \\sim \\text{Unif},\\,  x \\sim p_t(\\cdot|z)}\\left[\\|u^{\\rm target}_t(x_t)\\|^2\\right] }_{ C_{1} }\n\\end{split}\n\\]\nAs we can see, the third term is the constant w.r.t to the \\(\\theta\\), let check the second term: \\[\n\\begin{split}\n\\mathbb{E}_{t \\sim \\text{Unif},\\, x \\sim p_t}\n\\!\\left[u_t^\\theta(x)^{T} u_t^{\\text{target}}(x)\\right]\n&\\overset{(i)}{=}\n\\int_0^1 \\!\\!\\int p_t(x)\\, u_t^\\theta(x)^{T} u_t^{\\text{target}}(x)\\, dx\\, dt \\\\\n&\\overset{(ii)}{=}\n\\int_0^1 \\!\\!\\int p_t(x)\\, u_t^\\theta(x)^{T}\n\\left[\\int u_t^{\\text{target}}(x|z)\\,\n\\frac{p_t(x|z)\\,p_{\\text{data}}(z)}{p_t(x)}\\, dz \\right] dx\\, dt \\\\\n&\\overset{(iii)}{=}\n\\int_0^1 \\!\\!\\int\\!\\!\\int u_t^\\theta(x)^{T} u_t^{\\text{target}}(x|z)\\,\np_t(x|z)\\, p_{\\text{data}}(z)\\, dz\\, dx\\, dt \\\\\n&\\overset{(iv)}{=}\n\\mathbb{E}_{t \\sim \\text{Unif},\\, z \\sim p_{\\text{data}},\\, x \\sim p_t(\\cdot|z)}\n\\!\\left[u_t^\\theta(x)^{T} u_t^{\\text{target}}(x|z)\\right]\n\\end{split}\n\\]\nSo, we can get that: \\[\n\\begin{split}\n\\mathcal{L}_{FM}\n& = \\mathbb{E}_{t \\sim \\text{Unif},\\, z \\sim p_{\\text{data}},\\, x \\sim p_t(\\cdot|z)}\\left[\\|u^\\theta_t(x_t)\\|^2\\right]\n-2\\,\\mathbb{E}_{t \\sim \\text{Unif},\\,  x \\sim p_t(\\cdot|z)}\\left[u^\\theta_t(x_t)^{T} u^{\\rm target}_t(x_t)\\right] + C_{1} \\\\\n&=  \\mathbb{E}_{t \\sim \\text{Unif},\\, z \\sim p_{\\text{data}},\\, x \\sim p_t(\\cdot|z)}\\left[\\|u^\\theta_t(x_t)\\|^2\\right]  - 2 \\mathbb{E}_{t \\sim \\text{Unif},\\, z \\sim p_{\\text{data}},\\, x \\sim p_t(\\cdot|z)}\n\\left[u_t^\\theta(x)^{T} u_t^{\\text{target}}(x|z)\\right] + C_{1}\\\\\n&=  \\mathbb{E}_{t \\sim \\text{Unif},\\, z \\sim p_{\\text{data}},\\, x \\sim p_t(\\cdot|z)}\\left[\\|u^\\theta_t(x_t)\\|^2 - 2u_t^\\theta(x)^{T} u_t^{\\text{target}}(x|z)\\right]  + C_{1} \\\\\n&=  \\mathbb{E}_{t \\sim \\text{Unif},\\, z \\sim p_{\\text{data}},\\, x \\sim p_t(\\cdot|z)}\\left[\\|u^\\theta_t(x_t)\\|^2 - 2u_t^\\theta(x)^{T} u_t^{\\text{target}}(x|z) + \\|u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z})\\|^{2}  - \\|u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z}) \\|^{2} \\right]  + C_{1} \\\\\n&=  \\mathbb{E}_{t \\sim \\text{Unif},\\, z \\sim p_{\\text{data}},\\, x \\sim p_t(\\cdot|z)}\\left[\\|u^\\theta_t(x_t)-u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z})\\|^2  - \\|u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z}) \\|^{2} \\right]  + C_{1} \\\\\n&=  \\mathbb{E}_{t \\sim \\text{Unif},\\, z \\sim p_{\\text{data}},\\, x \\sim p_t(\\cdot|z)}\\left[\\|u^\\theta_t(x_t)-u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z})\\|^2   \\right] \\underbrace{ -\\mathbb{E} \\left[\\|u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z}) \\|^{2} \\right] }_{ C_{2} } + C_{1} \\\\\n&=  \\mathcal{L}_{CFM}(\\theta) + C_{2} + C_{1} \\\\\n\\end{split}\n\\]\nAs we can see the \\(\\mathcal{L}_{CFM}\\) is the \\(\\mathcal{L}_{FM}\\) to some constant \\(C\\). So, we can just minimizing \\(\\mathcal{L}_{CFM}\\), we will get the minizer value of \\(\\mathcal{L}_{FM}\\) as well. \\[\n\\mathcal{L}_{CFM} = \\mathbb{E}_{t \\sim \\text{Unif},\\, z \\sim p_{\\text{data}},\\, x \\sim p_t(\\cdot|z)}\\left[\\|u^\\theta_t(x_t)-u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z})\\|^2   \\right]\n\\]\nFrom there, we get the Flow Matching. This is just a simple regression problem with respect to the vector field. Now, letâ€™s see how to derive the score matching from the SDE ## Conditional Score Function & Marginal Score Function\n\n\n\n\nsamples_evolution\n\n\n\n\n\ndensity_evolution"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#mean-flow",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#mean-flow",
    "title": "All About Diffusion & Flow Models",
    "section": "6.3 Mean Flow",
    "text": "6.3 Mean Flow\nMean Flows for One-step Generative Modeling\nMMDiT"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#u-net",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#u-net",
    "title": "All About Diffusion & Flow Models",
    "section": "7.1 U-Net",
    "text": "7.1 U-Net\nU-Net: Convolutional Networks for Biomedical Image Segmentation\n\n\n\nSummarize the most important point of this section in 1â€“2 sentences.\n\nKeep it concise and action-oriented so readers walk away with clarity.\n\n\n\n\nPose a reflective or guiding question to the reader.\n\nExample: How would this method scale if we doubled the dataset size?"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#control-net",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#control-net",
    "title": "All About Diffusion & Flow Models",
    "section": "7.2 Control Net",
    "text": "7.2 Control Net\nAdding Conditional Control to Text-to-Image Diffusion Models"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#diffusion-transformer-dit",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#diffusion-transformer-dit",
    "title": "All About Diffusion & Flow Models",
    "section": "7.3 Diffusion Transformer (DiT)",
    "text": "7.3 Diffusion Transformer (DiT)"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#text-image-generation",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#text-image-generation",
    "title": "All About Diffusion & Flow Models",
    "section": "8.1 Text-Image Generation",
    "text": "8.1 Text-Image Generation\n\n8.1.1 Imagen\nPhotorealistic Text-to-Image Diffusion Models with Deep Language Understanding\n\n\n8.1.2 DALLÂ·E\n\n\n8.1.3 Stable Diffusion"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#text-video-generation",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#text-video-generation",
    "title": "All About Diffusion & Flow Models",
    "section": "8.2 Text-Video Generation",
    "text": "8.2 Text-Video Generation\n\n8.2.1 Meta Movie Gen Video\nMovie Gen: A Cast of Media Foundation Models\n\n\n8.2.2 Veo"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#language-modeling",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#language-modeling",
    "title": "All About Diffusion & Flow Models",
    "section": "8.3 Language Modeling",
    "text": "8.3 Language Modeling"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#diffusion-policy",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#diffusion-policy",
    "title": "All About Diffusion & Flow Models",
    "section": "8.4 Diffusion Policy",
    "text": "8.4 Diffusion Policy\nRectified Flow: Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow https://arxiv.org/pdf/2209.03003\nMean Flow Mean Flows for One-step Generative Modeling https://arxiv.org/pdf/2505.13447"
  },
  {
    "objectID": "posts/Blogs/KL-Divergence/post.html",
    "href": "posts/Blogs/KL-Divergence/post.html",
    "title": "From Entropy to KL Divergence: A Comprehensive Guide",
    "section": "",
    "text": "1 Entropy\n  2 Cross-Entropy\n  3 KL Divergence\n  \n  3.1 Monte Carlo Estimation of KL Divergence\n  3.2 Control Variates\n  \n  4 Applications of KL Divergence\n  \n  4.1 KL Divergence in Generative Models\n  \n  4.1.1 Variational Autoencoders (VAEs)\n  \n  4.2 Model Distillation\n  4.3 Reinforcement Learning\n  \n  5 Conclusion\nKL Divergence, one of the most important concepts in information theory and statistics, measures the difference between two probability distributions. It quantifies how much information is lost when one distribution is used to approximate another. It is widely used in various fields, including machine learning, data science, and artificial intelligence. In this blog post, we will explore the concept of KL Divergence, its mathematical formulation, and its applications.\nFirst, letâ€™s start with the concept of entropy, which is the foundation of KL Divergence."
  },
  {
    "objectID": "posts/Blogs/KL-Divergence/post.html#monte-carlo-estimation-of-kl-divergence",
    "href": "posts/Blogs/KL-Divergence/post.html#monte-carlo-estimation-of-kl-divergence",
    "title": "From Entropy to KL Divergence: A Comprehensive Guide",
    "section": "3.1 Monte Carlo Estimation of KL Divergence",
    "text": "3.1 Monte Carlo Estimation of KL Divergence\nIn practice, we sometimes donâ€™t known the true distribution P, but we have samples from it. In this case, we can estimate the KL Divergence using Monte Carlo sampling:\nThis is un-biased estimator of KL Divergence using samples from distribution \\(P\\). However, it may have high variance depending on the number of samples and the distributions involved.\nTo reduce variance, we can use importance sampling:\nThis method uses samples from distribution \\(Q\\) and weights them according to the ratio of probabilities under \\(P\\) and \\(Q\\), leading to a lower variance estimate of KL Divergence.\nAnother method is Control Variates"
  },
  {
    "objectID": "posts/Blogs/KL-Divergence/post.html#control-variates",
    "href": "posts/Blogs/KL-Divergence/post.html#control-variates",
    "title": "From Entropy to KL Divergence: A Comprehensive Guide",
    "section": "3.2 Control Variates",
    "text": "3.2 Control Variates\nControl variates is a variance reduction technique that involves using a correlated variable with known expected value to reduce the variance of an estimator. In the context of KL Divergence estimation, we can use a control variate to improve the estimate:\n\\[\nD_{KL}(P || Q) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\log \\frac{P(x_i)}{Q(x_i)} - c (g(x_i) - E[g(X)]) \\right)\n\\]"
  },
  {
    "objectID": "posts/Blogs/KL-Divergence/post.html#kl-divergence-in-generative-models",
    "href": "posts/Blogs/KL-Divergence/post.html#kl-divergence-in-generative-models",
    "title": "From Entropy to KL Divergence: A Comprehensive Guide",
    "section": "4.1 KL Divergence in Generative Models",
    "text": "4.1 KL Divergence in Generative Models\n\n4.1.1 Variational Autoencoders (VAEs)\nIn VAEs, KL Divergence is used to regularize the latent space by minimizing the divergence between the approximate posterior distribution and the prior distribution.\n\\[\n\\mathcal{L} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - D_{KL}(q(z|x) || p(z))\n\\]"
  },
  {
    "objectID": "posts/Blogs/KL-Divergence/post.html#model-distillation",
    "href": "posts/Blogs/KL-Divergence/post.html#model-distillation",
    "title": "From Entropy to KL Divergence: A Comprehensive Guide",
    "section": "4.2 Model Distillation",
    "text": "4.2 Model Distillation\nIn model distillation, KL Divergence is used to align the output distributions of the teacher and student models.\n\\[\n\\mathcal{L} = D_{KL}(P_{teacher} || P_{student})\n\\]"
  },
  {
    "objectID": "posts/Blogs/KL-Divergence/post.html#reinforcement-learning",
    "href": "posts/Blogs/KL-Divergence/post.html#reinforcement-learning",
    "title": "From Entropy to KL Divergence: A Comprehensive Guide",
    "section": "4.3 Reinforcement Learning",
    "text": "4.3 Reinforcement Learning\nIn reinforcement learning, KL Divergence is used to constrain policy updates to ensure stability. For example, the RL in the LLM (RLHF) uses KL Divergence to keep the updated policy close to the original policy.\n\\[\n\\mathcal{L} = \\mathbb{E}_{s \\sim \\pi_{ref}} \\left[ \\frac{\\pi_{new}(a|s)}{\\pi_{ref}(a|s)} A(s, a) \\right] - \\beta D_{KL}(\\pi_{new} || \\pi_{ref})\n\\]"
  },
  {
    "objectID": "posts/Blogs/Speed-up-training/post.html",
    "href": "posts/Blogs/Speed-up-training/post.html",
    "title": "Speed Up Training for Neural Networks",
    "section": "",
    "text": "1 Preliminary\n  \n  1.1 Data Representation\n  1.2 Calculate Memory Usage of Model\n  1.3 Collective operations\n  \n  2 Profiling\n  \n  2.1 Simple Benchmarking\n  2.2 PyTorch Profiler\n  2.3 NVIDIA Nsight Systems\n  \n  3 Single GPU Optimization\n  \n  3.1 Fusion\n  3.2 Tiling\n  3.3 Memory Coalescing\n  3.4 Mixed Precision Training\n  3.5 Gradient Accumulation\n  3.6 Case Study: Flash Attention\n  \n  4 Multi-GPU Optimization(Parallelism)\n  \n  4.1 Data Parallelism\n  4.2 Model Parallelism\n  4.3 Pipeline Parallelism\n  4.4 Tensor Parallelism\n  4.5 Context Parallelism\n  4.6 Case Study: DeepSpeed\n  4.7 Case Study: Megatron-LM\nTraining large neural networks(such as large language models) can be time-consuming and resource-intensive. This blog post explores various techniques to speed up the training process, including optimizing calculations on a single GPU through different techniques such as fusion, tiling, memory coalescing, and parallelizing the training across multiple GPUs such as model parallelism and data parallelism. But before that, we need to understand the basic concepts of GPU, and data types to better understand why and when we need to use these techniques."
  },
  {
    "objectID": "posts/Blogs/Speed-up-training/post.html#preliminary",
    "href": "posts/Blogs/Speed-up-training/post.html#preliminary",
    "title": "Speed Up Training for Neural Networks",
    "section": "1 Preliminary",
    "text": "1 Preliminary\n\n1.1 Data Representation\nIn deep learning, we often use different data types to represent our data and model parameters. The most common data types are:\n\nFloat32: also known as single-precision floating-point format. This is the default floating-point representation  used in most deep learning frameworks. It provides a good balance between precision and performance. It use 32 bits (4 bytes) to represent a number.\nFloat16: This is a half-precision floating-point representation that uses 16 bits instead of 32 bits. It can significantly speed up training and reduce memory usage, but it may lead to numerical instability in some cases.\nBFloat16: This is a truncated version of Float32 that retains the exponent bits but reduces the mantissa bits. It is designed to provide a good trade-off between precision and performance, especially for training large models.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The representation of Float32\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) The representation of Float16\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) The representation of BFloat16\n\n\n\n\n\n\n\nFigureÂ 1: The representation of float32, float16, and bfloat16 data types. The figure shows how the bits are allocated for the sign, exponent, and mantissa in each data type.\n\n\n\n\n\n\n\n\n\nHow those bits represent the number?\n\n\n\n\\[\n\\text{value} = (-1)^s \\times (1.f) \\times 2^{e - 127}\n\\]\nwhere:\n\n\\(s\\) is the sign bit (0 for positive, 1 for negative)\n\\(f\\) is the mantissa (the fractional part): \\(1.f = 1 + \\sum_{i=1}^{23} b_i \\cdot 2^{-i}\\), where \\(b_i\\) are the bits of the mantissa either 0 or 1.\n\\(e\\) is the exponent (an 8-bit unsigned int) with a bias of 127:\n\nFor Float32, \\(e\\) is 8 bits, which range from [1, 254]\nFor Float16, \\(e\\) is 5 bits, which range from [1, 30]\nFor BFloat16, \\(e\\) is 8 bits, which range from [1, 254]\n\n\n\n\nTo check the data type of a tensor and its properties in PyTorch, you can use the .dtype attribute. For example:\nx = torch.zeros(4, 8)\nx.dtype # check the data type of x\nx.numel() # check the number of elements in x\nx.element_size() # check the size of each element in bytes\nx.numel() * x.element_size() # check the total size in bytes\n\n\n1.2 Calculate Memory Usage of Model\nAssume we have a model with \\(N\\) parameters, and each parameter is represented by float32 (4 bytes). \\(A\\) is the number of activation elements stored during forward (depends on input and model depth). How can we calculate the memory need for training this model? Notice that the memory usage of a model is not only determined by the parameters, but also by:\n\nactivations\ngradients\noptimizer states\n\nFor a single parameter, the memory usage for one forward pass and backward pass is:\n\nparameter: 4 bytes (float32)\nactivation: 4 bytes (float32)\ngradient: 4 bytes (float32)\noptimizer state, which can vary depending on the optimizer used. For example, Adam optimizer requires 2 additional states (momentum and variance), each of which is 4 bytes (float32).\n\nSo, the total memory usage for one parameter is: \\[\n\\text{Memory per parameter} = 4 + 4  + 2 \\times 4 = 16 \\text{ bytes}\n\\]\nThus, the total memory usage for the model is: \\[\n\\text{Total Memory} = N \\times 16 \\text{ bytes} + A \\times 4 \\text{ bytes}\n\\]\n\n\n\n\n\n\nWhy need activation for backward pass?\n\n\n\nSay a layer denotes the input to the layer as \\(\\mathbf{x}\\), the weights as \\(\\theta\\), and the output as \\(\\mathbf{y}\\). The loss function is denoted as \\(L\\), which is a function of the output \\(\\mathbf{y}\\) and the target \\(t\\): \\[\n\\mathbf{y} = f(\\mathbf{x}; \\theta)\n\\] To compute the gradient of the loss with respect to the weights, we need to use the chain rule: \\[\n\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\cdot \\frac{\\partial \\mathbf{y}}{\\partial \\theta}\n\\]\nwhere \\(\\frac{\\partial \\mathbf{y}}{\\partial \\theta}\\) is the gradient of the output with respect to the weights \\(\\theta\\), which usually are the function of the input \\(\\mathbf{x}\\) and the weights \\(\\theta\\). To compute this gradient, we need to know the input \\(\\mathbf{x}\\). For example, the linear layer computes: \\[\n\\mathbf{y} = \\mathbf{x} \\cdot \\theta + b\n\\] to compute the gradient, we need to know the input \\(\\mathbf{x}\\), \\[\n\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\cdot \\mathbf{x}\n\\] where \\(\\mathbf{x}\\) is the input to the layer, which is the activation of the previous layer. Thus, we need to store the activation for the backward pass.\n\n\n\n\n1.3 Collective operations\nCollective operations are operations that involve multiple processes or devices, such as GPUs, to perform a computation. They are essential for parallelizing the training process across multiple GPUs. Some common collective operations include:\n\nBroadcast(FigureÂ 2 (a)): This operation sends data from one process to all other processes. It is commonly used to share model parameters or hyperparameters across multiple GPUs.\nScatter(FigureÂ 2 (b)): This operation distributes data from one process to multiple processes. It is often used to distribute input data across multiple GPUs.\nGather(FigureÂ 2 (c)): This operation collects data from multiple processes and combines it into a single process. It is useful for aggregating results from multiple GPUs.\nReduce(FigureÂ 2 (d)): This operation combines data from multiple processes into a single process. It is commonly used to compute the sum or maximum of gradients across multiple GPUs.\nAll-gather(FigureÂ 2 (e)): This operation collects data from all processes and distributes it back to all processes. It is often used to gather gradients or model parameters from multiple GPUs without losing any information.\nAll-reduce(FigureÂ 2 (g)): This operation combines data from all processes and distributes the result back to all processes. It is often used to average gradients across multiple GPUs during training.\nReduce-scatter(FigureÂ 2 (f)): This operation combines data from multiple processes and distributes the result to each process. It is often used to reduce the amount of data that needs to be communicated between processes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Broadcast\n\n\n\n\n\n\n\n\n\n\n\n(b) Scatter\n\n\n\n\n\n\n\n\n\n\n\n(c) Gather\n\n\n\n\n\n\n\n\n\n\n\n(d) Reduce\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) All-gather\n\n\n\n\n\n\n\n\n\n\n\n(f) Reduce-scatter\n\n\n\n\n\n\n\n\n\n\n\n(g) All-reduce\n\n\n\n\n\n\n\nFigureÂ 2: The illustration of different collective operations. The figure shows how data is communicated between processes in each operation.(Image take from: Stanford CS336)\n\n\n\n\nOne should take note is Reduce-Scatter combines two operations:\n\nReduce: Each process (or GPU) contributes its data, and a reduction operation (usually sum, mean, max, etc.) is applied across processes.\nScatter: The reduced result is partitioned and each process gets only a portion (its shard) of the reduced result.\n\n\n\n\n\n\n\nTip\n\n\n\nWay to remember the terminology:\n\nGather: collects data from multiple sources into one destination(not do any operation)\nReduce: performs some associative/commutative operation (sum, min, max)\nBroadcast/Scatter: is inverse of Gather\nAll: means destination is all devices"
  },
  {
    "objectID": "posts/Blogs/Speed-up-training/post.html#profiling",
    "href": "posts/Blogs/Speed-up-training/post.html#profiling",
    "title": "Speed Up Training for Neural Networks",
    "section": "2 Profiling",
    "text": "2 Profiling\nTo optimize the training process, we need to first profile our model to identify the bottlenecks. In this section, we will introduce several tools to profile the model and understand where the time is spent during training. We will discuss several tools that can help us profile our model and identify the bottlenecks in the training process:\n\nSimple Benchmarking (SectionÂ 2.1): The simplest way to measure the time taken for each operation in your model. You can use the time module in Python to measure the time taken for each operation. For example, you can wrap your forward pass in a timer to measure the time taken for each layer.\nPyTorch Profiler (SectionÂ 2.2): PyTorch provides a built-in profiler that can help you analyze the performance of your model. You can use the torch.profiler module to profile your model and visualize the results. The profiler provides detailed information about the time spent on each operation, memory usage, and more.\nNVIDIA Nsight Systems (SectionÂ 2.3): This is a powerful profiling tool that can help you analyze the performance of your model on NVIDIA GPUs. It provides detailed information about the GPU utilization, memory usage, and more. You can use it to identify bottlenecks in your model and optimize the performance.\n\n\n2.1 Simple Benchmarking\n\n\n2.2 PyTorch Profiler\n\n\n2.3 NVIDIA Nsight Systems"
  },
  {
    "objectID": "posts/Blogs/Speed-up-training/post.html#single-gpu-optimization",
    "href": "posts/Blogs/Speed-up-training/post.html#single-gpu-optimization",
    "title": "Speed Up Training for Neural Networks",
    "section": "3 Single GPU Optimization",
    "text": "3 Single GPU Optimization\nIn this section, we will discuss various techniques to optimize the training process on a single GPU. These techniques include: - Fusion(SectionÂ 3.1): This technique combines multiple operations into a single operation to reduce the number of kernel launches and improve performance. For example, you can fuse the forward and backward passes of a layer into a single operation. - Tiling(SectionÂ 3.2): This technique divides the input data into smaller tiles and processes them in parallel to improve memory access patterns and reduce memory usage. For example, you can tile the input data into smaller chunks and process them in parallel. - Memory Coalescing(SectionÂ 3.3): This technique optimizes memory access patterns to improve memory bandwidth utilization. For example, you can coalesce memory accesses to reduce the number of memory transactions and improve performance. - Mixed Precision Training(SectionÂ 3.4): This technique uses lower precision data types (such as float16 or bfloat16) to reduce memory usage and improve performance. It can significantly speed up training while maintaining model accuracy. PyTorch provides built-in support for mixed precision training through the torch.cuda.amp module, which allows you to automatically cast your model and inputs to lower precision during training. - Gradient Accumulation(SectionÂ 3.5): This technique accumulates gradients over multiple mini-batches before performing a weight update. It can help reduce the number of weight updates and improve training stability, especially when using large batch sizes. You can implement gradient accumulation by accumulating gradients in a buffer and updating the model parameters only after a certain number of mini-batches.\n\n3.1 Fusion\n\n\n3.2 Tiling\n\n\n3.3 Memory Coalescing\n\n\n3.4 Mixed Precision Training\n\n\n3.5 Gradient Accumulation\n\n\n3.6 Case Study: Flash Attention"
  },
  {
    "objectID": "posts/Blogs/Speed-up-training/post.html#multi-gpu-optimizationparallelism",
    "href": "posts/Blogs/Speed-up-training/post.html#multi-gpu-optimizationparallelism",
    "title": "Speed Up Training for Neural Networks",
    "section": "4 Multi-GPU Optimization(Parallelism)",
    "text": "4 Multi-GPU Optimization(Parallelism)\nIn this section, we will discuss various techniques to optimize the training process across multiple GPUs. These techniques include: - Data Parallelism(SectionÂ 4.1): This technique splits the input data across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different subset of the input data, and the gradients are averaged across GPUs before updating the model parameters. - Model Parallelism(SectionÂ 4.2): This technique splits the model across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different part of the model, and the gradients are averaged across GPUs before updating the model parameters. This is particularly useful for large models that do not fit into a single GPUâ€™s memory. - Pipeline Parallelism(SectionÂ 4.3): This technique splits the model into multiple stages and processes each stage in parallel across multiple GPUs. Each GPU processes a different stage of the model, and the output of one stage is passed to the next stage. This can help improve throughput and reduce memory usage, especially for large models. - Tensor Parallelism(SectionÂ 4.4): This technique splits the tensors across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different part of the tensor, and the gradients are averaged across GPUs before updating the model parameters. This is particularly useful for large tensors that do not fit into a single GPUâ€™s memory. - Context Parallelism(SectionÂ 4.5): This technique splits the context across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different part of the context, and the gradients are averaged across GPUs before updating the model parameters. This is particularly useful for large contexts that do not fit into a single GPUâ€™s memory.\n\n4.1 Data Parallelism\n\n\n4.2 Model Parallelism\n\n\n4.3 Pipeline Parallelism\n\n\n4.4 Tensor Parallelism\n\n\n4.5 Context Parallelism\n\n\n4.6 Case Study: DeepSpeed\n\n\n4.7 Case Study: Megatron-LM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nSelf CPU %\nSelf CPU\nCPU total %\nCPU total\nCPU time avg\nSelf CUDA\nSelf CUDA %\nCUDA total\nCUDA time avg\n# of Calls\n\n\n\n\naten::gelu\n6.76%\n669.785us\n13.48%\n1.336ms\n1.336ms\n8.642ms\n100.00%\n8.642ms\n8.642ms\n1\n\n\nvoid at::native::vectorized_elementwise_kernel&lt;â€¦&gt;\n0.00%\n0.000us\n0.00%\n0.000us\n0.000us\n8.642ms\n100.00%\n8.642ms\n8.642ms\n1\n\n\ncudaLaunchKernel\n6.72%\n665.807us\n6.72%\n665.807us\n665.807us\n0.000us\n0.00%\n0.000us\n0.000us\n1\n\n\ncudaDeviceSynchronize\n86.52%\n8.574ms\n86.52%\n8.574ms\n4.287ms\n0.000us\n0.00%\n0.000us\n0.000us\n2\n\n\n\nSelf CPU time total: 9.909 ms\nSelf CUDA time total: 8.642 ms"
  },
  {
    "objectID": "posts/Blogs/LLM-Inference/post.html",
    "href": "posts/Blogs/LLM-Inference/post.html",
    "title": "LLM Part2: Inference",
    "section": "",
    "text": "In the part 1 of the LLM series, we covered the architecture of LLMs, including key components such as position encoding, attention mechanisms, and more. We also explored various normalization techniques and the training process for these models. In the part 2, we will explore different inference techniques, which is necessary for effectively utilizing LLMs in real-world applications. And we will also explore several practical examples and use cases to illustrate these techniques in action, such as:\n\nvLLM\n\nhttps://lilianweng.github.io/posts/2023-01-10-inference-optimization/ # Resource Different Architecture: - Mixture of Recursion: https://arxiv.org/abs/2507.10524 - Diffusion Text:\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Blogs/Learnings-in-DeepLearning/post.html",
    "href": "posts/Blogs/Learnings-in-DeepLearning/post.html",
    "title": "All kinds of â€œLearningsâ€ in Deep Learning",
    "section": "",
    "text": "1 Supervised Learning\n  \n  1.1 Regression Problems\n  1.2 Classification Problems\n  \n  2 Unsupervised Learning\n  \n  2.1 Clustering\n  2.2 Generative Models\n  \n  2.2.1 Variational Autoencoders (VAE)\n  2.2.2 Generative Adversarial Networks (GANs)\n  2.2.3 Normalizing Flows\n  2.2.4 Energy-Based Models\n  2.2.5 Diffusion Models\n  \n  \n  3 Self-Supervised Learning\n  \n  3.1 Contrastive Learning\n  3.2 Masked Language Modeling (MLM)\n  3.3 AutoRegressive Modeling\n  \n  4 Semi-Supervised Learning\n  5 Deep Reinforcement Learning\n  6 Representation Learning\n  7 Transfer Learning\n  8 Multi-Task Learning\n  9 Meta Learning\n  10 Online Learning\n  11 Federated Learning\n  12 Curriculum Learning\n  13 Active Learning\n  14 Zero-Shot and Few-Shot Learning\n  15 Continual Learning / Lifelong Learning\n  16 Manifold Learning\n  \n  16.1 JEPA\n  \n  17 Nested Learning\n  18 Summary\nIn the AI field, Deep Learning, which is a subset of machine learning, has revolutionized the way we approach complex problems. One of the key aspects of Deep Learning is the various â€œlearningsâ€ or learning paradigms that enable models to acquire knowledge from data. In this blog, I will cover all kinds of learnings in Deep Learning, including supervised learning, unsupervised learning, self-supervised learning, reinforcement learning, and more. By the end, you will have a solid understanding of different learning paradigms in Deep Learning."
  },
  {
    "objectID": "posts/Blogs/Learnings-in-DeepLearning/post.html#regression-problems",
    "href": "posts/Blogs/Learnings-in-DeepLearning/post.html#regression-problems",
    "title": "All kinds of â€œLearningsâ€ in Deep Learning",
    "section": "1.1 Regression Problems",
    "text": "1.1 Regression Problems\nWhen the output variable is continuous, the task is referred to as a regression problem. Common examples of regression problems include predicting house prices, stock prices, or temperature values. Popular loss functions used in regression tasks include Mean Squared Error (MSE) and Mean Absolute Error (MAE).\nMean Squared Error (MSE): \\[\n\\mathcal{L}_\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - f(x_i))^2\n\\tag{1}\\]\nMean Absolute Error (MAE): \\[\n\\mathcal{L}_\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - f(x_i)|\n\\tag{2}\\]"
  },
  {
    "objectID": "posts/Blogs/Learnings-in-DeepLearning/post.html#classification-problems",
    "href": "posts/Blogs/Learnings-in-DeepLearning/post.html#classification-problems",
    "title": "All kinds of â€œLearningsâ€ in Deep Learning",
    "section": "1.2 Classification Problems",
    "text": "1.2 Classification Problems\nWhen the output variable is categorical, the task is referred to as a classification problem. Common examples of classification problems include image classification, spam detection, and sentiment analysis. Popular loss functions used in classification tasks include Cross-Entropy Loss and Hinge Loss.\nCross-Entropy Loss: \\[\n\\mathcal{L}_\\text{CE} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(f_c(x_i))\n\\tag{3}\\] Hinge Loss: \\[\n\\mathcal{L}_\\text{Hinge} = \\frac{1}{N} \\sum_{i=1}^{N} \\max(0, 1 - y_i f(x_i))\n\\tag{4}\\]"
  },
  {
    "objectID": "posts/Blogs/Learnings-in-DeepLearning/post.html#clustering",
    "href": "posts/Blogs/Learnings-in-DeepLearning/post.html#clustering",
    "title": "All kinds of â€œLearningsâ€ in Deep Learning",
    "section": "2.1 Clustering",
    "text": "2.1 Clustering\nClustering is the task of grouping similar data points together based on their features or characteristics. Common clustering algorithms include K-Means, Hierarchical Clustering, and DBSCAN."
  },
  {
    "objectID": "posts/Blogs/Learnings-in-DeepLearning/post.html#generative-models",
    "href": "posts/Blogs/Learnings-in-DeepLearning/post.html#generative-models",
    "title": "All kinds of â€œLearningsâ€ in Deep Learning",
    "section": "2.2 Generative Models",
    "text": "2.2 Generative Models\nGenerative models aim to learn the underlying distribution of the data and generate new samples that resemble the original data. Some popular generative models include:\n\nVariational Autoencoders (VAE)\nGenerative Adversarial Networks (GANs)\nNormalizing Flows\nEnergy-Based Models\nDiffusion Models\n\n\n2.2.1 Variational Autoencoders (VAE)\nThe Variational Autoencoder (VAE) is a generative model that combines principles from variational inference and autoencoders. It consists of an encoder network that maps input data to a latent space and a decoder network that reconstructs the data from the latent representation. The VAE is trained to maximize the evidence lower bound (ELBO) on the data likelihood, which encourages the model to learn a meaningful latent representation while also generating realistic samples. The loss function for VAE can be expressed as:\n\\[\n\\mathcal{L}_\\text{VAE} = \\mathbb{E}_{\nq_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) || p(z))\n\\tag{5}\\]\n\n\n2.2.2 Generative Adversarial Networks (GANs)\nGenerative Adversarial Networks (GANs) are a class of generative models that consist of two neural networks: a generator and a discriminator. The generator aims to produce realistic samples that resemble the training data, while the discriminator tries to distinguish between real and generated samples. The two networks are trained in a minimax game, where the generator tries to fool the discriminator, and the discriminator tries to correctly classify real and fake samples. The objective function for GANs can be expressed as:\n\\[\n\\min_G \\max_D \\mathbb{E}_{x \\sim p_\\text{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]\n\\tag{6}\\]\n\n\n2.2.3 Normalizing Flows\nNormalizing Flows are a class of generative models that transform a simple probability distribution (e.g., Gaussian) into a more complex distribution by applying a series of invertible and differentiable transformations. The key idea is to use the change of variables formula to compute the likelihood of the data under the transformed distribution. Normalizing flows are trained by maximizing the log-likelihood of the data, which can be expressed as:\n\\[\n\\log p_X(x) = \\log p_Z(f^{-1}(x)) + \\log \\left| \\det \\frac{\\partial f^{-1}(x)}{\\partial x} \\right|\n\\tag{7}\\]\n\n\n2.2.4 Energy-Based Models\nEnergy-Based Models (EBMs) are a class of generative models that define a probability distribution over the data using an energy function. The energy function assigns low energy values to data points that are likely to occur and high energy values to unlikely data points. The probability distribution is defined as:\n\\[\np(x) = \\frac{e^{-E(x)}}{Z}\n\\tag{8}\\] where \\(E(x)\\) is the energy function and \\(Z\\) is the partition function that normalizes the distribution. EBMs are trained by minimizing the energy of the training data while maximizing the energy of negative samples, which can be achieved using techniques such as Contrastive Divergence or Score Matching.\n\n\n2.2.5 Diffusion Models\nDiffusion Models are a class of generative models that learn to generate data by reversing a diffusion process. The diffusion process gradually adds noise to the data, transforming it into a simple distribution (e.g., Gaussian). The model is trained to learn the reverse process, which denoises the data step by step, ultimately generating samples from the original data distribution. The training objective for diffusion models can be expressed as:\n\\[\n\\mathcal{L}_\\text{diffusion} = \\mathbb{E}_{x_0, \\epsilon, t} \\left[ \\| x_0 - f_\\theta(x_t, t) \\|^2 \\right]\n\\tag{9}\\] where \\(x_0\\) is the original data, \\(x_t\\) is the noised data at time step \\(t\\), \\(\\epsilon\\) is the noise added, and \\(f_\\theta\\) is the denoising function parameterized by \\(\\theta\\)."
  },
  {
    "objectID": "posts/Blogs/Learnings-in-DeepLearning/post.html#contrastive-learning",
    "href": "posts/Blogs/Learnings-in-DeepLearning/post.html#contrastive-learning",
    "title": "All kinds of â€œLearningsâ€ in Deep Learning",
    "section": "3.1 Contrastive Learning",
    "text": "3.1 Contrastive Learning\n\n\n\n\n\n\nFigureÂ 1: Illustration of Contrastive Learning\n\n\n\nContrastive Learning is a self-supervised learning technique that aims to learn representations by contrasting positive and negative pairs of data points. The model is trained to bring similar data points (positive pairs) closer together in the representation space while pushing dissimilar data points (negative pairs) apart. A popular loss function used in contrastive learning is the InfoNCE loss, which can be expressed as\n\\[\n\\mathcal{L}_\\text{InfoNCE} = - \\log \\frac{\\exp(\\text{sim}(h_i, h_j) / \\tau)}{\\sum_{k=1}^{N} \\exp(\\text{sim}(h_i, h_k) / \\tau)}\n\\tag{10}\\] where \\(h_i\\) and \\(h_j\\) are the representations of the positive pair, \\(h_k\\) are the representations of negative samples, \\(\\text{sim}(\\cdot, \\cdot)\\) is a similarity function (e.g., cosine similarity), and \\(\\tau\\) is a temperature parameter.\nOne of the most popular frameworks for contrastive learning is SimCLR(Chen et al. 2020), which uses data augmentations to create positive pairs and employs a deep neural network to learn representations.\nThe other one is DINO (Caron et al. 2021), which leverages a teacher-student architecture to learn representations without the need for negative samples. \nThe other contrastive learning methods is CLIP(Radford et al. 2021), which learns joint representations of images and text by contrasting image-text pairs. CLIP has demonstrated impressive zero-shot learning capabilities across various vision tasks.\n\n\n\n\n\n\nFigureÂ 2: The illustration of CLIP model"
  },
  {
    "objectID": "posts/Blogs/Learnings-in-DeepLearning/post.html#masked-language-modeling-mlm",
    "href": "posts/Blogs/Learnings-in-DeepLearning/post.html#masked-language-modeling-mlm",
    "title": "All kinds of â€œLearningsâ€ in Deep Learning",
    "section": "3.2 Masked Language Modeling (MLM)",
    "text": "3.2 Masked Language Modeling (MLM)\nMasked Language Modeling (MLM) is a self-supervised learning technique commonly used in natural language processing (NLP). In MLM, a portion of the input tokens in a text sequence are randomly masked, and the model is trained to predict the original tokens based on the context provided by the unmasked tokens. This approach allows the model to learn contextual representations of words and phrases. The loss function for MLM can be expressed as:\n\\[\n\\mathcal{L}_\\text{MLM} = - \\sum_{i \\in \\mathcal{M}} \\log p_\\theta(x_i | x_{\\setminus \\mathcal{M}})\n\\tag{11}\\] where \\(\\mathcal{M}\\) is the set of masked token positions, \\(x_i\\) is the original token at position \\(i\\), and \\(x_{\\setminus \\mathcal{M}}\\) represents the input sequence with masked tokens."
  },
  {
    "objectID": "posts/Blogs/Learnings-in-DeepLearning/post.html#autoregressive-modeling",
    "href": "posts/Blogs/Learnings-in-DeepLearning/post.html#autoregressive-modeling",
    "title": "All kinds of â€œLearningsâ€ in Deep Learning",
    "section": "3.3 AutoRegressive Modeling",
    "text": "3.3 AutoRegressive Modeling\nAutoRegressive Modeling is another self-supervised learning technique widely used in natural language processing . In autoregressive modeling, the model is trained to predict the next token in a sequence given the previous tokens. This approach allows the model to learn sequential dependencies and generate coherent text. The loss function for autoregressive modeling can be expressed as: \\[\n\\mathcal{L}_\\text{AR} = - \\sum_{i=1}^{N} \\log p_\\theta(x_i | x_{&lt;i})\n\\tag{12}\\] where \\(x_i\\) is the token at position \\(i\\), and \\(x_{&lt;i}\\) represents the sequence of tokens preceding position \\(i\\)."
  },
  {
    "objectID": "posts/Blogs/Learnings-in-DeepLearning/post.html#jepa",
    "href": "posts/Blogs/Learnings-in-DeepLearning/post.html#jepa",
    "title": "All kinds of â€œLearningsâ€ in Deep Learning",
    "section": "16.1 JEPA",
    "text": "16.1 JEPA\nJoint Embedding Predictive Architecture (JEPA) is a novel learning paradigm that focuses on learning joint embeddings of data from multiple modalities or views. The main idea behind JEPA is to learn a shared representation space where data points from different modalities can be compared and related to each other. This approach allows the model to capture the underlying structure and relationships between different types of data, enabling it to perform tasks such as cross-modal retrieval, multi-modal classification, and representation learning."
  },
  {
    "objectID": "posts/Blogs/Position-Embedding/post.html",
    "href": "posts/Blogs/Position-Embedding/post.html",
    "title": "The Evolution of Position Encoding in the Transformer",
    "section": "",
    "text": "1 Preliminary\n  \n  1.1 \\(\\sin\\) and \\(\\cos\\) function\n  1.2 Rotation Matrix and Complex Exponential\n  1.3 Attention in Transformer\n  \n  2 Absolute Positional Encoding\n  \n  2.1 Learned Position Encoding\n  2.2 2D Positional Encoding\n  \n  3 Relative Positional Encoding\n  4 RoPE\n  \n  4.1 2D-RoPE\n  4.2 M-RoPE\n  \n  5 Summary\nAfter Transformer(Vaswani et al. 2023) was introduced, it become the default components in the Deep Learning. It has several components, one of the most important (and most confused for me) was Position Encoding. However the original position encoding methods has several limit, several different methods was proposed later to improve the perform of position encoding in the transformer. In the article, we will explore different methods of position encoding, and extend to the image and video position encoding. But before that, letâ€™s prepare some mathematic background to help us better understand the concepts."
  },
  {
    "objectID": "posts/Blogs/Position-Embedding/post.html#sin-and-cos-function",
    "href": "posts/Blogs/Position-Embedding/post.html#sin-and-cos-function",
    "title": "The Evolution of Position Encoding in the Transformer",
    "section": "1.1 \\(\\sin\\) and \\(\\cos\\) function",
    "text": "1.1 \\(\\sin\\) and \\(\\cos\\) function\n\\(\\sin\\) and \\(\\cos\\) function are two most basic periodic functions that we learned in high school. Both \\(\\sin\\) and \\(\\cos\\) function are periodic with period \\(2\\pi\\): \\[\n\\sin(\\theta + 2\\pi) = \\sin \\theta , \\quad   \\cos(\\theta + 2\\pi) = \\cos \\theta\n\\] The phase-shift(addition) formula for the \\(\\sin\\) and \\(\\cos\\) function is: \\[\n\\begin{split}\n& \\sin (\\theta + \\Delta ) = \\sin \\theta \\cos \\Delta + \\cos \\theta \\sin \\Delta  \\\\\n& \\cos(\\theta + \\Delta) = \\cos\\theta\\cos\\Delta - \\sin\\theta\\sin\\Delta\n\\end{split}\n\\]\nWe can view those as functions of a real variable \\(t\\), than, \\(\\sin t\\) and \\(\\cos t\\) are smooth oscillating waves. A general 1D wave can be written as: \\[\nA \\cos (\\omega t + \\phi)\n\\] where: - \\(A\\) is amplitude: max / min value - \\(\\omega\\) is frequency: how many cycles per unit time. - \\(\\phi\\) is phase shift: horizontal shift\nHere is the plot with different value of \\(\\omega\\):\n\n\n\n\n\n\nFigureÂ 1\n\n\n\nAs we can see in the FigureÂ 1, with larger values of \\(\\omega\\), the \\(\\cos\\) function oscillates more rapidly, while smaller values of \\(\\omega\\) produce slower, less frequent oscillations.\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Prepare data\nt = np.linspace(0, 10, 2000)\nomegas = [0.1, 1, 2]\n\ndata = []\nfor w in omegas:\n    data.append(pd.DataFrame({\"t\": t, \"value\": np.cos(w * t), \"omega\": str(w)}))\n\ndf = pd.concat(data)\n\n# Plot\nsns.set_theme(style=\"whitegrid\")\n\nplt.figure(figsize=(10, 5))\nsns.lineplot(data=df, x=\"t\", y=\"value\", hue=\"omega\")\n\nplt.title(\"cos(Ï‰ t) for different Ï‰\")\nplt.xlabel(\"t\")\nplt.ylabel(\"cos(Ï‰ t)\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/Blogs/Position-Embedding/post.html#rotation-matrix-and-complex-exponential",
    "href": "posts/Blogs/Position-Embedding/post.html#rotation-matrix-and-complex-exponential",
    "title": "The Evolution of Position Encoding in the Transformer",
    "section": "1.2 Rotation Matrix and Complex Exponential",
    "text": "1.2 Rotation Matrix and Complex Exponential\nThe other usage of the \\(\\sin\\) and \\(\\cos\\) function is the in the rotation matrix. To rotate any 2D vector: \\(\\mathbf{v} = \\begin{pmatrix}x  \\\\y \\end{pmatrix}\\) by and angle \\(\\theta\\) (counterclockwise), we can multiply it be the rotation matrix: \\[\nR(\\theta) =\n\\begin{pmatrix}\n\\cos\\theta & -\\sin\\theta \\\\  \n\\sin\\theta & \\cos\\theta  \\\\\n\\end{pmatrix}\n\\]\nSo, the new vector \\(\\mathbf{v}'\\) be come: \\[\n\\mathbf{v}' = R(\\theta) \\mathbf{v} =\n\\begin{pmatrix}  \nx\\cos\\theta - y\\sin\\theta \\\\\nx\\sin\\theta + y\\cos\\theta  \n\\end{pmatrix}\n\\] One of the good property of rotation matrix is that it is the orthonormal matrix(\\(R^{\\top}R = I\\)), which means the length of original vector will not change after rotate: \\[\n\\|Rv\\|^2 = (Rv)^\\top (Rv) = v^\\top (R^\\top R) v = v^\\top I v = v^\\top v = \\|v\\|^2\n\\]\n\nOn the other hand, a rotation in 2D can also be represented by multiplication be a complex number on the unit circle. The Eulerâ€™s Formula is define as: \\[\ne^{i\\theta} = \\cos \\theta + i \\sin \\theta\n\\] This complex number has: - magnitude 1 - angle \\(\\theta\\) By multiplying a complex number by \\(e^{i \\theta}\\) produces a rotation. For a point \\((x, y)\\), it can be represented as a complex number \\(z = x + iy\\). Rotate it by \\(\\theta\\) radians by multiplying \\(z = e^{i\\theta} z\\). Expand using Eulerâ€™s formula, we get: \\[\nz = (\\cos \\theta + i \\sin \\theta)(x + i y) = (x \\cos \\theta - y \\sin \\theta) + i(x \\sin \\theta + y \\cos \\theta)\n\\] which is exact the same as the matrix mutiplication.\nv = torch.tensor([1.5, 0.5])  # original vector v = (x, y)\ntheta = torch.tensor(np.pi / 2)  # rotation angle in radians (e.g\n\nz = torch.view_as_complex(v.view(-1, 2))\nrot = torch.exp(1j * theta)  # e^{iÎ¸} j = \\sqrt{-1}.\n\nz_rot = rot * z  # rotated complex number\nv_rot = torch.view_as_real(z_rot).view(-1)  # rotated vector v' = R(Î¸) v\nWe mentioned that a phase shift of \\(\\Delta\\) in the previous. One of important observation is that it is equivalent to applying a 2D rotation on the vector \\(\\begin{pmatrix} \\cos \\theta \\\\ \\sin  \\theta\\end{pmatrix}\\): \\[\n\\begin{split}\n\\begin{pmatrix} \\cos(\\theta+\\Delta) \\\\ \\sin(\\theta+\\Delta) \\end{pmatrix}\n& = \\begin{pmatrix}\n\\cos\\theta\\cos\\Delta - \\sin\\theta\\sin\\Delta   \\\\\n\\sin \\theta \\cos \\Delta + \\cos \\theta \\sin \\Delta\n\\end{pmatrix} \\\\\n& = \\begin{pmatrix} \\cos\\Delta & -\\sin\\Delta \\\\ \\sin\\Delta & \\cos\\Delta \\end{pmatrix} \\begin{pmatrix} \\cos\\theta \\\\ \\sin\\theta \\end{pmatrix}  \\\\\n& = R(\\Delta) \\begin{pmatrix} \\cos \\theta \\\\ \\sin  \\theta\\end{pmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "posts/Blogs/Position-Embedding/post.html#attention-in-transformer",
    "href": "posts/Blogs/Position-Embedding/post.html#attention-in-transformer",
    "title": "The Evolution of Position Encoding in the Transformer",
    "section": "1.3 Attention in Transformer",
    "text": "1.3 Attention in Transformer\nAttention mechanism is the most important component in the transformer (Vaswani et al. 2023). It process sequences by employ three learnable weight matrices \\(W_{Q}, W_{K}, W_{V}  \\in \\mathbb{R}^{d \\times d_{\\text{model}}}\\). where \\(d_{\\text{model}}\\) represents the dimensionality of the projected subspaces. The matrices transform the input \\(\\mathrm{x}\\) into queries, keys, and values respectively: \\[\nQ = XW_{Q}, \\quad  K = XW_{K}, \\quad  V = XW_{V}\n\\] And the attention matrix is computed using the following formula: \\[\n\\begin{split}\n\\text{Attenion}(Q, K) &= \\text{softmax} \\left( \\frac{QK^{\\top}}{\\sqrt{ d_{\\text{model}} }} \\right) \\\\\nZ &= \\text{Attenion}(Q, K) V\n\\end{split}\n\\]\nAs we can see, on\n\nWe have review some concepts, now we are going to learned different types of the position encodings. The position encoding can be seperated into three different types:\n\nAbsolute Positional Encoding\n\nLearned Positional Encoding\n\nRelative Positional Encoding\nRoPE\n\nLetâ€™s dig into one by one."
  },
  {
    "objectID": "posts/Blogs/Position-Embedding/post.html#learned-position-encoding",
    "href": "posts/Blogs/Position-Embedding/post.html#learned-position-encoding",
    "title": "The Evolution of Position Encoding in the Transformer",
    "section": "2.1 Learned Position Encoding",
    "text": "2.1 Learned Position Encoding\nInstead give each position a pre-defined position embedding vector, we can let neural network to learned the information. After Transformer was proposed, several work such as BERT (Devlin et al. 2019) and GPT. (Radford et al., n.d.) One of the most outstanding is the position encoding in the Vision Transformer (Dosovitskiy et al. 2021).\nEach patch in the image has a position id according to the raser order. Than the index is feed into MLP to project it into the position embedding space:\n\\[\nPE(p) = MLP(p)\n\\]\nCompare to the sine,, this is math simpler to implement:\nclass LearnedPositionEncoding(nn.Module):\n    def __init__(self, max_len, d_model):\n        super().__init__()\n        self.position_embeddings = nn.Embedding(max_len, d_model)\n\n    def forward(self, positions):\n        return self.position_embeddings(positions)\nJust 2 lines of the code.\nHowever, for the image, we known that it has 2D position, the x-axis and y-axis. So, with out flatten the patches, we can use the coordinate to represented the position encoding"
  },
  {
    "objectID": "posts/Blogs/Position-Embedding/post.html#d-positional-encoding",
    "href": "posts/Blogs/Position-Embedding/post.html#d-positional-encoding",
    "title": "The Evolution of Position Encoding in the Transformer",
    "section": "2.2 2D Positional Encoding",
    "text": "2.2 2D Positional Encoding\nSome model such as Diffusion Transformer (Peebles and Xie 2023), Masked Autoencoder (He et al. 2021) are use 2D sincos encoding as the position embedding.\nThe code implementation is as following:\nimport torch\nimport torch.nn as nn\n\n\nclass PosED(nn.Module):\n    def __init__(self, embed_dim, gird_zie):\n        super().__init__()\n\n        self.embed_dim = embed_dim\n        self.grid_size = grid_size\n\n        self.pos_embed = self.build_2d_sincos_pos_embed(\n            self.embed_dim, self.grid_size, cls_token=False, extra_tokens=0\n        )\n\n    def build_2d_sincos_pos_embed(self, embed_dim, grid_size, cls_token, extra_tokens):\n        # --- 1. build 2D grid ---\n        h = torch.arange(grid_size, dtype=torch.float32)\n        w = torch.arange(grid_size, dtype=torch.float32)\n        w_grid, h_grid = torch.meshgrid(w, h, indexing=\"ij\")  # match original numpy ordering: w first\n        grid = torch.stack([w_grid, h_grid], dim=0)  # (2, H, W)\n\n        # reshape to (2, H*W)\n        grid = grid.reshape(2, -1)\n\n        # --- 2. build 2D sin-cos embeddings ---\n        emb_h = self.build_1d_sincos(embed_dim // 2, grid[0])\n        emb_w = self.build_1d_sincos(embed_dim // 2, grid[1])\n        pos_embed = torch.cat([emb_h, emb_w], dim=1)  # (H*W, D)\n\n        # prepend extra tokens (e.g., cls token)\n        if cls_token and extra_tokens &gt; 0:\n            extra = torch.zeros(extra_tokens, embed_dim)\n            pos_embed = torch.cat([extra, pos_embed], dim=0)\n\n        return pos_embed\n\n    def build_1d_sincos(self, embed_dim, pos):\n        \"\"\"\n        embed_dim: D/2\n        pos: (M,)\n        returns: (M, D/2)\n        \"\"\"\n        assert embed_dim % 2 == 0\n\n        omega = torch.arange(embed_dim // 2, dtype=torch.float32)\n        omega = omega / (embed_dim / 2.0)\n        omega = 1.0 / (10000**omega)\n\n        # outer product\n        out = pos[:, None] * omega[None, :]  # (M, D/2)\n\n        emb = torch.cat([torch.sin(out), torch.cos(out)], dim=1)\n        return emb\n\n    def forward(self):\n        return self.pos_embed\nFor the image 2D position encoding is more natural than the 1D-sequence position encoding. For example, a 256Ã—256 image split into 16Ã—16 patches â†’ 16Ã—16 = 256 patches.\nThe actual spatial positions of these patches look like this:\n(0,0) (0,1) (0,2) ... (0,15)\n(1,0) (1,1) (1,2) ... (1,15)\n...\n(15,0)...          (15,15)\nIf you use 1-D sinusoidal positional encoding, the sequence becomes:\ntoken0, token1, token2, ...\nThis causes the model to incorrectly assume: â€¢ token1 is closer to token2 â€¢ token1 is farther from token17\nBut in the real 2-D image space: â€¢ token1 and token2 (leftâ€“right neighbors) are close â€¢ token1 and token17 (the patch directly below) are also close"
  },
  {
    "objectID": "posts/Blogs/Position-Embedding/post.html#d-rope",
    "href": "posts/Blogs/Position-Embedding/post.html#d-rope",
    "title": "The Evolution of Position Encoding in the Transformer",
    "section": "4.1 2D-RoPE",
    "text": "4.1 2D-RoPE"
  },
  {
    "objectID": "posts/Blogs/Position-Embedding/post.html#m-rope",
    "href": "posts/Blogs/Position-Embedding/post.html#m-rope",
    "title": "The Evolution of Position Encoding in the Transformer",
    "section": "4.2 M-RoPE",
    "text": "4.2 M-RoPE"
  },
  {
    "objectID": "posts/Projects/projects_index.html",
    "href": "posts/Projects/projects_index.html",
    "title": "Projects",
    "section": "",
    "text": "Qwen3-VL Inference\n\n4 min\n\n\nLarge Language Model\n\nMulti-Modality\n\nFine-Tuning\n\n\n\nBuilt a Qwen3-VL model from scratch in PyTorch, loaded the 14B (224x224) model, fine-tuned it with LoRA for specific tasks, and developed a Gradio app to showcase itsâ€¦\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaliGemma Inference and Fine Tuning\n\n1 min\n\n\nLarge Language Model\n\nMulti-Modality\n\nFine-Tuning\n\n\n\nBuilt a PaliGemma model from scratch in PyTorch, loaded the 3B (224x224) model, fine-tuned it with LoRA for specific tasks, and developed a Gradio app to showcase itsâ€¦\n\n\n\nYuyang Zhang\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "posts/PapersWithCode/06-vae/VAE.html",
    "href": "posts/PapersWithCode/06-vae/VAE.html",
    "title": "06: Auto-Encoding Variational Bayes(VAE)",
    "section": "",
    "text": "1 Preliminary\n  \n  1.1 Latent Variable Models\n  1.2 AutoEncoder\n  1.3 KL-Divergence\n  1.4 Variational Inference\n  \n  2 VAE\n  \n  2.1 Re-parameterization Trick\n  2.2 Amortized Inference\n  2.3 Experiements\n  \n  3 Summary\n  4 Key Concepts\n  5 Q & A\n  6 Related resource & Further Reading\nä»‹ç»äº†Vision Model å’Œ Language Modelï¼Œ ä»¥åŠå¦‚ä½•é€šè¿‡Self-Supervised Learning æ¥å­¦ä¹ Visionçš„ Representation DINO, æ¥ä¸‹æ¥æˆ‘ä»¬å°†ä¼šæ›´æ–°ä¸€ç³»åˆ—çš„ç”Ÿæˆå¼æ¨¡å‹ï¼ˆGenerative Modelsï¼‰ã€‚ç”Ÿæˆå¼æ¨¡å‹åœ¨å½“ä¸‹æ˜¯å¾ˆçƒ­é—¨çš„è¯é¢˜ï¼Œæ¯”å¦‚Stable Diffusionï¼Œ Nana Banana è¿™äº›æ¨¡å‹éƒ½æ˜¯åŸºæœ¬æ€æƒ³éƒ½æ˜¯åŸºäºLatent Variable Modelsï¼Œæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°±æ¥çœ‹çœ‹Variable Autoencoderè¿™ä¸€ç¯‡ï¼ŒLatent Variable æ¨¡å‹ã€‚"
  },
  {
    "objectID": "posts/PapersWithCode/06-vae/VAE.html#latent-variable-models",
    "href": "posts/PapersWithCode/06-vae/VAE.html#latent-variable-models",
    "title": "06: Auto-Encoding Variational Bayes(VAE)",
    "section": "1.1 Latent Variable Models",
    "text": "1.1 Latent Variable Models\nåœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸å¤„ç† é«˜ç»´ã€å¤æ‚ã€å¸¦å™ªå£°çš„çœŸå®ä¸–ç•Œæ•°æ®ï¼šå›¾åƒã€è¯­éŸ³ã€æ–‡æœ¬â€¦â€¦ è¿™äº›æ•°æ®çš„èƒŒåï¼Œå…¶å®å¸¸å¸¸æœ‰ä¸€äº› æœªè¢«ç›´æ¥è§‚æµ‹åˆ°çš„éšè—ç»“æ„ â€”â€” è¿™å°±æ˜¯ â€œLatent Variablesâ€ï¼ˆæ½œå˜é‡ï¼‰ã€‚\næƒ³è±¡ä¸€ä¸‹ï¼Œä½ çœ‹ä¸€å¼ äººè„¸ç…§ç‰‡ã€‚ç…§ç‰‡æ˜¯è§‚æµ‹å˜é‡ï¼ˆobserved variableï¼‰ã€‚ ä½†æ˜¯è®©è¿™å¼ è„¸â€œçœ‹èµ·æ¥åƒæŸä¸ªäººâ€çš„ï¼Œæ˜¯ä¸€äº›ä¸èƒ½ç›´æ¥çœ‹åˆ°çš„å› ç´ ï¼š â€¢ å…‰ç…§ï¼ˆlightingï¼‰ â€¢ æƒ…ç»ªï¼ˆexpressionï¼‰ â€¢ å§¿æ€ï¼ˆposeï¼‰ â€¢ è„¸éƒ¨ç‰¹å¾ï¼ˆidentityï¼‰ â€¢ èƒŒæ™¯ï¼ˆbackgroundï¼‰\nè¿™äº›å› ç´ è™½ç„¶æ²¡æœ‰åœ¨æ•°æ®ä¸­æ˜¾å¼æ ‡æ³¨ï¼Œå´çœŸå®å­˜åœ¨ï¼Œå¹¶å†³å®šäº†æˆ‘ä»¬çœ‹åˆ°çš„å›¾åƒã€‚ æ½œå˜é‡æ¨¡å‹çš„ç›®æ ‡ï¼Œå°±æ˜¯ç”¨æ•°å­¦æ–¹å¼æŠŠè¿™äº›â€œéšè—å› ç´ â€å»ºæ¨¡å‡ºæ¥ã€‚\n\nLatent Variable Modelï¼ˆLVMï¼‰æ˜¯ä¸€ç±» å‡è®¾è§‚æµ‹æ•°æ®æ˜¯ç”±ä¸€äº›éšè—å˜é‡ç”Ÿæˆçš„æ¦‚ç‡æ¨¡å‹ã€‚ æ¯”å¦‚æˆ‘ä»¬çœ‹åˆ°çš„æ•°æ®\\(\\mathrm{x} \\in \\mathbb{R}^{d}\\), é‚£å®ƒæœ‰ç›¸å¯¹åº”çš„éšè—å˜é‡ \\(z \\in \\mathbb{R}^{k}\\) å…¶ä¸­ \\(k \\ll d\\) . \\(z\\) æ˜¯æˆ‘ä»¬è§‚å¯Ÿä¸åˆ°çš„ï¼Œä¹Ÿå°±æ˜¯æ‰€è°“çš„ Latent Variable.\nåœ¨ä½¿ç”¨Latent Variable Modelæ—¶ï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªå…³é”®çš„ä»»åŠ¡ï¼š 1. Inference: æ ¹æ® \\(\\mathrm{x}\\) æˆ‘ä»¬æ¥æ¨æ–­å‡º \\(\\mathrm{z}\\) æ˜¯ä»€ä¹ˆï¼Œæ¯”å¦‚ï¼Œç»™å®šä¸€å¼ äººè„¸çš„ç…§ç‰‡ï¼Œè¿™ä¸ªæ¨¡å‹éœ€è¦æ‰¾å‡º - è¿™å¼ è„¸æ˜¯ç”·ç”Ÿè¿˜æ˜¯å¥³ç”Ÿ - è¡¨æƒ…æ˜¯å¼€å¿ƒè¿˜æ˜¯æ‚²ä¼¤ - æ˜¯ä»€ä¹ˆæ ·çš„å§¿åŠ¿ åœ¨æ•°å­¦ä¸Šï¼Œå°±æ˜¯æ±‚åéªŒåˆ†å¸ƒ \\(p(\\mathrm{z} | \\mathrm{x})\\) 2. Generation: é€šè¿‡ \\(\\mathrm{z}\\)ï¼Œ æˆ‘ä»¬æ¥ç”Ÿæˆä¸€ä¸ª \\(\\mathrm{x}\\)ã€‚ è¿™ä¸ªå°±æ˜¯ç”Ÿæˆæ¨¡å‹ã€‚\nLatent Variable Model æ˜¯ç°ä»£ç”Ÿæˆå¼æ¨¡å‹çš„åŸºçŸ³ï¼ŒåŸºæœ¬ä¸Šæ‰€æœ‰çš„ç”Ÿæˆå¼æ¨¡å‹ï¼Œæ¯”å¦‚ GANï¼Œ DDPMï¼Œ Flow Model ç­‰ï¼Œéƒ½æ˜¯ä»¥Latent Variable Modelä¸ºåŸºç¡€ï¼Œåœ¨æ­¤æ¡ä»¶ä¸‹ï¼Œé€šè¿‡ä¸åŒæ±‚ Latent Variable çš„æ–¹æ³•ï¼Œæ¥è§£å†³è¿™ç§é—®é¢˜ã€‚"
  },
  {
    "objectID": "posts/PapersWithCode/06-vae/VAE.html#autoencoder",
    "href": "posts/PapersWithCode/06-vae/VAE.html#autoencoder",
    "title": "06: Auto-Encoding Variational Bayes(VAE)",
    "section": "1.2 AutoEncoder",
    "text": "1.2 AutoEncoder\n\nAutoEncoder æ˜¯ä¸€ç§Self-Supervised Learning çš„æ–¹æ³•ã€‚ å®ƒå¯ä»¥è®©ç¥ç»ç½‘ç»œå­¦ä¼šå‹ç¼©ï¼Œå¹¶ä¸”åœ¨è¿˜åŸæ•°æ®ï¼Œé€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥å­¦ä¹ åˆ°çš„ä½çº¬åº¦çš„Latent Variable \\(\\mathrm{z}\\)ã€‚ AutoEncoder ä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯Latent Variable Modelçš„ä¸€ç§å­¦ä¹ æ–¹æ³•ã€‚ AutoEncoder ä¹Ÿé€šå¸¸ç”¨åœ¨Representation Learning è¡¨å¾å­¦ä¹ ã€‚"
  },
  {
    "objectID": "posts/PapersWithCode/06-vae/VAE.html#kl-divergence",
    "href": "posts/PapersWithCode/06-vae/VAE.html#kl-divergence",
    "title": "06: Auto-Encoding Variational Bayes(VAE)",
    "section": "1.3 KL-Divergence",
    "text": "1.3 KL-Divergence\nKL æ•£åº¦ï¼ˆKullbackâ€“Leibler Divergenceï¼‰æ˜¯ç”¨æ¥è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´å·®å¼‚çš„ä¸€ç§åº¦é‡æ–¹æ³•ã€‚å®šä¹‰å¦‚ä¸‹: \\[\nD_{KL}(Q \\| P) = \\mathbb{E}_{x \\sim Q}\\left[ \\log \\frac{Q(x)}{P(x)} \\right]\n\\]\nç›´è§‚çš„è§£é‡ŠKL-Divergenceå°±æ˜¯: - å¦‚æœæ•°æ® \\(x\\) æ˜¯ä» \\(Q\\) åˆ†å¸ƒä¸­æ¥çš„ï¼Œä½†å¦‚æœæˆ‘ä»¬ç”¨ \\(P\\) åˆ†å¸ƒ æ¥è§£é‡Šè¿™äº›æ•°æ®ï¼Œä¼šæŸå¤±å¤šå°‘ä¿¡æ¯é‡\nå¯¹äºé«˜æ–¯åˆ†å¸ƒï¼ˆGaussian Distributionï¼‰ï¼Œæˆ‘ä»¬KL- Divergenceæœ‰ä»¥ä¸‹çš„å½¢å¼ï¼š\n$$\nD_{KL}(Q | P) = $$\nå¦‚æœæ˜¯Diagonal Gaussianï¼Œ é‚£ä¹ˆKL-Divergence å¯ä»¥ç®€åŒ–ä¸ºï¼š $$ D_{KL}(q ,|, p)\n_{i=1}^{d} $$\n\\[\nD_{KL}(q(\\mathbf{z}) \\ \\| \\  \\mathcal{N}(0, I)) =\n\\frac{1}{2}\n\\sum_{i=1}^{d}\n\\left(\n\\mu_{q,i}^2 + \\sigma_{q,i}^2 - \\log \\sigma_{q,i}^2 - 1\n\\right)\n\\]\néœ€è¦æ³¨æ„çš„ä¸€ä¸ªç‚¹æ˜¯ï¼ŒKL-Divergenceæ˜¯ä¸å¯¹ç§°çš„ï¼Œ \\[\nD_{KL}(Q \\| P) \\neq D_{KL}(P \\| Q)\n\\]\nä¸ºä»€ä¹ˆæˆ‘ä»¬è¦å¼ºè°ƒè¿™ä¸€ç‚¹ï¼Œæ˜¯å› ä¸ºï¼š å¯¹äºä¸åŒä½ç½®çš„Qï¼ŒPæˆ‘ä»¬æ‰€æ±‚çš„æ˜¯ä¸ä¸€æ ·çš„ï¼Œç®€å•æ¥è¯´ï¼Œå°±æ˜¯ç”¨ \\(\\|\\) åé¢çš„åˆ†å¸ƒï¼Œæ¥approximate \\(\\|\\) å‰é¢çš„åˆ†å¸ƒï¼Œå…·ä½“å¦‚ä¸‹å›¾ã€‚"
  },
  {
    "objectID": "posts/PapersWithCode/06-vae/VAE.html#variational-inference",
    "href": "posts/PapersWithCode/06-vae/VAE.html#variational-inference",
    "title": "06: Auto-Encoding Variational Bayes(VAE)",
    "section": "1.4 Variational Inference",
    "text": "1.4 Variational Inference\nVariational Inferenceï¼ˆVIï¼Œå˜åˆ†æ¨æ–­ï¼‰æ˜¯ä¸€ç§ç”¨å¯ä¼˜åŒ–ã€æ˜“è®¡ç®—çš„åˆ†å¸ƒæ¥è¿‘ä¼¼ä¸€ä¸ªéš¾ä»¥æ±‚è§£çš„åéªŒåˆ†å¸ƒï¼Œé€šè¿‡æœ€å°åŒ–ä¸¤è€…ä¹‹é—´çš„ KL è·ç¦»ï¼Œä»è€Œå®ç°é«˜æ•ˆæ¦‚ç‡æ¨æ–­çš„æ–¹æ³•ã€‚ åœ¨ Latent Variable æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬æƒ³æ±‚ï¼š \\[\np (\\mathrm{z} | \\mathrm{x}) = \\frac{p(\\mathrm{x}, \\mathrm{z})}{p(\\mathrm{x})}\n\\]\né€šå¸¸ \\(p(\\mathrm{x})\\) æ˜¯ä¸å¯æ±‚çš„ï¼Œå› ä¸º: \\[\np(\\mathrm{x}) = \\int p(\\mathrm{x}, \\mathrm{z}) d\\mathrm{z}\n\\] é€šå¸¸æ˜¯ä¸å¯èƒ½ç›´æ¥æ±‚çš„ã€‚å› æ­¤æˆ‘ä»¬é‡‡å–ä¸€ç§ â€œæ›²çº¿æ•‘å›½â€ çš„æ–¹å¼ï¼š æˆ‘ä»¬ä¸å»è®¡ç®—çœŸæ­£çš„åéªŒï¼Œè€Œæ˜¯æ‰¾ä¸€ä¸ª å¯è®¡ç®—å¹¶ä¸”å¯ä¼˜åŒ–çš„åˆ†å¸ƒæ¥è¿‘ä¼¼å®ƒï¼š \\[\nq(z | x) \\approx p(z | x)\n\\] å®ç°èµ·æ¥ä¹Ÿæ˜¯å¾ˆç®€å•çš„ 1. é€‰ä¸€ä¸ªå¯è®¡ç®—çš„åˆ†å¸ƒæ— ï¼ˆVariational Familyï¼‰ 2. è®©å®ƒå°½å¯èƒ½çš„æ¥è¿‘çœŸå®çš„åéªŒï¼š \\[\n\\underset{\\phi}{\\min}  D_{KL}(q_{\\phi}(z | x ) \\| p(z | x))\n\\]\nç›´è§‚çš„æ¥è¯´ï¼ŒVariational Inference å°±æ˜¯ï¼šä¸è¿‡ä¸æ–­æ‹‰å‡ï¼Œæ—‹è½¬ï¼Œä¸€ä¸ªæ¤­åœ†å‹ \\(q\\), æ¥ä½¿å®ƒå°½å¯èƒ½çš„å¯ä»¥å’Œäº‘æœµå½¢çŠ¶çš„ \\(p\\) æ¥é‡å "
  },
  {
    "objectID": "posts/PapersWithCode/06-vae/VAE.html#re-parameterization-trick",
    "href": "posts/PapersWithCode/06-vae/VAE.html#re-parameterization-trick",
    "title": "06: Auto-Encoding Variational Bayes(VAE)",
    "section": "2.1 Re-parameterization Trick",
    "text": "2.1 Re-parameterization Trick\n å¦‚ä¸Šå›¾å¯è§ï¼Œ\\(z\\) æ˜¯ç”± \\(x, \\phi\\) å†³å®šçš„ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œé‚£ä¹ˆæˆ‘ä»¬æ”¹å–æ€ä¹ˆæ ·çš„ \\(g_{\\phi}(x, \\epsilon)\\) å‘¢ã€‚æ–‡ç« ä¸­ç»™å‡ºäº†3ä¸ªåŸºæœ¬çš„æ–¹æ³•ï¼š 1. æ–¹æ³•1 2. 2 3. c\n\n\n\n\n\n\nREINFORCE\n\n\n\nå¯¹äº Re-Parametrization Trickæ˜¯é’ˆå¯¹ \\(z\\) æ˜¯ Continuousçš„æƒ…å†µï¼Œå¦‚æœ \\(z\\) æ˜¯ç¦»æ•£ï¼ˆDiscrete) çš„ é‚£ä¹ˆæˆ‘ä»¬åˆ™å¯ä»¥ä½¿ç”¨ REINFOCE çš„æ–¹æ³•ã€‚"
  },
  {
    "objectID": "posts/PapersWithCode/06-vae/VAE.html#amortized-inference",
    "href": "posts/PapersWithCode/06-vae/VAE.html#amortized-inference",
    "title": "06: Auto-Encoding Variational Bayes(VAE)",
    "section": "2.2 Amortized Inference",
    "text": "2.2 Amortized Inference\nè¿˜æœ‰ä¸€ä¸ªæ¯”è¾ƒå®¹æ˜“è¢«å¿½ç•¥çš„ä¸€ç‚¹å°±æ˜¯ï¼ŒVAE è¿˜è¿ç”¨äº†Amortized Inferenceã€‚ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿ\nä¼ ç»Ÿçš„å˜åˆ†æ¨æ–­ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸€ä¸ªè§‚æµ‹æ ·æœ¬ x å•ç‹¬ä¼˜åŒ–ä¸€ä¸ªå˜åˆ†åˆ†å¸ƒ q(z|x) çš„å‚æ•°ï¼Œè¿™æ„å‘³ç€æ¯æ¥ä¸€ä¸ªæ–°æ ·æœ¬éƒ½è¦é‡æ–°åšä¸€éå˜åˆ†ä¼˜åŒ–ï¼Œæˆæœ¬éå¸¸é«˜ã€‚è€Œåœ¨ VAE ä¸­ï¼Œæˆ‘ä»¬ä¸å†ä¸ºæ¯ä¸ªæ ·æœ¬å•ç‹¬ä¼˜åŒ–åéªŒï¼Œè€Œæ˜¯è®­ç»ƒä¸€ä¸ª å…±äº«çš„æ¨æ–­ç½‘ç»œï¼ˆEncoderï¼‰ æ¥é¢„æµ‹ q_(z|x) çš„å‚æ•°ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¨¡å‹é€šè¿‡å­¦ä¹ ä¸€ä¸ªå‡½æ•° f_(x) æ¥ä¸€æ¬¡æ€§â€œæ‘Šé”€â€æ‰€æœ‰æ ·æœ¬çš„æ¨æ–­æˆæœ¬ï¼Œä½¿å¾—å¯¹ä»»æ„æ–°æ ·æœ¬ xï¼Œåªéœ€ä¸€æ¬¡å‰å‘ä¼ æ’­å°±èƒ½å¾—åˆ°è¿‘ä¼¼åéªŒï¼Œä¸å†éœ€è¦æ˜‚è´µçš„ per-sample ä¼˜åŒ–ã€‚è¿™ç§æ–¹å¼æå¤§åœ°æå‡äº†æ¨æ–­æ•ˆç‡ï¼Œä¹Ÿè®©å˜åˆ†æ¨æ–­èƒ½å¤Ÿåœ¨æ·±åº¦å­¦ä¹ è§„æ¨¡ä¸Šè½åœ°ã€‚\nä¹Ÿå°±æ˜¯ç”¨ç¥ç»ç½‘ç»œæ¥ä¸€æ¬¡æ€§å­¦ä¹ åéªŒ\n å°†ä¸Šé¢çš„å‡ ä¸ªç»“åˆèµ·æ¥ï¼Œæˆ‘ä»¬å°±å¾—åˆ°çš„äº† Auto-Encoding VB Algorithmã€‚\n## å’Œ AutoEncoderçš„å…³ç³»"
  },
  {
    "objectID": "posts/PapersWithCode/06-vae/VAE.html#experiements",
    "href": "posts/PapersWithCode/06-vae/VAE.html#experiements",
    "title": "06: Auto-Encoding Variational Bayes(VAE)",
    "section": "2.3 Experiements",
    "text": "2.3 Experiements\nVAE Loss can be defined as this one:\nclass VAELoss(nn.Module):\n    def __init__(self, rec_loss=\"bce\", kl_beta=1.0):\n        super().__init__()\n\n        self.rec_loss = rec_loss.lower()\n        self.kl_beta = kl_beta\n\n        self.eval()\n\n    def forward(self, x, x_recon, mu, logvar):\n        B = x.shape[0]\n        if self.rec_loss == \"bce\":\n            rec = F.binary_cross_entropy(x_recon, x, reduction=\"sum\")\n        elif self.rec_loss == \"mse\":\n            rec = F.mse_loss(x_recon, x, reduction=\"sum\")\n\n        # KL divergence: D_KL(q(z|x) || p(z))\n        kl = 0.5 * torch.sum(logvar.exp() + mu.pow(2) - logvar - 1)\n\n        total = (rec + self.kl_beta * kl) / B\n\n        return total, {\n            \"recon\": rec.detach().cpu() / B,\n            \"kl\": kl.detach().cpu() / B,\n        }\nThe Loss curve and illustration examples:"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "1 Swin-Transformer\n  \n  1.1 Window Multi-Head-Attention (W-MHA)\n  1.2 Shifted Window Multi-Head-Attention (SW-MHA)\n  1.3 Consecutive Swin Transformer Block\n  1.4 Patch Merge\n  1.5 Relative Position Encoding\n  \n  1.5.1 Fine-Tuning in different image size\n  \n  1.6 Others\n  1.7 Downstream Tasks\n  \n  1.7.1 Image Classification\n  \n  1.8 Object Detection & Semantic segmentation\n  1.9 Training Details\n  \n  1.9.1 DropPath\n  1.9.2 Gradient Checkpoint\n  \n  1.10 Swin V2\n  \n  1.10.1 Post normalization\n  1.10.2 Scaled cosine attention\n  1.10.3 Log-spaced Continuous Position Bias(Log-CPB)\n  \n  \n  2 Summary\n  3 Key Concept Check Table\n  4 Q & A\n  5 Related resource & Further Reading"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#window-multi-head-attention-w-mha",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#window-multi-head-attention-w-mha",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "1.1 Window Multi-Head-Attention (W-MHA)",
    "text": "1.1 Window Multi-Head-Attention (W-MHA)\nW-MHA çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š\n\næŠŠå›¾åƒåˆ’åˆ†æˆå›ºå®šå¤§å°çš„çª—å£ï¼ˆwindowï¼‰ï¼Œæ¯”å¦‚ 7Ã—7 patch çš„çª—å£ã€‚\nåœ¨çª—å£å†…çš„ token ä¹‹é—´åšå±€éƒ¨è‡ªæ³¨æ„åŠ›ï¼Œè€Œä¸æ˜¯åœ¨æ•´å¼ å›¾åƒçš„æ‰€æœ‰ token ä¹‹é—´åšå…¨å±€æ³¨æ„åŠ›ã€‚\næ¯ä¸ªçª—å£ç‹¬ç«‹è®¡ç®— Multi-Head Attention â†’ é™ä½è®¡ç®—é‡ï¼Œå¹¶ä¸”æˆ‘ä»¬å¯ä»¥å¹¶è¡Œçš„è®¡ç®—\n\nè¿™æ ·ä¸€æ¥ï¼š\n\nå•ä¸ªçª—å£ token æ•°é‡å›ºå®š = \\(M^{2}\\)ï¼ˆå¦‚ 7Ã—7=49ï¼‰ã€‚\næ³¨æ„åŠ›è®¡ç®—å¤æ‚åº¦ä» \\(\\mathcal{O}((hw)^{2}C)\\) é™ä½ä¸º \\(\\mathcal{O}(M^{2}hwC)\\)ï¼Œå…¶ä¸­ \\(M \\ll \\sqrt{ N }\\)ã€‚\n\né™¤äº†é™ä½è®¡ç®—å¤æ‚åº¦ä¹‹å¤–ï¼ŒW-MHAï¼Œè¿˜æœ‰ä¿ç•™CNN åœ¨å›¾åƒå¤„ç†ä¸­å¼ºå¤§çš„ä¸€ç‚¹æ˜¯ å±€éƒ¨æ„Ÿå—é‡ å’Œ å¹³ç§»ä¸å˜æ€§ã€‚\n\nW-MHA é€šè¿‡çª—å£é™åˆ¶ï¼Œä½¿å¾—æ³¨æ„åŠ›æœºåˆ¶ä¹Ÿå…·å¤‡ç±»ä¼¼çš„å±€éƒ¨å½’çº³åç½®ï¼ˆinductive biasï¼‰ï¼Œé€‚åˆå›¾åƒå»ºæ¨¡ã€‚\n\n\nFor efficient modeling, we propose to compute self-attention within local windows. The windows are arranged to evenly partition the image in a non-overlapping manner.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.4"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#shifted-window-multi-head-attention-sw-mha",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#shifted-window-multi-head-attention-sw-mha",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "1.2 Shifted Window Multi-Head-Attention (SW-MHA)",
    "text": "1.2 Shifted Window Multi-Head-Attention (SW-MHA)\nW-MHA å¾ˆå¥½ï¼Œä½†æ˜¯å®ƒå­˜åœ¨çš„ä¸€ä¸ªé—®é¢˜å°±æ˜¯ï¼š\n\nçª—å£ä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œç¼ºå°‘è·¨çª—å£çš„ä¿¡æ¯äº¤æµã€‚è¿™ä¼šå¯¼è‡´ï¼Œæ¨¡å‹åªèƒ½çœ‹è§å±€éƒ¨ï¼Œä¸èƒ½è·å¾—å…¨å±€çš„ä¿¡æ¯ã€‚\n\n\nThe window-based self-attention module lacks connections across windows, which limits its modeling power. To introduce cross-window connections while maintaining the efficient computation of non-overlapping windows, we propose a shifted window partitioning approach which alternates between two partitioning configurations in consecutive Swin Transformer blocks.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.4 \n\nä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒSwin- Transformeræå‡ºæ¥ Shifted Window Mulit-Head-Attention (SW-MHA) çª—å£ä½ç½®ç›¸å¯¹å‰ä¸€å±‚å¹³ç§»ï¼Œæ¯”å¦‚ 7Ã—7 çª—å£ â†’ å¹³ç§» 3 ä¸ª patchã€‚ è¿™æ ·ï¼Œæ–°çš„çª—å£ä¼šè·¨è¶ŠåŸæ¥çš„è¾¹ç•Œï¼Œtoken ä¼šå’Œç›¸é‚»çª—å£çš„ token ä¸€èµ·è®¡ç®—æ³¨æ„åŠ›ã€‚ ç›¸å½“äºå¼ºåˆ¶è·¨çª—å£äº¤äº’ï¼Œè®©ä¿¡æ¯å¯ä»¥åœ¨ä¸åŒåŒºåŸŸä¹‹é—´æµåŠ¨ã€‚\n å¦‚ä¸Šå¦‚æ‰€ç¤ºï¼Œæˆ‘ä»¬å°†Windowé€šè¿‡å‘å·¦ä¸Šè§’ç§»åŠ¨ï¼Œé€šè¿‡ç»™å›¾ç‰‡å¢åŠ Paddingæ¥ï¼Œä½†æ˜¯è¿™ç§åŠæ³•æ˜¾ç„¶ä¼šå¢åŠ è®¡ç®—çš„å¤æ‚åº¦ã€‚Swin Transformerç”¨äº†ä¸€ç§å¾ˆèªæ˜çš„åŠæ³•ï¼Œå«åš Cycling Shiftï¼Œè¿™ç§æ–¹æ³•å°±æ˜¯å°†å°†ä¸€ä¸ªå¼ é‡æˆ–å›¾åƒåœ¨æŸä¸ªç»´åº¦ä¸Šåš å¹³ç§»ï¼Œä½†ä¸æ˜¯æŠŠç§»å‡ºå»çš„éƒ¨åˆ†ä¸¢æ‰ï¼Œè€Œæ˜¯ é‡æ–°ä»å¦ä¸€è¾¹è¡¥å›æ¥ã€‚å°±åƒâ€œç¯å½¢é˜Ÿåˆ—â€æˆ–â€œé’Ÿè¡¨èµ°ä¸€åœˆåˆå›åˆ°èµ·ç‚¹â€ã€‚ å¦‚ä¸‹å›¾æ‰€ç¤º \nå¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡Cycling Shiftï¼Œæˆ‘ä»¬å¾—åˆ°çš„æ¯ä¸ªwindowçš„å†…å®¹ï¼Œå’Œä¹‹å‰æ˜¯ä¸€æ ·çš„ï¼Œä½†æ˜¯æ‰€éœ€è¦çš„Windowçš„æ•°é‡ï¼Œå°äº†å¾ˆå¤šï¼Œè¿™ä¹Ÿå°±æ„å‘³ç€ï¼Œæ‰€éœ€è¦çš„æ—¶é—´å¤æ‚åº¦ï¼Œä¹Ÿå°äº†å¾ˆå¤šã€‚\n\nä¸è¿‡Cycling Shiftä¹Ÿæœ‰ä¸€ä¸ªé—®é¢˜ï¼Œå°±æ˜¯åŒä¸€ä¸ªçª—å£é‡Œé¢ï¼Œå¯èƒ½æœ‰æ¥è‡ªä¸åŒå›¾ç‰‡çš„ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯åœ¨åŸå›¾ç‰‡ä¸Šä¸æ˜¯ç›¸é‚»çš„ï¼Œè‡ªç„¶ä¸åº”è¯¥ç›¸äº’äº¤æµä¿¡æ¯ã€‚æˆ‘ä»¬å¯ä»¥å°†å›¾ç‰‡ï¼ŒæŠ½è±¡æˆä¸‹å›¾çš„å½¢å¼ã€‚ç»„ç»‡Attentionäº¤æµï¼Œå¾ˆè‡ªç„¶çš„ä¸€ç§æ–¹æ³•æ˜¯åˆ©ç”¨Maskï¼Œå°±åƒTransformeré‡Œçš„Causal Maskä¸€æ ·ã€‚ä½†æ˜¯ï¼Œè¿™ä¸ªMaské•¿ä»€ä¹ˆæ ·å­å‘¢\n\næˆ‘ä»¬å¯ä»¥çœ‹ä¸€ä¸‹Maskï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæœ‰é¢œè‰²çš„åŒºåŸŸè¡¨ç¤ºMask == 1ï¼Œ åœ¨æ­¤ä¸ºäº†æ›´å¥½çš„"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#consecutive-swin-transformer-block",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#consecutive-swin-transformer-block",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "1.3 Consecutive Swin Transformer Block",
    "text": "1.3 Consecutive Swin Transformer Block\n\nSwin Transformer is built by replacing the standard multi-head self attention (MSA) module in a Transformer block by a module based on shifted windows (described in Section 3.2), with other layers kept the same. As illustrated in Figure 3(b), a Swin Transformer block consists of a shifted window based MSA module, followed by a 2-layer MLP with GELU nonlinearity in between. A LayerNorm (LN) layer is applied before each MSA module and each MLP, and a residual connection is applied after each module  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.4 \n\n\n\\[\n\\begin{split}\n\\hat{z}^l &= \\text{W-MSA} \\left( \\text{LN} \\left( z^{l-1} \\right) \\right) + z^{l-1} \\\\\nz^l &= \\text{MLP} \\left( \\text{LN} \\left( \\hat{z}^l \\right) \\right) + \\hat{z}^l \\\\\n\\hat{z}^{l+1} &= \\text{SW-MSA} \\left( \\text{LN} \\left( z^l \\right) \\right) + z^l \\\\\nz^{l+1} &= \\text{MLP} \\left( \\text{LN} \\left( \\hat{z}^{l+1} \\right) \\right) + \\hat{z}^{l+1}\n\\end{split}\n\\]\nå°†W-MSA å’Œ SW-MSAå åœ¨ä¸€èµ·ï¼Œå°±å¾—åˆ°äº†Transformer Blockï¼Œå½“ç„¶ï¼Œè¿˜æœ‰ä¸€ä¸ªMLPï¼ŒLayer Normalizationï¼Œåœ¨æ­¤å°±ä¸èµ˜è¿°äº†ã€‚"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#patch-merge",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#patch-merge",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "1.4 Patch Merge",
    "text": "1.4 Patch Merge\nè®²å®Œäº†W-MHAï¼Œå’ŒSW-MHAï¼Œæˆ‘ä»¬å°±ç†è§£äº†Swin- Transformerä¸­æœ€éš¾ç†è§£ï¼Œä¹Ÿæ˜¯æœ€ç»ˆçš„éƒ¨åˆ†ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬çœ‹çœ‹å…¶ä»–ç®€å•çš„éƒ¨åˆ†ã€‚ Patch Merge , å›¾ä¸­ç»¿è‰²çš„éƒ¨åˆ†ï¼Œé€æ­¥é™ä½ token æ•°é‡ï¼ˆé™é‡‡æ ·ï¼‰ï¼ŒåŒæ—¶å¢åŠ ç‰¹å¾ç»´åº¦çš„æ“ä½œã€‚è¿™ç±»ä¼¼äºCNNä¸­çš„æ“ä½œï¼Œéšç€å±‚æ•°çš„å¢åŠ ï¼Œåˆ†è¾¨ç‡é€æ­¥é™ä½ã€é€šé“æ•°é€æ­¥å¢åŠ ï¼Œè¿™æ ·æ—¢å‡å°‘äº†è®¡ç®—é‡ï¼Œåˆèƒ½æå–å±‚çº§ç‰¹å¾ã€‚å…·ä½“çš„å®ç°ï¼š\n\nåˆ†ç»„ï¼šå°†ç›¸é‚»çš„ 2Ã—2 patch åˆå¹¶æˆä¸€ä¸ªæ–°çš„ patchã€‚\n\nå‡è®¾è¾“å…¥ç‰¹å¾å¤§å°ä¸º (H, W, C)ã€‚\næ¯ 2Ã—2 çš„ patch â†’ åˆå¹¶ä¸º 1 ä¸ªæ–° tokenã€‚\næ–°ç‰¹å¾å›¾å¤§å°å˜ä¸º (H/2, W/2, 4C)ã€‚\n\nçº¿æ€§å˜æ¢:\n\nå°†åˆå¹¶åçš„ 4C ç»´ç‰¹å¾é€šè¿‡ä¸€ä¸ª çº¿æ€§å±‚ (Linear Projection)ï¼Œé™åˆ° 2C ç»´ã€‚\nè¾“å‡ºç»´åº¦ç¿»å€ï¼ˆ2Cï¼‰ï¼Œä»¥è¡¥å¿åˆ†è¾¨ç‡å‡åŠå¸¦æ¥çš„ä¿¡æ¯æŸå¤±ã€‚ ğŸ”¹ ä¸ºä»€ä¹ˆæå‡º Patch Merging\n\n\nåˆ†å±‚è¡¨ç¤º (Hierarchical Representation) â€¢ æ¨¡ä»¿ CNN çš„é‡‘å­—å¡”ç»“æ„ï¼Œä»å±€éƒ¨ç»†èŠ‚é€æ­¥èšåˆåˆ°å…¨å±€è¯­ä¹‰ã€‚ â€¢ æœ‰åˆ©äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆæ£€æµ‹ã€åˆ†å‰²ï¼‰ä¸­ä¸åŒå°ºåº¦çš„ç›®æ ‡å»ºæ¨¡ã€‚\nè®¡ç®—æ•ˆç‡ â€¢ token æ•°é‡é€å±‚å‡å°‘ â†’ Attention çš„å¤æ‚åº¦å¤§å¹…ä¸‹é™ã€‚ â€¢ ä¿è¯æ¨¡å‹å¯æ‰©å±•åˆ°å¤§åˆ†è¾¨ç‡å›¾åƒã€‚\nè¯­ä¹‰ä¿¡æ¯èšåˆ â€¢ é€šè¿‡åˆå¹¶ç›¸é‚» patchï¼Œæ¨¡å‹èƒ½æŠŠæ›´å¤§æ„Ÿå—é‡çš„ä¿¡æ¯æ•´åˆåˆ°æ–°çš„ token ä¸­ã€‚\n\n\nx = x.view(B, H, W, C)\n\nx0 = x[:, 0::2, 0::2, :]  # (B, H/2, W/2, C)\nx1 = x[:, 1::2, 0::2, :]  # (B, H/2, W/2, C)\nx2 = x[:, 0::2, 1::2, :]  # (B, H/2, W/2, C)\nx3 = x[:, 1::2, 1::2, :]  # (B, H/2, W/2, C)\n\nx = torch.cat([x0, x1, x2, x3], -1)  # (B, H/2, W/2, 4*C)\nx = x.view(B, -1, 4 * C)  # (B, H/2*W/2, 4*C)\n\nx = self.reduction(x)  # (B, H/2*W/2, 2*C)"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#relative-position-encoding",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#relative-position-encoding",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "1.5 Relative Position Encoding",
    "text": "1.5 Relative Position Encoding\nä¸Transformer å’Œ Vision-Transformer ä¸­ä¸åŒçš„æ˜¯ï¼ŒSwin Transformeråˆ©ç”¨çš„æ˜¯Relative Position Encodingã€‚\n\\[\n\\text{Attention}(Q, K, V) = \\text{Softmax}\\left( \\frac{QK^{T}}{\\sqrt{ d }} +B\\right) V\n\\]\n1.  å®šä¹‰åç½®è¡¨ (relative_position_bias_table)\nâ€¢   å¤§å°æ˜¯ (2*Wh-1) * (2*Ww-1, num_heads)\nâ€¢   æ„å‘³ç€çª—å£å†…çš„ä»»æ„ä¸¤ä¸ª token çš„ç›¸å¯¹ä½ç½® (dx, dy)ï¼Œéƒ½æœ‰ä¸€ä¸ªå¯å­¦ä¹ çš„åç½®å€¼ï¼ˆæ¯ä¸ª head ä¸€ä»½ï¼‰ã€‚\nâ€¢   ä¾‹å¦‚çª—å£æ˜¯ 7Ã—7 â†’ ç›¸å¯¹ä½ç½®èŒƒå›´æ˜¯ [-6,6]ï¼Œæ‰€ä»¥è¡¨å¤§å°æ˜¯ 13Ã—13=169ï¼Œæ¯ä¸ªä½ç½®å­˜ä¸€ç»„åç½®\n\n\n2.  è®¡ç®—ç›¸å¯¹ä½ç½®ç´¢å¼• (relative_position_index)\nâ€¢   é¦–å…ˆç”Ÿæˆçª—å£å†…æ¯ä¸ª token çš„åæ ‡ã€‚\nâ€¢   ç„¶ååšå·®ï¼Œå¾—åˆ°ä»»æ„ä¸¤ä¸ª token çš„ç›¸å¯¹åæ ‡ (dx, dy)ã€‚\nâ€¢   å†æ˜ å°„æˆè¡¨çš„ç´¢å¼•ï¼ˆé€šè¿‡ç§»ä½å’Œå“ˆå¸Œæˆä¸€ä¸ªæ•´æ•° indexï¼‰ã€‚\nâ€¢   ç»“æœæ˜¯ä¸€ä¸ª (Wh*Ww, Wh*Ww) çš„çŸ©é˜µï¼Œæ¯ä¸ªå…ƒç´ å­˜ä¸¤ä¸ª token ä¹‹é—´åœ¨ bias è¡¨é‡Œçš„ç´¢å¼•ã€‚\n\n\nâ€¢   åœ¨å›¾åƒé‡Œï¼Œç›¸å¯¹ä½ç½®æ¯”ç»å¯¹ä½ç½®æ›´é‡è¦ï¼š\nâ€¢   æ¯”å¦‚ä¸€ä¸ªåƒç´ çš„å·¦é‚»å’Œå³é‚»å¾ˆç›¸ä¼¼ï¼Œæ— è®ºè¿™ä¸ªåƒç´ åœ¨å›¾åƒçš„å“ªä¸ªåœ°æ–¹ã€‚\n\\[\n\\begin{tabular}\n\\Xhline{1.0pt}\n& \\multicolumn{2}{c|}{ImageNet} & \\multicolumn{2}{c|}{COCO} & \\multicolumn{1}{c}{ADE20k} \\\\\n& top-1 & top-5  & AP$^\\text{box}$ & AP$^\\text{mask}$ & mIoU \\\\\n\\hline\nno pos. & 80.1 & 94.9 & 49.2 & 42.6  & 43.8 \\\\\nabs. pos. & 80.5 & 95.2 & 49.0 & 42.4  & 43.2 \\\\\nabs.+rel. pos. & 81.3 & 95.6 & 50.2 & 43.4 & 44.0\\\\\nrel. pos. w/o app. & 79.3 & 94.7 & 48.2 & 41.9 & 44.1 \\\\\nrel. pos. & \\textbf{81.3} & \\textbf{95.6} & \\textbf{50.5} & \\textbf{43.7} & \\textbf{46.1} \\\\\n\\Xhline{1.0pt}\n\\end{tabular}\n\\]\n\n1.5.1 Fine-Tuning in different image size\nå’Œ Vision-Transformer ä¸€æ ·ï¼Œå½“è¾“å…¥çš„å›¾ç‰‡å’Œè®­ç»ƒæ—¶ä¸ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ bi-cubic interpolation æ¥å¢å¤§Relative Position\n\nThe learnt relative position bias in pre-training can be also used to initialize a model for fine-tuning with a different window size through bi-cubic interpolation  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.5 \n\n\\[\n\\begin{table}\n    \\centering\n\\small\n\\addtolength{\\tabcolsep}{-4.0pt}\n\\begin{tabular}\n\\Xhline{1.0pt}\n\\multirow{2}{*}{method} & \\multicolumn{4}{c|}{MSA in a stage (ms)} & \\multicolumn{3}{c}{Arch. (FPS)} \\\\\n& S1 & S2 & S3 & S4 & T & S & B \\\\\n\\hline\nsliding window (naive) & 122.5 & 38.3 & 12.1 & 7.6 & 183 & 109 & 77 \\\\\nsliding window (kernel)  & 7.6 & 4.7 & 2.7 & 1.8 & 488 & 283 & 187 \\\\\n\\hline\nPerformer~\\cite{choromanski2020performer} & 4.8 & 2.8 & 1.8 & 1.5 & 638 & 370 & 241 \\\\\n\\hline\nwindow (w/o shifting) & 2.8 & 1.7 & 1.2 & 0.9 & 770 & 444 & 280 \\\\\n\\hline\nshifted window (padding) & 3.3 & 2.3 & 1.9 & 2.2 & 670 & 371 & 236 \\\\\nshifted window (cyclic)  & 3.0 & 1.9 & 1.3 & 1.0 & 755 & 437 & 278 \\\\\n\\Xhline{1.0pt}\n\\end{tabular}\n    \\caption{Real speed of different self-attention computation methods and implementations on a V100 GPU. }\n    \\label{tab:ablation-selfatt-efficient}\n\\normalsize\n\\end{table}\n\\]"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#others",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#others",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "1.6 Others",
    "text": "1.6 Others\né™¤äº†ä»¥ä¸Šå‡ ä¸ªï¼ŒSwin Transformer ä¸­è¿˜æœ‰å…¶ä»–Componentï¼Œæ¯”å¦‚ ï¼š\n\nPatch Embedding\nLinear Projection\nFeedForward\nLayer Normalization åœ¨æ­¤ï¼Œå°±ä¸èµ˜è¿°äº†ï¼Œæœ‰éœ€è¦çš„åŒå­¦ï¼Œè¯·å‚è€ƒå‰ä¸€ç¯‡ Vision-Transformerï¼Œ æˆ–è€… Transformer"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#downstream-tasks",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#downstream-tasks",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "1.7 Downstream Tasks",
    "text": "1.7 Downstream Tasks\nå½“ä¸€åœºå›¾ç‰‡ä¼ å…¥Swin Transformerï¼Œ å®ƒå¯ä»¥æå–å‡ºå›¾ç‰‡çš„ç‰¹å¾ã€‚ \\[\n\\mathrm{z} = f_{\\theta}(\\mathrm{x}), \\quad \\text{where}\\ \\mathrm{x} \\in \\mathbb{R}^{3\\times H \\times W}, \\mathrm{z} \\in \\mathbb{R}^{H'W' \\times C}\n\\]\nä¸€å¼ å›¾ç‰‡è½¬åŒ–æˆäº† \\(H'W'\\) ä¸ªç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾çš„å¤§å°ä¸º $Cã€‚\nSwin Transformer å¯ä»¥æœ‰å½“ä½œåŸºæœ¬çš„backboneï¼Œåœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥å¯¹ä¸‹æ¸¸è¿›è¡Œä¸åŒçš„ä»»åŠ¡ï¼Œæ¯”å¦‚ï¼š\n\nImage Classification\nObject Detection\nSemantic segmentation\n\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å¦‚ä½•ç”¨Swin Transformeråœ¨ä¸åŒçš„ä»»åŠ¡ä¸­\n\n\n1.7.1 Image Classification\nå¯¹äº \\(\\mathrm{z}\\) çš„ hidden statesï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œä¸€ä¸ªAverage Poolingï¼Œå¯¹äºæ¯ä¸€ä¸ªç‰¹å¾æ±‚å‡å€¼ï¼Œç„¶åå†å°†è¿™ä¸ªä¼ å…¥ä¸€ä¸ªåˆ†ç±»å¤´ï¼Œå°±å¯ä»¥å¾—åˆ°æˆ‘ä»¬Classificationäº†ã€‚ä¸ Vision-Transformer ä¸åŒçš„æ˜¯ï¼ŒSwin Transformer æ²¡æœ‰ [CLS] token æ¥å½“æ”¶é›†å…¨éƒ¨çš„ä¿¡æ¯ã€‚"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#object-detection-semantic-segmentation",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#object-detection-semantic-segmentation",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "1.8 Object Detection & Semantic segmentation",
    "text": "1.8 Object Detection & Semantic segmentation\nBackbone (Swin Transformer)ï¼š\n\nStage 1: [N, C1, H/4, W/4]\nStage 2: [N, C2, H/8, W/8]\nStage 3: [N, C3, H/16, W/16]\nStage 4: [N, C4, H/32, W/32]\n\nå¯ä»¥å¾—åˆ° FPN(Lin et al. 2017) to create a pyramid of feature maps suitable for detection.\n{P2: [N, C, H/4, W/4], \n P3: [N, C, H/8, W/8], \n P4: [N, C, H/16, W/16], \n P5: [N, C, H/32, W/32]}\næœ‰äº†è¿™äº›FPN ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥ç»“åˆä¸åŒçš„ç®—æ³•ï¼Œæ¥è¿›è¡Œä¸åŒçš„ä»»åŠ¡ï¼Œæ¯”å¦‚\nä¾‹å­ 1ï¼šç›®æ ‡æ£€æµ‹ (Object Detection)\nä»¥ Swin Transformer + Faster R-CNN (Ren et al. 2016) ä¸ºä¾‹ï¼š 1. è¾“å…¥å›¾åƒï¼šä¸€å¼  800Ã—1333 çš„ COCO æ•°æ®é›†å›¾ç‰‡ã€‚\n3.  FPN (ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œ)ï¼šå°†å¤šå°ºåº¦ç‰¹å¾èåˆï¼Œå½¢æˆç»Ÿä¸€çš„é‡‘å­—å¡”ç‰¹å¾ã€‚\n4.  RPN (Region Proposal Network)ï¼šåœ¨ç‰¹å¾å›¾ä¸Šç”Ÿæˆå€™é€‰åŒºåŸŸã€‚\n5.  RoI Headï¼šå¯¹å€™é€‰åŒºåŸŸè¿›è¡Œåˆ†ç±» (è½¦ã€äººã€ç‹—â€¦) å’Œè¾¹æ¡†å›å½’ã€‚\n6.  è¾“å‡ºï¼šé¢„æµ‹ç»“æœï¼Œä¾‹å¦‚ï¼š\nâ€¢   â€œä¸€è¾†è½¦â€ â†’ è¾¹æ¡† (x1,y1,x2,y2) + ç±»åˆ« â€œcarâ€\nâ€¢   â€œä¸€ä¸ªäººâ€ â†’ è¾¹æ¡† + ç±»åˆ« â€œpersonâ€\n ğŸ‘‰ åœ¨ COCO æ•°æ®é›†ä¸Šï¼ŒSwin-T + Faster R-CNNæ¯” ResNet-50 + Faster R-CNN çš„ mAP æé«˜çº¦ 5~6 ä¸ªç‚¹ã€‚\nè¯­ä¹‰åˆ†å‰² (Semantic Segmentation)  ä»¥ Swin Transformer + UPerNet(Xiao et al. 2018)ä¸ºä¾‹ï¼š 1. è¾“å…¥å›¾åƒï¼šä¸€å¼  512Ã—512 çš„ ADE20K æ•°æ®é›†å›¾ç‰‡ã€‚ 2. Backbone (Swin Transformer)ï¼šåŒæ ·è¾“å‡º 1/4, 1/8, 1/16, 1/32 å››ä¸ªå°ºåº¦ç‰¹å¾ã€‚ 3. FPN/UPerNet Headï¼š â€¢ å°†å¤šå±‚ç‰¹å¾èåˆï¼Œå¯¹åº”ä¸åŒè¯­ä¹‰å±‚çº§ã€‚ â€¢ åˆ©ç”¨èåˆåçš„ç‰¹å¾ç”Ÿæˆåƒç´ çº§é¢„æµ‹ã€‚ 4. é¢„æµ‹å›¾ (segmentation map)ï¼šå¤§å° 512Ã—512ï¼Œæ¯ä¸ªåƒç´ å±äºä¸€ä¸ªç±»åˆ«ã€‚ â€¢ [0,0] åƒç´  â†’ â€œskyâ€ â€¢ [100,150] åƒç´  â†’ â€œbuildingâ€ â€¢ [200,300] åƒç´  â†’ â€œroadâ€ 5. è¾“å‡ºï¼šå®Œæ•´çš„è¯­ä¹‰åˆ†å‰²å›¾ï¼Œæ¯ä¸ªåƒç´ éƒ½æœ‰ç±»åˆ«æ ‡ç­¾ã€‚\nğŸ‘‰ åœ¨ ADE20K ä¸Šï¼ŒSwin-L + UPerNet çš„ mIoU è¾¾åˆ° 53.5+ï¼Œæ¯”ä¼ ç»Ÿ CNN backbone æå‡æ˜¾è‘—ã€‚ å…·ä½“çš„å®ç°ç»†èŠ‚ï¼Œç­‰åˆ°ä»¥åæˆ‘ä»¬é˜…è¯»åˆ°å…³äºSegmentationçš„å†…å®¹åœ¨ï¼Œå†æ¥å®ç°"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#training-details",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#training-details",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "1.9 Training Details",
    "text": "1.9 Training Details\n\nWe employ an AdamW optimizer for 300 epochs using a cosine decay learning rate scheduler and 20 epochs of linear warm-up. A batch size of 1024, an initial learning rate of 0.001, and a weight decay of 0.05 are used. We include most of the augmentation and regularization strategies of in training, except for repeated augmentation and EMA, which do not enhance performance.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.5 \n\n\n1.9.1 DropPath\nè®ºæ–‡ä¸­è¿˜ç”¨åˆ°äº† DropPath æ¥å½“ä½œä¸€ç§ Regularizationã€‚ DropPath ä¹Ÿç§°ä¹‹ä¸º Stochastic Depth (Huang et al. 2016) , å®ƒæ˜¯ä¸€ç§åº”ç”¨åœ¨Residual Networkï¼Œ åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œéšæœºä¸¢å¼ƒæ•´ä¸ª æ®‹å·®åˆ†æ”¯ (residual branch) æˆ– æ•´ä¸ªè·¯å¾„ (path)ã€‚å‡å°‘è¿‡æ‹Ÿåˆï¼ŒåŒæ—¶è®©æ¨¡å‹å­¦ä¼šä¾èµ–ä¸åŒæ·±åº¦çš„è·¯å¾„ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§ã€‚ \nä¸Dropout ä¸åŒçš„æ˜¯ï¼Œ Dropout ä¸¢å¼ƒçš„æ˜¯å•ä¸ªç¥ç»å…ƒçš„è¾“å‡ºï¼Œ è€ŒDropPath ä¸¢å¼ƒçš„æ˜¯æ•´ä¸ªæ®‹å·®åˆ†æ”¯ / æ•´å±‚ Block\n\n\n\n\n\n\n\n\nç‰¹æ€§\nDropout (ç»å…¸)\nDropPath (Stochastic Depth)\n\n\n\n\nä¸¢å¼ƒå¯¹è±¡\nå•ä¸ªç¥ç»å…ƒçš„è¾“å‡º\næ•´ä¸ªæ®‹å·®åˆ†æ”¯ / æ•´å±‚ Block\n\n\nåº”ç”¨ç²’åº¦\né€å…ƒç´  (element-wise)\nå±‚çº§ (layer-wise)\n\n\nä½¿ç”¨åœºæ™¯\nå…¨è¿æ¥å±‚ã€CNNã€RNN ç­‰\næ®‹å·®ç½‘ç»œã€Transformer ç­‰\n\n\næ¨ç†é˜¶æ®µæ•ˆæœ\nä¸ä¸¢å¼ƒï¼Œä½¿ç”¨ç¼©æ”¾è¡¥å¿\nä¸ä¸¢å¼ƒï¼Œä¿ç•™å®Œæ•´è·¯å¾„\n\n\nä½œç”¨\nå‡å°‘ç¥ç»å…ƒè¿‡æ‹Ÿåˆ\né˜²æ­¢æ·±å±‚ç½‘ç»œè¿‡æ‹Ÿåˆã€æå‡ç¨³å®šæ€§\n\n\n\ndef drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    output = x.div(keep_prob) * random_tensor\n    return output\n\n\nclass DropPath(nn.Module):\n    def __init__(self, drop_prob=None):\n        super().__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n        \n\nclass SwinTransformerBlock(nn.Module):\n    def __init__(self):\n        ...\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else nn.Identity()\n        \n    \n    def forward(self, x):\n        shortcut = x \n        ... # Attention \n        x = shortcut + self.drop_path(x)\n        \n        shortcut = x \n        ... # FFN\n        x = shortcut + self.drop_path(x)\n        \n        ...\n        \n        return x \n\nğŸ“ TAKEAWAY DropPathï¼ˆä¹Ÿå« Stochastic Depthï¼‰æ˜¯ä¸€ç§æ­£åˆ™åŒ–æ–¹æ³•ï¼Œå®ƒåœ¨è®­ç»ƒæ—¶éšæœºè·³è¿‡ï¼ˆä¸¢å¼ƒï¼‰æ•´ä¸ªç½‘ç»œå±‚æˆ–åˆ†æ”¯çš„è®¡ç®—ï¼Œä»¥å‡å°‘è¿‡æ‹Ÿåˆå¹¶æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚\n\n\n\n1.9.2 Gradient Checkpoint\nåœ¨æ­¤ï¼Œæˆ‘ä»¬åœ¨ä»‹ç»ä¸€ä¸ªè®­ç»ƒæ–¹æ³•ï¼Œç”¨äºåŠ é€Ÿè®­ç»ƒï¼Œå«åšGradient Checkpointåˆå«åšActivation Checkpointï¼Œ ç”¨PyTorhå®ç°ï¼Œæ˜¯å¾ˆå®¹æ˜“çš„ çš„ï¼Œæˆ‘ä»¬åªéœ€è¦call utils.checkpoint\næ­£å¸¸è®­ç»ƒæµç¨‹ï¼š åœ¨å‰å‘ä¼ æ’­ï¼ˆforwardï¼‰æ—¶ï¼Œæ¯ä¸€å±‚çš„ä¸­é—´æ¿€æ´»å€¼ï¼ˆactivationï¼‰éƒ½ä¼šä¿å­˜ä¸‹æ¥ï¼Œä»¥ä¾¿åå‘ä¼ æ’­ï¼ˆbackwardï¼‰æ—¶ç”¨æ¥è®¡ç®—æ¢¯åº¦ã€‚ é—®é¢˜æ˜¯ï¼šä¿å­˜æ‰€æœ‰ä¸­é—´æ¿€æ´»å€¼ä¼šæ¶ˆè€—å¤§é‡æ˜¾å­˜ï¼ˆGPU memoryï¼‰ã€‚ â€¢ Gradient Checkpoint çš„æ€è·¯ï¼š å¹¶ä¸æ˜¯ä¿å­˜æ‰€æœ‰æ¿€æ´»å€¼ï¼Œè€Œæ˜¯åªåœ¨éƒ¨åˆ†å…³é”®èŠ‚ç‚¹ï¼ˆcheckpointï¼‰ä¿å­˜æ¿€æ´»ã€‚ å¯¹äºæœªä¿å­˜çš„æ¿€æ´»å€¼ï¼Œåœ¨åå‘ä¼ æ’­æ—¶é‡æ–°å†è·‘ä¸€æ¬¡å‰å‘è®¡ç®—æ¥å¾—åˆ°å®ƒä»¬ï¼Œä»è€ŒèŠ‚çœæ˜¾å­˜ã€‚\næ¢å¥è¯è¯´ï¼šç”¨è®¡ç®—æ¢æ˜¾å­˜ã€‚\nğŸ”¹ å·¥ä½œæœºåˆ¶ 1. åœ¨å‰å‘ä¼ æ’­æ—¶ï¼š â€¢ æ¨¡å‹è¢«åˆ‡åˆ†æˆè‹¥å¹²å—ï¼ˆsegmentsï¼‰ã€‚ â€¢ åªä¿å­˜æ¯ä¸€å—çš„è¾“å…¥ï¼Œä¸¢å¼ƒä¸­é—´çš„æ¿€æ´»ã€‚ 2. åœ¨åå‘ä¼ æ’­æ—¶ï¼š â€¢ éœ€è¦ç”¨åˆ°æ¢¯åº¦æ—¶ï¼Œé‡æ–°å¯¹é‚£ä¸€å—åšä¸€æ¬¡ forward æ¥æ¢å¤æ¿€æ´»ã€‚ â€¢ ç„¶åæ­£å¸¸è®¡ç®—æ¢¯åº¦ã€‚\nâ€¢   å¢åŠ è®¡ç®—å¼€é”€ï¼šå› ä¸ºè¦åœ¨ backward æ—¶é‡æ–°åšä¸€æ¬¡ forwardã€‚\nâ€¢   ä¸€èˆ¬ä¼šå¸¦æ¥ 20%ï½30% é¢å¤–çš„è®­ç»ƒæ—¶é—´ã€‚\nimport torch\nfrom torch import nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(1024, 1024)\n        self.layer2 = nn.Linear(1024, 1024)\n\n    def forward(self, x):\n        def custom_forward(*inputs):\n            return self.layer2(self.layer1(*inputs))\n        \n        # å¯¹è¿™éƒ¨åˆ†ä½¿ç”¨ checkpoint\n        x = checkpoint(custom_forward, x)\n        return x\n\nğŸ“ TAKEAWAY Gradient Checkpointing æ˜¯ä¸€ç§ ç”¨é¢å¤–è®¡ç®—æ¢æ˜¾å­˜ çš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨å‰å‘ä¼ æ’­æ—¶å°‘å­˜æ¿€æ´»ï¼Œåå‘ä¼ æ’­æ—¶é‡ç®—ï¼Œèƒ½è®©å¤§æ¨¡å‹åœ¨æœ‰é™æ˜¾å­˜ä¸‹å®Œæˆè®­ç»ƒã€‚"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#swin-v2",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#swin-v2",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "1.10 Swin V2",
    "text": "1.10 Swin V2\nâ€œSwin V2â€ (Liu et al. 2022) æ˜¯åœ¨åŸå§‹ Swin Transformer çš„åŸºç¡€ä¸Šï¼Œä¸ºäº†æ›´å¥½åœ° æ‰©å±•æ¨¡å‹å®¹é‡ï¼ˆæ›´å¤šå‚æ•°ï¼‰ã€å¤„ç†é«˜åˆ†è¾¨ç‡è¾“å…¥ ä»¥åŠ æé«˜è®­ç»ƒç¨³å®šæ€§ æ‰€åšçš„ä¸€ç³»åˆ—æ”¹è¿›ã€‚ åœ¨è§†è§‰ä»»åŠ¡ä¸­ï¼ŒTransformer æ¨¡å‹è‹¥è¦å˜å¾—æ›´å¼ºï¼ˆæ›´å¤šå‚æ•°ã€æ›´é«˜åˆ†è¾¨ç‡è¾“å…¥ã€æ›´å¤šå±‚æ•°ï¼‰å°±ä¼šé‡åˆ°å‡ ä¸ªæŒ‘æˆ˜ï¼š 1. è®­ç»ƒä¸ç¨³å®šï¼šéšç€æ¨¡å‹å˜æ·±ã€é€šé“å˜å®½ï¼Œå†…éƒ¨æ¿€æ´»çš„å¹…åº¦å¯èƒ½æ€¥å‰§å¢é•¿ï¼Œå¯¼è‡´æ¢¯åº¦ã€æ•°å€¼ä¸ç¨³å®šã€‚ 2. åˆ†è¾¨ç‡è¿ç§»é—®é¢˜ï¼šæ¨¡å‹åœ¨ä½åˆ†è¾¨ç‡ä¸‹é¢„è®­ç»ƒï¼ˆä¾‹å¦‚ 224Ã—224ï¼‰åï¼Œç”¨åœ¨é«˜åˆ†è¾¨ç‡ï¼ˆä¾‹å¦‚ 1,536Ã—1,536ï¼‰æˆ–ä¸åŒçª—å£å°ºå¯¸æ—¶è¡¨ç°ä¼šä¸‹é™ã€‚ 3. å¯¹æ ‡æ³¨æ•°æ®çš„è¿‡åº¦ä¾èµ–ï¼šå¤§æ¨¡å‹éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®æ‰èƒ½è®­ç»ƒå¾—å¥½ã€‚\nSwin V2 å°±æ˜¯ä¸ºäº†å…‹æœè¿™äº›éšœç¢ï¼Œæ”¯æŒè®­ç»ƒè¶…å¤§æ¨¡å‹ï¼ˆå¦‚ 30 äº¿å‚æ•°çº§åˆ«ï¼‰ï¼ŒåŒæ—¶èƒ½å¤„ç†å¤§å°ºå¯¸è¾“å…¥ \n\n1.10.1 Post normalization\nclass SwinTransformerBlock(nn.Module):\n    def __init__(self):\n        ...\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else nn.Identity()\n        \n    \n    def forward(self, x):\n        shortcut = x \n        ... # Attention \n        x = shortcut + self.drop_path(self.norm1(x))\n        \n        shortcut = x \n        ... # FFN\n        x = x + self.drop_path(self.norm2(self.mlp(x)))\n        \n        ...\n        \n        return x \n\n\n1.10.2 Scaled cosine attention\nclass WindowAttention:\n    def __init__(self,):\n    \n        ...\n        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)\n        ...\n    \n    \n    def forward(self, x):\n        attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1))\n        logit_scale = torch.clamp(self.logit_scale, max=torch.log(torch.tensor(1. / 0.01))).exp()\n        attn = attn * logit_scale\n\n\n1.10.3 Log-spaced Continuous Position Bias(Log-CPB)\nlog-spaced continuous position bias approach to address the issue in transferring across window resolutions\n\\[\n\\begin{split}\n\\widehat{\\Delta x} &= \\operatorname{sign}(x) \\cdot \\log(1 + |\\Delta x|) \\\\\n\\widehat{\\Delta y} &= \\operatorname{sign}(y) \\cdot \\log(1 + |\\Delta y|)\n\\end{split}\n\\]\nself.cpb_mlp = nn.Sequential(nn.Linear(2, 512, bias=True),\n                             nn.ReLU(inplace=True),\n                             nn.Linear(512, num_heads, bias=False))\n                             \ndef forward(self, x):\n    relative_position_bias_table = self.cpb_mlp(self.relative_coords_table).view(-1, self.num_heads)"
  },
  {
    "objectID": "posts/PapersWithCode/17 VQ-VAE/VQ_VAE.html",
    "href": "posts/PapersWithCode/17 VQ-VAE/VQ_VAE.html",
    "title": "VQ-VAE",
    "section": "",
    "text": "1 Preliminary\n  \n  1.1 Vector Quantization\n  1.2 Straight Through Estimator\n  \n  2 VQ_VAE\n  3 Summary\n  4 Key Concepts\n  5 Q & A\n  6 Related resource & Further Reading\n  7 VQ_VAE\n  8 Summary\n  9 Key Concepts\n  10 Q & A\n  11 Related resource & Further Reading"
  },
  {
    "objectID": "posts/PapersWithCode/17 VQ-VAE/VQ_VAE.html#vector-quantization",
    "href": "posts/PapersWithCode/17 VQ-VAE/VQ_VAE.html#vector-quantization",
    "title": "VQ-VAE",
    "section": "1.1 Vector Quantization",
    "text": "1.1 Vector Quantization\nå‘é‡é‡åŒ–ï¼ˆVector Quantizationï¼‰æ˜¯ä¸€ç§æŠŠè¿ç»­çš„å‘é‡è½¬æ¢ä¸ºç¦»æ•£çš„â€œç´¢å¼•â€ çš„æ–¹æ³•ã€‚é€šè¿‡è¿™ä¸ªç´¢å¼•ï¼Œåœ¨å­—å…¸ï¼ˆcodebookï¼‰ä¸­æ‰¾åˆ°ä¸€ä¸ªä¸å…¶æœ€ç›¸è¿‘çš„ä¸€ä¸ªå‘é‡ï¼Œè¿™ä¸ªå­—å…¸ä¹Ÿå«åšï¼š - CodeBook - Embedding Table - Centroids è¿™æ—¶ï¼Œæ¯ä¸ªå‘é‡å°±å˜æˆäº†ä¸€ä¸ªç¦»æ•£çš„ç¼–å·ï¼ˆIndexï¼‰ ç”¨æ•°å­¦è¡¨è¾¾å°±æ˜¯ï¼š æˆ‘ä»¬æœ‰: - A vector \\(z \\in \\mathbb{R}^{d}\\) - A codebook \\(E \\in \\mathbb{R}^{K \\times d}\\), å…¶ä¸­ æœ‰ \\(K\\) ä¸ªç´¢å¼•\næˆ‘ä»¬é€šè¿‡æ¯”è¾ƒ \\(z\\) å’Œ \\(K\\) ä¸ªå‘é‡ä¸­ï¼Œæ‰¾åˆ°æœ€è¿‘çš„ä¸€ä¸ªå‘é‡\n\\[\n\\text{quantized}(z) = e_{k} \\quad \\text{where}\\ k = \\underset{j}{\\operatorname{arg\\min}}   \\|z - e_{j} \\|^{2}\n\\]\nimport torch\n\ndef quantize(embedding_table: torch.Tensor, z: torch.Tensor):\n    \"\"\"\n    embedding_table: (K, D)\n    z: (B, D)\n    \"\"\"\n    # (B, 1, D) - (1, K, D) â†’ (B, K, D)\n    diff = z.unsqueeze(1) - embedding_table.unsqueeze(0)\n\n    # (B, K, D) -&gt; (B, K)\n    distances = torch.linalg.norm(diff, dim=2)\n\n    #  (B, K) -&gt; (B,)\n    indices = distances.argmin(dim=1)\n\n    # Gather quantized embeddings â†’ (B, D)\n    q = embedding_table[indices]\n\n    return q, indices\n\n\nK, D = 512, 64\nB = 8\n\ncodebook = torch.randn(K, D)\nz = torch.randn(B, D)\n\nq, idx = quantize(codebook, z)\nassert q.shape == (B, D)\nassert idx.shape == (B,)"
  },
  {
    "objectID": "posts/PapersWithCode/17 VQ-VAE/VQ_VAE.html#straight-through-estimator",
    "href": "posts/PapersWithCode/17 VQ-VAE/VQ_VAE.html#straight-through-estimator",
    "title": "VQ-VAE",
    "section": "1.2 Straight Through Estimator",
    "text": "1.2 Straight Through Estimator\nK, D = 512, 64\nB = 8\n\nembedding_table = torch.randn(K, D)\nz_enc = torch.randn(B, D, requires_grad=True)\nassert z_enc.grad is None\n\nz_k, _ = quantize(embedding_table, z_enc)\n\n# STE\nz_k = z_enc + (z_k - z_enc).detach()\nz_k.retain_grad()\n\nloss = (z_k**2).mean()\nloss.backward()\n\nassert z_k.grad is not None\nassert z_enc.grad is not None\nassert torch.allclose(z_enc.grad, z_k.grad)"
  },
  {
    "objectID": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html",
    "href": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html",
    "title": "Vision-Transformer",
    "section": "",
    "text": "1 Preliminary\n  2 Vision-Transformer\n  3 Summary\n  4 Key Concepts\n  5 Q & A\n  6 Related resource & Further Reading\n  7 Preliminary\n  8 Vision-Transformer\n  \n  8.1 Patch Embedding\n  8.2 Position Encoding\n  \n  8.2.1 Extending Position Encoding\n  \n  8.3 [CLS] Tokens & MLP Head\n  8.4 Transformer Encoder Block\n  8.5 CNN vs.Â ViTï¼š Inductive bias\n  8.6 ViT Model Variants\n  \n  9 Q&A\n  10 æ‰©å±•\n  \n  10.1 å‡å°‘Tokens\n  10.2 Vision Language Model\n  \n  11 Appendix\n  \n  11.1 Axial Attentionï¼ˆè½´å‘æ³¨æ„åŠ›ï¼‰\n  \n  12 Summary\n  13 Key Concepts\n  14 Q & A\n  15 Related resource & Further Reading"
  },
  {
    "objectID": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#patch-embedding",
    "href": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#patch-embedding",
    "title": "Vision-Transformer",
    "section": "8.1 Patch Embedding",
    "text": "8.1 Patch Embedding\n\nThe standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image \\(x \\in \\mathbb{R}^{H \\times W \\times C}\\) into a sequence of flattened 2D patches\\(x \\in \\mathbb{R}^{N \\times (P^{2} \\times C)}\\), where (\\(H, W\\)) is the resolution of the original image, \\(C\\) is the number of channels, (\\(P, P\\)) is the resolution of each image patch, and \\(N = HW/P^{2}\\) is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size \\(D\\) through all of its layers, so we flatten the patches and map to \\(D\\) dimensions with a trainable linear projection. We refer to the output of this projection as the patch embeddings.  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.3 \n\nåœ¨Transformer è¿™ä¸€ç¯‡ï¼Œæˆ‘ä»¬äº†è§£åˆ°ï¼Œå®ƒæ˜¯ä½œç”¨äºSequence Modelingçš„ï¼Œå¾ˆæ˜¾ç„¶ï¼ŒImage ä¸æ˜¯ Sequenceçš„ã€‚å¾ˆç›´è§‚çš„ç¬¬ä¸€ç§æƒ³æ³•å°±æ˜¯ï¼Œå°†å›¾ç‰‡ç›´æ¥å±•å¼€ï¼Œä»äºŒç»´ (\\(3, H, W\\)) å±•å¼€æˆä¸€ç»´çš„ (\\(3, H \\times W\\)). è¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°çš„å›¾ç‰‡çš„Sequence Modelã€‚å¦‚ä¸‹å›¾@fig-flat-imageæ‰€ç¤º\n\n\n\n\n\n\nFigureÂ 2\n\n\n\nè¿™ç§æ–¹æ³•æœ‰ä¸€ç§æ˜æ˜¾çš„é—®é¢˜å°±æ˜¯ï¼šSequenceçš„é•¿åº¦å¤ªé•¿ï¼Œä¸¾ä¸ªä¾‹å­ï¼Œå¯¹äº \\(3\\times 256 \\times 256\\) çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬æœ‰ \\(256 \\times 256 = 65,336\\) ä¸ªtokensï¼Œé€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæ‰€éœ€è¦çš„è®­ç»ƒæ—¶é•¿å¾ˆé•¿ã€‚å¹¶ä¸”å®ƒæ²¡æœ‰ç”¨åˆ°å›¾ç‰‡çš„ä¸€ä¸ªç‰¹æ€§ï¼šç›¸é‚»çš„pixel ä¹‹é—´ï¼Œæ˜¯æœ‰å¾ˆé«˜çš„correlationçš„ã€‚æ‰€ä»¥æˆ‘ä»¬å¾ˆè‡ªç„¶çš„æƒ³åˆ°ï¼šå¦‚æœæŠŠç›¸é‚»çš„pixelså’Œåœ¨ä¸€ç»„ï¼Œç»„æˆä¸€ä¸ªpatchï¼Œè¿™æ ·ä¸å°±æ—¢å‡å°‘äº†tokensçš„æ•°é‡ï¼Œåˆç”¨åˆ°äº†pixelä¹‹é—´çš„correlationã€‚è¿™å°±æ˜¯Vision Transformer çš„Patch Embeddingã€‚ è¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°äº†ã€‚ æ¥ä¸‹æ¥æˆ‘ä»¬åªéœ€è¦ç”¨ï¼Œä¸€ä¸ªMLPï¼Œå°†æˆ‘ä»¬å±•å¼€çš„patchï¼Œæ˜ å°„åˆ° \\(D\\)- dimensionçš„ç©ºé—´ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ä¼ å…¥Transformer æ¨¡å‹äº†ã€‚\næ¥ä¸‹æ¥æˆ‘ä»¬æ¥çœ‹çœ‹ä»£ç æ€ä¹ˆå®ç°ï¼š\n# Load Image and resize it to certain size\nimage_path = IMAGE_PATN\nimg_bgr = cv2.imread(image_path)\nimg_resized = cv2.resize(img_bgr, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA)\nimg = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB) \n\n# Patchify \npatches = einops.rearrange(\n     img, \"(h ph) (w pw) c -&gt; (h w) ph pw c\", ph=PATCH_SIZE, pw=PATCH_SIZE\n)\né€šè¿‡è¿™ä¸ªPatchifyåªæœ‰ï¼Œæˆ‘ä»¬å¾—åˆ°å°†å›¾ç‰‡Patchåˆ°äº†\n\n\nåˆ†æˆäº†ä¸åŒçš„å°Patchã€‚\næ¥ä¸‹æ¥æˆ‘ä»¬è¦åšçš„å°±æ˜¯ï¼Œå°†è¿™äº›Patch å±•å¼€ï¼Œç„¶åä¼ å…¥ä¸€ä¸ªMLPï¼Œ\nflat_patch = einops.rearrange(\n     patches, \"n ph pw c -&gt; n (ph pw c)\"\n)\n\n\nmlp = nn.Linear(PATCH_SIZE * PATCH_SIZE * 3, d_model)\npatch_embedding = mlp(flat_patch)\nåŒè¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å°±å¯ä»¥è§å›¾ç‰‡è½¬åŒ–ä¸ºTransformerå¯ä»¥æ¥å—çš„vectorã€‚ä¸è¿‡åœ¨å®é™…æ“ä½œä¸­ï¼Œæˆ‘ä»¬å¹¶ä¸ä¼šç”¨ä»¥ä¸Šçš„æ–¹å¼ï¼Œå› ä¸ºä¸Šé¢çš„æ–¹å¼å®ç°èµ·æ¥æ¯”è¾ƒæ…¢ï¼Œæˆ‘ä»¬å¯ä»¥å°†Patch å’Œ Linear Projectå’Œåœ¨ä¸€èµ·ã€‚\n\n\n\n\n\n\nTip\n\n\n\nå°†å‡ ä¸ªtensor çš„operationæ“ä½œåˆæˆä¸€ä¸ªçš„æ–¹æ³•ï¼Œå«åškernel fusionï¼Œè¿™æ˜¯ä¸€ç§æé«˜è®­ç»ƒå’Œæ¨ç†ç´ çš„æ–¹æ³•\n\n\nåœ¨å®é™…çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬ç”¨Convolution Layer ä»£æ›¿ Patch + Flatten+ Linear çš„æ–¹æ³•. å¦‚æœæˆ‘ä»¬ç”¨ä¸€ä¸ª å·ç§¯å±‚ï¼Œå‚æ•°è®¾ç½®ä¸ºï¼š â€¢ kernel_size = PATCH_SIZE ï¼ˆå·ç§¯æ ¸è¦†ç›–ä¸€ä¸ª patchï¼‰ â€¢ stride = PATCH_SIZE ï¼ˆä¸é‡å åœ°ç§»åŠ¨ï¼Œç›¸å½“äºåˆ‡ patchï¼‰ â€¢ in_channels = 3ï¼ˆRGBï¼‰ â€¢ out_channels = d_model\né‚£ä¹ˆå·ç§¯ä¼šï¼š 1. æŠŠè¾“å…¥å›¾ç‰‡åˆ†æˆ PATCH_SIZE x PATCH_SIZE çš„ä¸é‡å å—ï¼ˆå› ä¸º stride = kernel_sizeï¼‰ã€‚ 2. å¯¹æ¯ä¸ª patch åšä¸€æ¬¡çº¿æ€§æ˜ å°„ï¼ˆå› ä¸ºå·ç§¯æœ¬è´¨ä¸Šå°±æ˜¯å¯¹å±€éƒ¨åŒºåŸŸåšåŠ æƒæ±‚å’Œï¼Œç›¸å½“äº Linearï¼‰ã€‚ 3. è¾“å‡ºçš„ shape è‡ªåŠ¨å°±æ˜¯ (batch, num_patches, d_model)ã€‚\nè¿™æ­£å¥½ç­‰ä»·äº åˆ‡ patch + flatten + Linear çš„ç»„åˆã€‚\nä»£ç å¦‚ä¸‹ï¼š\nclass PatchEmbedding(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.conv = nn.Conv2d(\n            in_channels=config.num_channels,\n            out_channels=config.hidden_dim,\n            kernel_size=config.patch_size,\n            stride=config.patch_size,\n            padding=\"valid\" if config.patch_size == 16 else \"same\",\n        )\n\n    def forward(self, imgs: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        imgs: (batch_size, num_channels, height, width)\n        Returns: (batch_size,  num_patches_height, num_patches_width, hidden_dim)\n        \"\"\"\n        # (B, C, H, W) -&gt; (B, hidden_dim, H', W')\n        x = self.conv(imgs)\n\n        # (B, hidden_dim, H', W') -&gt; (B, hidden_dim, H' * W')\n        x = x.flatten(2)\n\n        # (B, hidden_dim, H' * W') -&gt; (B, H' * W', hidden_dim)\n        x = x.transpose(1, 2)\n        return x\nç”¨å·ç§¯çš„å¥½å¤„ï¼Œé™¤äº†å¯ä»¥æ›´é«˜æ•ˆçš„å®ç°Patch Embeddingï¼Œä»£ç æ›´åŠ ç®€æ´ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡æ”¹å˜ stride æ¥ä½¿ä¸€äº›Patch overlappingï¼Œè·å¾—ä¸€ä¸ªå¤šå°ºåº¦çš„ç»“æ„ï¼Œ"
  },
  {
    "objectID": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#position-encoding",
    "href": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#position-encoding",
    "title": "Vision-Transformer",
    "section": "8.2 Position Encoding",
    "text": "8.2 Position Encoding\nå°†å›¾ç‰‡è½¬åŒ–ä¸º Transformer çš„è¾“å…¥ä¹‹åï¼Œæ¥ä¸‹æ¥Transformerä¸­çš„å¦ä¸€ä¸ªç»„ä»¶å°±æ˜¯ä¼ å…¥ Position Informationã€‚æˆ‘ä»¬çŸ¥é“åœ¨Transformer ä¸­ï¼Œä»–ä»¬ç”¨çš„æ˜¯ sine-cosine position embeddingï¼Œåœ¨é‚£ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿæåˆ°äº†ï¼Œè¿˜å­˜åœ¨å…¶ä»–ä¸åŒçš„Position Encodingçš„åŠæ³•ï¼ŒViT ç”¨çš„å°±æ˜¯å¦ä¸€ç§åŠæ³•ï¼ŒLearned Position Embeddingã€‚Learned Position Embeddingçš„æ–¹æ³•å¾ˆç®€å•ï¼Œä¹Ÿå¾ˆå¥½ç†è§£ï¼Œå¯¹äºæ¯ä¸€ä¸ªä½ç½®ï¼Œæˆ‘ä»¬ç»™ä»–ä¸€ä¸ªindexï¼Œå°†è¿™ä¸ªindexä¼ å…¥ä¸€ä¸ª Embedding Matrixï¼Œ æˆ‘ä»¬å°±å¾—åˆ°ä¸€ä¸ªPosition Embeddingã€‚ä¸è¿‡ä¸Token Embeddingä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬ä¼šç”¨åˆ°æ‰€æœ‰çš„Positionï¼Œä¹Ÿæ•´ä¸ªmatrixï¼Œ æ‰€ä»¥æˆ‘ä»¬ä¸ç”¨å®šindexï¼Œç›´æ¥å®šä¹‰æ•´ä¸ªEmbeddingï¼Œç„¶åå°†å®ƒä¼ å…¥Transformerä¸­ã€‚\nclass PositionalEncoding(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.positional_embedding = nn.Parameter(\n            torch.randn(\n                1,\n                (config.image_size // config.patch_size) ** 2 + 1,\n                config.hidden_dim,\n            )\n        )\n\n        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_dim))\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        x: (batch_size, num_patches, hidden_dim)\n        Returns: (batch_size, num_patches, hidden_dim)\n        \"\"\"\n        # Add positional encoding to the input tensor\n        batch_size = x.size(0)\n\n        pos_embedding = self.positional_embedding.expand(batch_size, -1, -1)\n        cls_token = self.cls_token.expand(batch_size, -1, -1)\n\n        x = torch.cat((cls_token, x), dim=1)\n        return x + pos_embedding\nä¸ºä»€ä¹ˆViTè¦ç”¨Learned Position Embeddingå‘¢ï¼Ÿåœ¨ViTè¿™ç¯‡æ–‡ç« ä¸­ï¼Œä»–ä»¬å°è¯•è¿‡ä¸åŒçš„Position Embeddingï¼Œæ¯”å¦‚ï¼š\n\nNo Positional Information\n1-dimensional Positional Embedding\n2-dimensional Positional Embedding\nRelative Positional Embedding\n\nå‘ç°ï¼Œé™¤äº†No Positional Informationä¹‹å¤–ï¼Œå…¶ä½™3ç§åœ¨Image Classificationä¸­çš„è¡¨ç°ï¼Œéƒ½æ˜¯å·®ä¸å¤šçš„ã€‚\n\n\n\n\n\n\nFigureÂ 3\n\n\n\nè®ºæ–‡ä¸­è¡¨ç¤ºï¼Œå¯èƒ½æ˜¯å› ä¸ºæ‰€éœ€è¦çš„ Positionçš„ä¿¡æ¯è¾ƒå°ï¼Œå¯¹äºä¸åŒç§ç±»çš„Position Embeddingçš„æ–¹æ³•ï¼Œå­¦ä¹ è¿™ä¸ªPosition Informationçš„èƒ½åŠ›ï¼Œéƒ½æ˜¯å·®ä¸å¤šçš„ã€‚\n\nWe speculate that since our Transformer encoder operates on patch-level inputs, as opposed to pixel-level, the differences in how to encode spatial information is less important. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original pixel-level inputs, e.g., \\(14 \\times 14\\) as opposed to \\(224 \\times 224\\), and learning to represent the spatial relations in this resolution is equally easy for these different positional encoding strategies.  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.18 \n\nä¸è¿‡ï¼Œå°½ç®¡Positionçš„æ–¹æ³•ä¸é‡è¦ï¼Œä½†æ˜¯ä¸åŒçš„è®­ç»ƒå‚æ•°ï¼Œè¿˜æ˜¯ä¼šå½±å“åˆ°å­¦ä¹ åˆ°çš„Position Information, ä¸‹å›¾æ‰€ç¤ºï¼š\n\n\n\n\n\n\nFigureÂ 4\n\n\n\n\n8.2.1 Extending Position Encoding\nå½“æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªPre-Trainingçš„æ¨¡å‹ï¼Œæˆ‘ä»¬æƒ³ç”¨å®ƒFine-Tuningåˆ°ä¸€ä¸ªä¸åŒå›¾ç‰‡å¤§å°çš„æ•°æ®åº“ï¼Œæˆ‘ä»¬æ”¹æ€ä¹ˆåšå‘¢ï¼Œç¬¬ä¸€ä¸ªæ–¹æ³•å½“ç„¶æ˜¯ï¼ŒResize æˆ‘ä»¬çš„å›¾ç‰‡ï¼Œåˆ°ViT Pre-trainingçš„å›¾ç‰‡å¤§å°ï¼Œä½†æ˜¯ï¼Œè¿™ä¸ªèƒ½å¯¼è‡´è¾ƒå¤§çš„å›¾ç‰‡ï¼Œå¤±å»å¾ˆå¤šç»†èŠ‚ã€‚å¦‚æœæˆ‘ä»¬æƒ³ä¿æŒå›¾ç‰‡çš„å¤§å°ä¸å˜ï¼ŒåŒæ—¶è®©æ¨¡å‹è®­ç»ƒï¼Œæˆ‘ä»¬å°±éœ€è¦Extend Position Encodingï¼Œå› ä¸ºå½“Patch Sizeä¸å˜ï¼Œå›¾ç‰‡å¤§å°å˜äº†çš„è¯ï¼Œäº§ç”Ÿçš„Number of Patches ä¹Ÿæ˜¯ä¼šæ”¹å˜çš„ï¼Œè¿™æ ·ï¼Œå°±æ˜¯æŸå¤±ä¸€äº›ä¿¡æ¯ã€‚æˆ‘ä»¬éœ€è¦åšçš„æ˜¯ï¼Œæ‰¾åˆ°ä¸€ç§æ–¹æ³•ï¼Œå¢å¤§æˆ–è€…å‡å°Positionçš„æ•°é‡ã€‚ è¿™å°±æ˜¯æ‰€è°“çš„Position Interpolationã€‚\n2D interpolation of the pre-trained position embeddings â€¢ ViT åœ¨é¢„è®­ç»ƒæ—¶ï¼Œé€šå¸¸ç”¨å›ºå®šè¾“å…¥åˆ†è¾¨ç‡ï¼ˆæ¯”å¦‚ 224Ã—224ï¼‰ â†’ ç”Ÿæˆå›ºå®šæ•°é‡çš„ patchï¼ˆæ¯”å¦‚ 16Ã—16 patch â†’ 196 ä¸ª patchï¼‰ã€‚ â€¢ ä½†åœ¨ fine-tuning æ—¶ï¼Œè¾“å…¥å›¾ç‰‡å¯èƒ½å¤§å°ä¸ä¸€æ ·ï¼Œæ¯”å¦‚ 384Ã—384ï¼Œè¿™æ—¶ patch æ•°é‡å°±å˜äº†ã€‚ â€¢ è¿™ä¼šå¯¼è‡´åŸæœ¬çš„ ä½ç½®ç¼–ç  (position embeddings) å’Œæ–°çš„ patch æ•°é‡å¯¹ä¸ä¸Šã€‚ â€¢ è§£å†³åŠæ³•ï¼šå¯¹é¢„è®­ç»ƒå¥½çš„ä½ç½®ç¼–ç åš äºŒç»´æ’å€¼ (2D interpolation)ï¼Œæ ¹æ® patch åœ¨åŸå›¾ä¸­çš„ç©ºé—´ä½ç½®ï¼ŒæŠŠä½ç½®ç¼–ç æ‹‰ä¼¸/ç¼©æ”¾åˆ°æ–°çš„åˆ†è¾¨ç‡ã€‚\n\nThe Vision Transformer can handle arbitrary sequence lengths (up to memory constraints), however, the pre-trained position embeddings may no longer be meaningful. We therefore perform 2D interpolation of the pre-trained position embeddings, according to their location in the original image  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.4 \n\ndef interpolate_pos_encoding(self, x, w, h):\n    npatch = x.shape[1] - 1\n    N = self.pos_embed.shape[1] - 1\n    if npatch == N and w == h:\n        return self.pos_embed\n    class_pos_embed = self.pos_embed[:, 0]\n    patch_pos_embed = self.pos_embed[:, 1:]\n    dim = x.shape[-1]\n    w0 = w // self.patch_embed.patch_size\n    h0 = h // self.patch_embed.patch_size\n    \n    patch_pos_embed = F.interpolate(\n        patch_pos_embed.reshape(\n            1, \n            int(math.sqrt(N)), \n            int(math.sqrt(N)), \n            dim\n        ).permute(0, 3, 1, 2),\n        scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n        mode='bicubic',\n    )\n    patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n    return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)"
  },
  {
    "objectID": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#cls-tokens-mlp-head",
    "href": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#cls-tokens-mlp-head",
    "title": "Vision-Transformer",
    "section": "8.3 [CLS] Tokens & MLP Head",
    "text": "8.3 [CLS] Tokens & MLP Head\nåœ¨ Transformer è¿™ä¸€èŠ‚ï¼Œæˆ‘ä»¬äº†è§£åˆ°ï¼šæ¯è¾“å…¥ä¸€ä¸ªtokenï¼ŒTransformerä¼šè¾“å‡ºå¯¹åº”çš„tokenã€‚è¿™å°±æ˜¯è¯´ï¼Œå¯¹äºæ¯ä¸ªpatchï¼ŒTransformerä¼šè¾“å‡ºå¯¹åº”çš„Tokensï¼Œé‚£ä¹ˆï¼Œæˆ‘ä»¬åº”è¯¥é€‰æ‹©å“ªä¸€ä¸ªtokenä½œä¸ºæˆ‘ä»¬å›¾ç‰‡çš„è¡¨ç¤ºå‘¢ã€‚ BERT (Devlin et al. 2019)ï¼Œ ç”¨äº†ä¸€ä¸ª [CLS], æ¥è¡¨ç¤ºä¸€ä¸ªå¥å­ã€‚åŒç†ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æ·»åŠ ä¸€ä¸ª [CLS] token, æ¥è¡¨ç¤ºä¸€å¼ å›¾ç‰‡ã€‚åŒæ—¶ï¼Œå¯¹äº [CLS] token, æˆ‘ä»¬ä¹Ÿè¦åœ¨ç»™ä»–ä¸€ä¸ªè¡¨ç¤ºä½ç½®çš„ä¿¡æ¯ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåœ¨Position Encodingä¸Šï¼Œæˆ‘ä»¬æœ‰ (config.image_size // config.patch_size) ** 2 + 1, ä½ç½®ä¿¡æ¯ï¼Œå…¶ä¸­ +1 å°±æ˜¯ [CLS] çš„ä½ç½®ä¿¡æ¯ã€‚ æ€»ç»“ä¸€ä¸‹ [CLS] token çš„ä½œç”¨å°±æ˜¯ç”¨æ¥èšåˆæ‰€æœ‰çš„Patchçš„æ¶ˆæ¯ï¼Œç„¶åç”¨æ¥Image çš„Representationã€‚\næˆ‘ä»¬æƒ³ä¸€ä¸‹ï¼Œé™¤äº†åŠ ä¸€ä¸ª [CLS] tokenï¼Œä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æœ‰å…¶ä»–åŠæ³•æ¥è¡¨ç¤ºå›¾ç‰‡å—ã€‚æœ‰ä¸€ç§å¾ˆè‡ªç„¶çš„æ–¹æ³•å°±æ˜¯ï¼Œå°†æ‰€æœ‰çš„patchçš„æ¶ˆæ¯æ”¶é›†èµ·æ¥ï¼Œç„¶åå»ä¸€ä¸ªå¹³å‡å€¼æ¥è¡¨ç¤ºè¿™ä¸ªå›¾ç‰‡ã€‚ç±»ä¼¼äºä¼ ç»Ÿçš„ConvNet(e.g.Â ResNet) æˆ‘ä»¬å¯ä»¥é€šè¿‡ AvgPooling æ¥å®ç°ã€‚ ä¸è¿‡è®ºæ–‡ä¸­æåˆ°ï¼Œ å¯¹äºä¸¤ç§ä¸åŒçš„Image Representationï¼Œéœ€è¦æœ‰ä¸åŒçš„Learning Rate æ¥è®­ç»ƒè¿™ä¸ªç½‘ç»œã€‚\nOther content \næœ‰äº†Image Representä¹‹åï¼Œæˆ‘ä»¬åªéœ€è¦å°†è¿™ä¸ªä¼ å…¥ä¸€ä¸ªç®€å•çš„MLPï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°ä¸€ä¸ªClassifierã€‚MLPçš„è¾“å…¥æ˜¯hidden dimï¼Œè¾“å…¥åˆ™æ˜¯æˆ‘ä»¬Number of Classesã€‚ä¸åŒçš„Index è¡¨ç¤ºä¸åŒçš„Classsesã€‚\n\nAn initial attempt at using only image-patch embeddings, globally average-pooling (GAP) them, followed by a linear classifierâ€”just like ResNetâ€™s final feature mapâ€”performed very poorly. However, we found that this is neither due to the extra token, nor to the GAP operation. Instead, the difference in performance is fully explained by the requirement for a different learning-rate,  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.17 \n\n\nBoth during pre-training and fine-tuning, a classification head is attached to \\(\\mathrm{z}_{L}^{0}\\). The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time.  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.3 \n\nclass ClassifierHead(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n        \n        self.fc1 = nn.Linear(config.hidden_dim, config.mlp_dim)\n        self.fc2 = nn.Linear(config.mlp_dim, config.num_classes)\n        self.dropout = nn.Dropout(config.dropout_rate)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        x: (batch_size, num_patches, hidden_dim)\n        Returns: (batch_size, num_classes)\n        \"\"\"\n        # Use the CLS token for classification\n        cls_token = x[:, 0, :]\n        x = F.relu(self.fc1(cls_token))\n        \n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x"
  },
  {
    "objectID": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#transformer-encoder-block",
    "href": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#transformer-encoder-block",
    "title": "Vision-Transformer",
    "section": "8.4 Transformer Encoder Block",
    "text": "8.4 Transformer Encoder Block\nè‡³æ­¤ï¼Œæˆ‘ä»¬å·²ç»è®²å®Œäº†ViTä¸Transformerçš„ä¸»è¦ä¸åŒä¹‹å¤„ã€‚æ¥ä¸‹æ¥ï¼Œå°±æ˜¯Transformerçš„Encoderã€‚ \nè¿™éƒ¨åˆ†ï¼Œå’ŒTransformeråŸæœ¬çš„Encoderå¾ˆç±»ä¼¼ï¼Œåªä¸è¿‡æœ‰å‡ å¤„ä¸åŒï¼š\n\nPre-Norm: åœ¨ViTåŒï¼Œè¾“å…¥å…ˆè¿›è¡Œä¸€ä¸ªLayerNormï¼Œç„¶ååœ¨ä¼ å…¥MHAæˆ–è€…MLPä¸­ï¼Œåè§‚åœ¨TransformeråŸæœ¬çš„Encoderä¸­ï¼Œæˆ‘ä»¬æ˜¯å…ˆå°†MHAæˆ–è€…MLPçš„è¾“å‡ºä¸è¾“å…¥åŠ åœ¨ä¸€èµ·ï¼Œä¹‹åå†è¿›è¡Œä¸€ä¸ªNormalizationã€‚è¿™å«åšPost-Norm\nMLPçš„å®ç°ï¼šåœ¨Transformer Encoderä¸­ï¼Œç”¨çš„æ˜¯ ReLU, è€Œåœ¨ViTä¸­ï¼Œç”¨çš„æ˜¯ GELU\n\né™¤æ­¤ä¹‹å¤–ï¼Œå…¶ä»–éƒ¨åˆ†éƒ½æ˜¯ä¸€æ ·çš„ã€‚ä¸€ä¸‹æ˜¯ViT Encoderçš„å®ç°ï¼š\nclass EncoderBlock(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n        \n        self.norm1 = LayerNorm(config.hidden_dim)\n        self.mha = MHA(config)\n        \n        self.norm2 = LayerNorm(config.hidden_dim)\n        self.ffn = FFN(config)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        x: (batch_size, num_patches, hidden_dim)\n        Returns: (batch_size, num_patches, hidden_dim)\n        \"\"\"\n        # Multi-head attention\n        redisual = x\n        x = self.norm1(x)\n        x = redisual + self.mha(x)\n\n        # Feed-forward network\n        redisual = x\n        x = self.norm2(x)\n        x = x + self.ffn(x)\n\n        return x"
  },
  {
    "objectID": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#cnn-vs.-vit-inductive-bias",
    "href": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#cnn-vs.-vit-inductive-bias",
    "title": "Vision-Transformer",
    "section": "8.5 CNN vs.Â ViTï¼š Inductive bias",
    "text": "8.5 CNN vs.Â ViTï¼š Inductive bias\nè‡³æ­¤ï¼Œæˆ‘ä»¬å·²ç»ä»‹ç»å®Œäº†Vision Transformerï¼Œæˆ‘ä»¬æ¥ä»Inductive Bias çš„æ–¹é¢ï¼Œçœ‹çœ‹ CNN å’Œ ViT æœ‰ä»€ä¹ˆä¸åŒ\n\n\n\n\n\n\nä»€ä¹ˆæ˜¯Inductive Bias\n\n\n\nåœ¨æ·±åº¦å­¦ä¹ é‡Œï¼ŒInductive Biasï¼ˆå½’çº³åç½®ï¼‰æ˜¯æŒ‡æ¨¡å‹åœ¨å­¦ä¹ ä¹‹å‰ï¼Œå› ç»“æ„æˆ–è®¾è®¡è€Œè‡ªå¸¦çš„å‡è®¾æˆ–å…ˆéªŒã€‚\n\n\nå¯¹äºå›¾åƒæ¥è¯´ï¼Œå¸¸è§çš„å…ˆéªŒå°±æ˜¯ï¼š\n\nå±€éƒ¨åƒç´ æ˜¯ç›¸å…³çš„ï¼ˆlocalityï¼‰\nç›¸é‚»åŒºåŸŸçš„æ¨¡å¼æœ‰è§„å¾‹ï¼ˆ2D neighborhoodï¼‰\nç‰©ä½“æ— è®ºå‡ºç°åœ¨å›¾åƒå“ªé‡Œï¼Œè¯†åˆ«æ–¹å¼åº”è¯¥ä¸€æ ·ï¼ˆtranslation equivarianceï¼‰\n\nğŸ”¹ 2. CNN çš„ç»“æ„æ€ä¹ˆä½“ç°è¿™äº›åç½®ï¼Ÿ 1. å±€éƒ¨æ€§ (Locality) â€¢ å·ç§¯æ ¸ï¼ˆä¾‹å¦‚ 3Ã—3ï¼‰åªå’Œå±€éƒ¨åƒç´ æ‰“äº¤é“ï¼Œè€Œä¸æ˜¯å…¨å›¾ã€‚ â€¢ è¿™æ„å‘³ç€æ¨¡å‹â€œç›¸ä¿¡â€å›¾åƒçš„é‡è¦ç‰¹å¾æ¥è‡ªå±€éƒ¨é‚»åŸŸï¼Œè€Œä¸æ˜¯é¥è¿œåŒºåŸŸã€‚ 2. äºŒç»´é‚»åŸŸç»“æ„ (2D structure) â€¢ å·ç§¯æ“ä½œæ˜¯æ²¿ç€ å›¾åƒçš„äºŒç»´ç½‘æ ¼è¿›è¡Œçš„ï¼Œå¤©ç„¶åˆ©ç”¨äº†å›¾åƒçš„è¡Œåˆ—ç»“æ„ã€‚ â€¢ è¿™å’Œæ–‡æœ¬ï¼ˆåºåˆ— 1Dï¼‰ä¸ä¸€æ ·ï¼ŒCNN æ˜ç¡®çŸ¥é“è¾“å…¥æ˜¯ 2D æ’åˆ—çš„ã€‚ 3. å¹³ç§»ç­‰å˜æ€§ (Translation equivariance) â€¢ å·ç§¯æ ¸çš„å‚æ•°åœ¨æ•´å¼ å›¾å…±äº«ã€‚ â€¢ æ‰€ä»¥çŒ«åœ¨å·¦ä¸Šè§’è¿˜æ˜¯å³ä¸‹è§’ï¼Œå·ç§¯æ ¸éƒ½èƒ½æ£€æµ‹åˆ°â€œçŒ«è€³æœµâ€ã€‚ â€¢ è¿™è®© CNN è‡ªåŠ¨å…·æœ‰â€œè¯†åˆ«ä½ç½®æ— å…³â€çš„èƒ½åŠ›ã€‚\nè¿™äº›æ€§è´¨ä¸æ˜¯æ¨¡å‹é€šè¿‡è®­ç»ƒå­¦å‡ºæ¥çš„ï¼Œè€Œæ˜¯å› ä¸º å·ç§¯æ“ä½œæœ¬èº«çš„æ•°å­¦ç»“æ„å°±å¸¦æ¥çš„ï¼š â€¢ kernel çš„å±€éƒ¨è¿æ¥ â†’ å±€éƒ¨æ€§ â€¢ kernel æ»‘åŠ¨è¦†ç›–å…¨å›¾ â†’ å¹³ç§»ç­‰å˜æ€§ â€¢ æ“ä½œåœ¨äºŒç»´ç©ºé—´å®šä¹‰ â†’ é‚»åŸŸç»“æ„ â€¢ æ‰€ä»¥ï¼Œå“ªæ€•ä½ ä¸ç»™ CNN å–‚å¤ªå¤šæ•°æ®ï¼Œå®ƒä¹Ÿä¼šåˆ©ç”¨è¿™äº›åç½®å»å­¦ä¹ ç‰¹å¾ã€‚\nè€Œå¯¹äº ViT æ¥è¯´ï¼š ViT çš„å½’çº³åç½®éå¸¸å¼±ï¼Œå‡ ä¹å®Œå…¨ä¾èµ–æ•°æ®å’Œè®­ç»ƒæ¥å­¦ä¹ ã€‚\n\nPatch åˆ‡åˆ† (Patchification) â€¢ ViT å”¯ä¸€çš„â€œå›¾åƒå…ˆéªŒâ€ä¹‹ä¸€å°±æ˜¯æŠŠè¾“å…¥å›¾ç‰‡åˆ‡æˆ patchã€‚ â€¢ è¿™ä¸€æ“ä½œéšå«äº†ï¼šå›¾åƒæ˜¯ä¸€ä¸ªäºŒç»´ç»“æ„ï¼Œå¯ä»¥è¢«åˆ†å—å¤„ç†ã€‚\nä½ç½®ç¼–ç  (Positional Embeddings) â€¢ Transformer æœ¬èº«åªå¤„ç†åºåˆ—ï¼Œæ²¡æœ‰ç©ºé—´ç»“æ„çš„æ¦‚å¿µã€‚ â€¢ ViT é€šè¿‡åŠ ä½ç½®ç¼–ç å‘Šè¯‰æ¨¡å‹ patch åœ¨å›¾åƒä¸­çš„ç›¸å¯¹ä½ç½®ã€‚ â€¢ åœ¨è¾“å…¥åˆ†è¾¨ç‡å˜åŒ–æ—¶ï¼Œä¼šåš äºŒç»´æ’å€¼ (2D interpolation) æ¥é€‚é…ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ç§äººå·¥å¼•å…¥çš„ 2D å…ˆéªŒã€‚\nå…¶ä»–éƒ¨åˆ† â€¢ é™¤äº†ä»¥ä¸Šä¸¤ç‚¹ï¼ŒViT çš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯ å…¨å±€çš„ (global)ï¼Œæ²¡æœ‰å±€éƒ¨æ€§çº¦æŸã€‚ â€¢ æ²¡æœ‰åƒ CNN é‚£æ ·å†…ç½®çš„å¹³ç§»ç­‰å˜æ€§æˆ–å±€éƒ¨é‚»åŸŸç»“æ„ã€‚\n\nè¿™æ ·å°±æ˜¯ä¸ºä»€ä¹ˆViTéœ€è¦æ›´å¤šæ•°æ®å’Œè®¡ç®—æ‰èƒ½å­¦åˆ°åŒæ ·çš„ç©ºé—´å½’çº³è§„å¾‹ã€‚"
  },
  {
    "objectID": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#vit-model-variants",
    "href": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#vit-model-variants",
    "title": "Vision-Transformer",
    "section": "8.6 ViT Model Variants",
    "text": "8.6 ViT Model Variants\nViT æœ‰3ç§ä¸åŒçš„åŸºæœ¬å˜å½¢ï¼Œ å¦‚ä¸‹å›¾æ‰€ç¤º \nViTçš„åå­—é€šå¸¸è¡¨ç¤ºä¸º: ViT-L/16: æ„æ€æ˜¯ï¼ŒViT-Largeï¼Œç„¶åç”¨çš„16 Patch Sizeã€‚ éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒPatch Sizeè¶Šå¤§ï¼Œæˆ‘ä»¬å¾—åˆ°çš„tokenså°±è¶Šå°‘ï¼Œä¹Ÿå°±æ˜¯éœ€è¦æ›´å°‘çš„è®­ç»ƒæ—¶å®ç°ã€‚"
  },
  {
    "objectID": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#å‡å°‘tokens",
    "href": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#å‡å°‘tokens",
    "title": "Vision-Transformer",
    "section": "10.1 å‡å°‘Tokens",
    "text": "10.1 å‡å°‘Tokens\n\nPatch Merge\nPatch Shuffle"
  },
  {
    "objectID": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#vision-language-model",
    "href": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#vision-language-model",
    "title": "Vision-Transformer",
    "section": "10.2 Vision Language Model",
    "text": "10.2 Vision Language Model\næˆ‘ä»¬ä»¥åŠå­¦ä¹ äº†ViT for computer Visionï¼Œ Transformer for NLPï¼Œ æ¥ä¸‹æ¥æœ‰ä»€ä¹ˆåŠæ³•è®©è¿™ä¸¤ç§æ¨¡å‹ç»“åˆèµ·æ¥å‘¢ï¼Ÿ CLIP (2021): å°† ViT èåˆåˆ° vision-language é¢„è®­ç»ƒä¸­ã€‚"
  },
  {
    "objectID": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#axial-attentionè½´å‘æ³¨æ„åŠ›",
    "href": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#axial-attentionè½´å‘æ³¨æ„åŠ›",
    "title": "Vision-Transformer",
    "section": "11.1 Axial Attentionï¼ˆè½´å‘æ³¨æ„åŠ›ï¼‰",
    "text": "11.1 Axial Attentionï¼ˆè½´å‘æ³¨æ„åŠ›ï¼‰\nåœ¨å¤„ç† å›¾åƒæˆ–è§†é¢‘ è¿™ç±»é«˜ç»´è¾“å…¥æ—¶ï¼Œå¦‚æœç›´æ¥å¯¹æ‰€æœ‰åƒç´ åš å…¨å±€ self-attentionï¼Œå¤æ‚åº¦æ˜¯ \\(\\mathcal{O}(H^2 W^2)\\) ï¼ˆ\\(H, W\\) æ˜¯é«˜å’Œå®½ï¼‰ã€‚å½“å›¾åƒå¾ˆå¤§æ—¶ï¼Œè¿™ä¸ªä»£ä»·å¤ªé«˜ã€‚ æ ¸å¿ƒæƒ³æ³•ï¼šæŠŠäºŒç»´ attention æ‹†æˆä¸¤æ¬¡ä¸€ç»´ attentionï¼ˆæ²¿ç€å›¾åƒçš„ä¸¤ä¸ªâ€œè½´â€åˆ†åˆ«åšï¼‰ã€‚ 1. Row-wise Attentionï¼ˆè¡Œæ³¨æ„åŠ›ï¼‰ â€¢ æ²¿ç€æ°´å¹³æ–¹å‘ï¼ˆå®½åº¦è½´ Wï¼‰åšæ³¨æ„åŠ›ï¼Œæ¯ä¸€è¡Œçš„åƒç´ äº’ç›¸å…³æ³¨ã€‚ â€¢ å¤æ‚åº¦ï¼š\\(\\mathcal{O}(H \\cdot W^2)\\)ã€‚ 2. Column-wise Attentionï¼ˆåˆ—æ³¨æ„åŠ›ï¼‰ â€¢ æ²¿ç€å‚ç›´æ–¹å‘ï¼ˆé«˜åº¦è½´ Hï¼‰åšæ³¨æ„åŠ›ï¼Œæ¯ä¸€åˆ—çš„åƒç´ äº’ç›¸å…³æ³¨ã€‚ â€¢ å¤æ‚åº¦ï¼š \\(\\mathcal{O}(W \\cdot H^2)\\)ã€‚\nç»„åˆèµ·æ¥ï¼Œç›¸å½“äºåœ¨ H å’Œ W ä¸¤ä¸ªè½´ä¸Šéƒ½åšäº†å…¨å±€ä¾èµ–å»ºæ¨¡ã€‚\n\n\n\n\n\n\nFigureÂ 5"
  },
  {
    "objectID": "posts/PapersWithCode/05-dino/DINO.html",
    "href": "posts/PapersWithCode/05-dino/DINO.html",
    "title": "05: Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "",
    "text": "1 Preliminaries\n  \n  1.1 Self-Supervised Learning(SSL)\n  1.2 Knowledge Distillation\n  1.3 Exponential Moving Average (EMA)\n  \n  2 DINO\n  3 Summary\n  4 Key Concepts\n  5 Q & A\n  6 æ‰©å±•\n  7 DINO\n  \n  7.1 æ•°æ®å¤„ç†\n  7.2 DINO V2\n  7.3 DINO V3\n  7.4 DINO V1 vs.Â DINO V2 vs.Â DINO V3\n  \n  8 Implementation\n  \n  8.1 Safe-Softmax\nåœ¨æˆ‘ä»¬PwCçš„ç¬¬äºŒç¯‡ï¼Œæˆ‘ä»¬å­¦ä¹ äº†ä»€ä¹ˆæ˜¯Vision Transformer ä»¥åŠå®ƒçš„åŸºæœ¬æ¶æ„å’Œå·¥ä½œåŸç†ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº† ViT ç›¸å¯¹äºä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚ ä½†æ˜¯ViTå­˜åœ¨çš„ä¸»è¦é—®é¢˜æ˜¯éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ï¼š\næˆ‘ä»¬åœ¨è®­ç»ƒCIFAR-10 Classificationé—®é¢˜æ—¶ä¹Ÿå¯ä»¥å‘ç°ï¼ŒViTåœ¨å…¶å‡†ç¡®åº¦åªæœ‰ 55% å·¦å³ã€‚ å…¶ä¸»è¦çš„åŸå› æ˜¯ï¼šViT æ²¡æœ‰ CNN çš„ inductive biasï¼ˆå·ç§¯çš„å¹³ç§»ä¸å˜æ€§ã€å±€éƒ¨æ„Ÿå—é‡ï¼‰ï¼Œéœ€è¦æ›´å¤šæ ‡æ³¨æ¥â€œå­¦ä¼šâ€è¿™äº›å…ˆéªŒã€‚\nè€ŒDINO (Caron et al. 2021) çš„æå‡ºï¼Œæ—¨åœ¨è§£å†³â€œåœ¨æ²¡æœ‰æ ‡ç­¾æ•°æ®æƒ…å†µä¸‹ï¼Œè®­ç»ƒ Vision Transformerï¼ˆViTï¼‰æå–æœ‰åŒºåˆ†æ€§ã€è¯­ä¹‰ä¸°å¯Œçš„è§†è§‰è¡¨ç¤ºâ€çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•æ—¨åœ¨åˆ©ç”¨Self-Supervised Learningæ–¹å¼ï¼Œé€šè¿‡æ¨¡å‹è‡ªèº«è’¸é¦å®ç°æœ‰æ•ˆé¢„è®­ç»ƒã€‚\nåœ¨æˆ‘ä»¬äº†è§£DINOä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆäº†è§£å‡ ä¸ªåŸºæœ¬æ¦‚å¿µï¼Œä»¥ä¾¿æˆ‘ä»¬æ›´å¥½çš„ç†è§£DINOã€‚"
  },
  {
    "objectID": "posts/PapersWithCode/05-dino/DINO.html#self-supervised-learningssl",
    "href": "posts/PapersWithCode/05-dino/DINO.html#self-supervised-learningssl",
    "title": "05: Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "1.1 Self-Supervised Learning(SSL)",
    "text": "1.1 Self-Supervised Learning(SSL)\nSelf-Supervised Learningæ˜¯ä¸€ç§Un-supervised learningçš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨æœªæ ‡è®°æ•°æ®ä¸­çš„ç»“æ„ä¿¡æ¯æ¥è¿›è¡Œç‰¹å¾å­¦ä¹ ã€‚å®ƒé€šå¸¸é€šè¿‡è®¾è®¡é¢„æ–‡æœ¬ä»»åŠ¡ï¼ˆpretext tasksï¼‰æ¥å®ç°ã€‚é¢„æ–‡æœ¬ä»»åŠ¡æŒ‡çš„æ˜¯ï¼šåœ¨æ²¡æœ‰äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œäººä¸ºè®¾è®¡ä¸€ä¸ªâ€œä¼ªä»»åŠ¡â€ï¼Œè®©æ¨¡å‹é€šè¿‡è§£å†³è¿™ä¸ªä»»åŠ¡æ¥å­¦ä¹ æœ‰ç”¨çš„ç‰¹å¾è¡¨ç¤ºã€‚ä¸¾ä¸ªä¾‹å­ï¼Œåœ¨ Language Modelä¸­ï¼ŒGPT-2(Radford et al., n.d.) é€šè¿‡é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼ˆNext Token Predictionï¼‰æ¥å­¦ä¹ è¯­è¨€è¡¨ç¤ºï¼Œè€ŒBERT(Devlin et al. 2019) é€šè¿‡æ©ç è¯­è¨€æ¨¡å‹ä»»åŠ¡ï¼ˆMasked Language Modelingï¼‰æ¥å­¦ä¹ ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚è€Œå¯¹äºå›¾ç‰‡ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡\n\né®æŒ¡å›¾ç‰‡çš„åŒºåŸŸï¼ˆInpaintingï¼‰ï¼Œ æ¥è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ã€‚\næ‹¼å›¾ä»»åŠ¡ï¼ˆJigsaw Puzzleï¼‰ï¼šæŠŠå›¾ç‰‡åˆ†å‰²æˆå°å—ï¼Œæ‰“ä¹±é¡ºåºï¼Œè®©æ¨¡å‹é‡æ–°æ’åˆ—ã€‚\n\n\nä¸€å¥è¯æ€»ç»“Self-Supervised Learningå°±æ˜¯: è®©æ¨¡å‹å…ˆç©ä¸€äº›è‡ªå¸¦æ ‡ç­¾çš„å°ä»»åŠ¡ï¼Œä»è€Œå­¦ä¼šç†è§£æ•°æ®ã€‚"
  },
  {
    "objectID": "posts/PapersWithCode/05-dino/DINO.html#knowledge-distillation",
    "href": "posts/PapersWithCode/05-dino/DINO.html#knowledge-distillation",
    "title": "05: Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "1.2 Knowledge Distillation",
    "text": "1.2 Knowledge Distillation\nçŸ¥è¯†è’¸é¦(Hinton, Vinyals, and Dean 2015)æ˜¯ä¸€ç§æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œé€šè¿‡å°†ä¸€ä¸ªå¤§å‹æ¨¡å‹ï¼ˆæ•™å¸ˆæ¨¡å‹ï¼‰çš„çŸ¥è¯†è½¬ç§»åˆ°ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹ï¼ˆå­¦ç”Ÿæ¨¡å‹ï¼‰ä¸­ï¼Œä»è€Œæé«˜å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ã€‚è¿™ä¸¤ä¸ªæ¨¡å‹ï¼Œé€šå¸¸æœ‰ä»¥ä¸‹å‡ ä¸ªç‰¹ç‚¹ï¼š\n\nTeacher æ¨¡å‹é€šå¸¸å®¹é‡å¤§ã€æ€§èƒ½å¥½ï¼Œä½†è®¡ç®—å¼€é”€é«˜ã€‚\nStudent æ¨¡å‹è¾ƒå°ï¼Œä½†é€šè¿‡å­¦ä¹  Teacher çš„çŸ¥è¯†ï¼Œå¯ä»¥åœ¨ä½æˆæœ¬ä¸‹æ¥è¿‘ Teacher çš„æ€§èƒ½ã€‚\n\nçŸ¥è¯†è’¸é¦æ ¸å¿ƒæ€æƒ³æ˜¯ï¼ŒTeacher è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ (soft targets) å¾€å¾€åŒ…å«ç€æ¯”ç¡¬æ ‡ç­¾ï¼ˆHard Labelï¼‰æ›´å¤šçš„ä¿¡æ¯ï¼Œæ¯”å¦‚ç±»é—´ç›¸ä¼¼æ€§ã€‚å…¶æŸå¤±å‡½æ•°å®šä¹‰ä¸º\n\\[\n\\mathcal{L}_{KD} =\n(1-\\lambda)\\,\n\\underbrace{\\mathcal{L}_{CE}(y, p_s)}_{\\text{Hard Label Loss}} +\n\\lambda \\, T^2 \\,\n\\underbrace{\\mathcal{L}_{KL}(p_t^T, p_s^T)}_{\\text{Soft Label Loss}}\n\\tag{1}\\]\nå…¶ä¸­ï¼š\n\n\\(p_s, p_t\\): æ˜¯ student/teacher çš„è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒã€‚\n\\(T\\): æ˜¯æ¸©åº¦å‚æ•°ï¼Œç”¨äºæ§åˆ¶softmaxçš„å¹³æ»‘ç¨‹åº¦ã€‚\n\\(\\lambda\\): æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œç”¨äºå¹³è¡¡ä¸¤ç§æŸå¤±çš„è´¡çŒ®ã€‚\n\\(\\mathcal{L}_{CE}\\): æ˜¯äº¤å‰ç†µæŸå¤±å‡½æ•°, ç”¨äºè¡¡é‡å­¦ç”Ÿæ¨¡å‹è¾“å‡ºä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®è·ã€‚ \\[\n\\mathcal{L}_{CE}(y, p_s) = -\\sum_{i} y_i \\log(p_{s,i})\n\\tag{2}\\]\n\\(\\mathcal{L}_{KL}\\): æ˜¯Kullback-Leibleræ•£åº¦æŸå¤±å‡½æ•°ï¼Œç”¨äºè¡¡é‡æ•™å¸ˆæ¨¡å‹è¾“å‡ºä¸å­¦ç”Ÿæ¨¡å‹è¾“å‡ºä¹‹é—´çš„å·®è·ã€‚\n\n\\[\n\\mathcal{L}_{KL}(p_t, p_s) = \\sum_{i} p_t(i) \\log\\frac{p_t(i)}{p_s(i)}\n\\tag{3}\\]\n\nTakeaway 2 çŸ¥è¯†è’¸é¦(Knowledge Distillation)æ˜¯ä¸€ç§æ¨¡å‹å‹ç¼©(Model Compression)æ–¹æ³•ï¼Œé€šè¿‡è®©å°æ¨¡å‹ï¼ˆstudentï¼‰å­¦ä¹ å¤§æ¨¡å‹ï¼ˆteacherï¼‰çš„è¾“å‡ºåˆ†å¸ƒï¼Œä»è€Œç»§æ‰¿å…¶çŸ¥è¯†å¹¶æå‡å°æ¨¡å‹æ€§èƒ½ã€‚"
  },
  {
    "objectID": "posts/PapersWithCode/05-dino/DINO.html#exponential-moving-average-ema",
    "href": "posts/PapersWithCode/05-dino/DINO.html#exponential-moving-average-ema",
    "title": "05: Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "1.3 Exponential Moving Average (EMA)",
    "text": "1.3 Exponential Moving Average (EMA)\næŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰æ˜¯ä¸€ç§ Weight Average çš„æ–¹æ³•ã€‚åœ¨DINOä¸­ï¼ŒEMAè¢«ç”¨æ¥æ›´æ–°æ•™å¸ˆç½‘ç»œçš„å‚æ•°ï¼Œä½¿å…¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´åŠ ç¨³å®šã€‚å…·ä½“æ¥è¯´ï¼Œæ•™å¸ˆç½‘ç»œçš„å‚æ•°æ˜¯å­¦ç”Ÿç½‘ç»œå‚æ•°çš„æŒ‡æ•°åŠ æƒå¹³å‡ï¼Œè¿™æ ·å¯ä»¥é¿å…è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‰§çƒˆæ³¢åŠ¨ï¼Œä»è€Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚\n\\[\nx^{\\text{EMA}}_{t+1} = \\alpha x^{\\text{EMA}}_t + (1 - \\alpha) x_{t+1}, \\quad \\text{where} \\ \\alpha \\in [0, 1]\n\\tag{4}\\]\nEMA é€šå¸¸ç”¨æ¥æå‡æ¨¡å‹çš„generalizationèƒ½åŠ›ã€‚\n\nTakeaway 3 æŒ‡æ•°ç§»åŠ¨å¹³å‡(Exponential Moving Average)é€šè¿‡ç»™æ–°æ•°æ®æ›´é«˜æƒé‡ã€æ—§æ•°æ®æŒ‡æ•°è¡°å‡æ¥å¹³æ»‘æ›´æ–°ï¼Œä»è€Œè·å¾—æ›´ç¨³å®šçš„å‚æ•°æˆ–ä¿¡å·ã€‚\n\næœ‰äº†è¿™äº›å‚¨å¤‡çŸ¥è¯†ï¼Œæ¥ä¸‹æ¥è®©æˆ‘ä»¬çœ‹çœ‹DINOæ˜¯å¦‚ä½•åˆ©ç”¨è¿™äº›çŸ¥è¯†æ¥è®­ç»ƒæ¨¡å‹çš„ã€‚"
  },
  {
    "objectID": "posts/PapersWithCode/05-dino/DINO.html#æ•°æ®å¤„ç†",
    "href": "posts/PapersWithCode/05-dino/DINO.html#æ•°æ®å¤„ç†",
    "title": "05: Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "7.1 æ•°æ®å¤„ç†",
    "text": "7.1 æ•°æ®å¤„ç†\nDINO åœ¨æ•°æ®å¤„ç†ä¸Šé‡‡ç”¨äº†å¤šè§†å›¾å¢å¼ºç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ¯ä¸€å¼ è¾“å…¥å›¾åƒï¼ŒDINO ä¼šç”Ÿæˆå¤šä¸ªä¸åŒçš„è§†å›¾ï¼Œè¿™äº›è§†å›¾é€šè¿‡ä¸åŒçš„æ•°æ®å¢å¼ºæŠ€æœ¯è·å¾—ï¼Œä¾‹å¦‚éšæœºè£å‰ªã€é¢œè‰²æŠ–åŠ¨ç­‰ã€‚è¿™äº›å¢å¼ºè§†å›¾å°†ä½œä¸º Student ç½‘ç»œçš„è¾“å…¥ã€‚\nåŒæ—¶ï¼ŒTeacher ç½‘ç»œåˆ™åªä½¿ç”¨å…¨å±€è§†å›¾ï¼Œå³å¯¹æ•´ä¸ªå›¾åƒè¿›è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­ï¼Œå¾—åˆ°å…¨å±€ç‰¹å¾è¡¨ç¤ºã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒStudent ç½‘ç»œå¯ä»¥å­¦ä¹ åˆ°æ›´ä¸°å¯Œçš„å±€éƒ¨ç‰¹å¾ï¼ŒåŒæ—¶ä¹Ÿèƒ½ä¸ Teacher ç½‘ç»œçš„å…¨å±€ç‰¹å¾è¿›è¡Œå¯¹é½ã€‚\n\n\n\nå¯¹äºä¸€å¼ ç…§ç‰‡ï¼ŒDINOè¿›è¡Œä»¥ä¸‹å‡ ä¸ªæ“ä½œï¼š\n\né¢œè‰²æŠ–åŠ¨ï¼ˆColor Jitterï¼‰\né«˜æ–¯æ¨¡ç³Šï¼ˆGaussian Blurï¼‰\nå¤ªé˜³åŒ–ï¼ˆSolarizationï¼‰\n\nå¹¶ä¸”å¯¹æ¯ä¸ªè§†å›¾ï¼Œè¿›è¡ŒMulti-Cropçš„ç­–ç•¥ï¼š\n\nç”Ÿæˆ ä¸¤ä¸ª global cropï¼ˆå¤§è§†é‡ï¼‰ï¼Œ\nåŠ è‹¥å¹²ä¸ª local cropï¼ˆå°è§†é‡ï¼‰ï¼Œ\nç„¶åæ‰€æœ‰è£å‰ªè§†å›¾éƒ½è¾“å…¥åˆ° å­¦ç”Ÿç½‘ç»œ\nåªæœ‰ global views è¾“å…¥åˆ° æ•™å¸ˆç½‘ç»œ\n\n\n\n\n\n\n \n\n\nFigureÂ 2: DINO Image Pre-process Steps\n\n\n\n\n\n\nä¸‹é¢æ˜¯å‡ ä¸ªData Augmentationçš„ä¾‹å­\n\n\n\n\n\n\n\n\n\n\n\n(a) Original Image\n\n\n\n\n\n\n\n\n\n\n\n(b) Gaussian Blur\n\n\n\n\n\n\n\n\n\n\n\n(c) Solarized\n\n\n\n\n\n\n\nFigureÂ 3: ä¸¤ä¸ªä¸åŒçš„Data Augmentation Examples\n\n\n\nThe full augmentation pipeline\n\n\n\n\n\n\nFigureÂ 4: The Data Augmentation Pipeline"
  },
  {
    "objectID": "posts/PapersWithCode/05-dino/DINO.html#dino-v2",
    "href": "posts/PapersWithCode/05-dino/DINO.html#dino-v2",
    "title": "05: Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "7.2 DINO V2",
    "text": "7.2 DINO V2\n\n\n\n\n\n\nFigureÂ 5\n\n\n\nâ€¢ DINO 1ï¼šæ„å»ºåŸºç¡€æ¡†æ¶ï¼Œå¼€å¯è‡ªç›‘ç£å­¦ä¹ åœ¨è§†è§‰ Transformer çš„åº”ç”¨ã€‚ â€¢ DINO 2ï¼šå…¨é¢æ‰©å±•è®­ç»ƒè§„æ¨¡ä¸æŠ€æœ¯ï¼Œä½¿æ¨¡å‹æˆä¸ºçœŸæ­£â€œå¼€ç®±å³ç”¨â€çš„è§†è§‰åŸºç¡€æ¨¡å‹ã€‚ â€¢ DINO 3ï¼šè¿›ä¸€æ­¥æå‡è§„æ¨¡ä¸æŠ€æœ¯ï¼Œè§£å†³å¯†é›†ç‰¹å¾é€€åŒ–é—®é¢˜ï¼Œå¢å¼ºå¤šåœºæ™¯é€‚åº”æ€§ï¼Œå¹¶æ¨åŠ¨æ€§èƒ½è‡³æ–°é«˜åº¦ã€‚"
  },
  {
    "objectID": "posts/PapersWithCode/05-dino/DINO.html#dino-v3",
    "href": "posts/PapersWithCode/05-dino/DINO.html#dino-v3",
    "title": "05: Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "7.3 DINO V3",
    "text": "7.3 DINO V3"
  },
  {
    "objectID": "posts/PapersWithCode/05-dino/DINO.html#dino-v1-vs.-dino-v2-vs.-dino-v3",
    "href": "posts/PapersWithCode/05-dino/DINO.html#dino-v1-vs.-dino-v2-vs.-dino-v3",
    "title": "05: Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "7.4 DINO V1 vs.Â DINO V2 vs.Â DINO V3",
    "text": "7.4 DINO V1 vs.Â DINO V2 vs.Â DINO V3\n\n\nSummary of 4 Tokenization Algorithms\n\n\n\n\n\n\n\n\nDINO Version\næ ¸å¿ƒåˆ›æ–°ä¸æ”¹è¿›\næ•°æ®è§„æ¨¡ä¸æ¨¡å‹è§„æ¨¡\nåº”ç”¨ç‰¹æ€§ä¸ä¼˜åŠ¿\n\n\n\n\nDINO 1\nstudentâ€“teacher è‡ªç›‘ç£è’¸é¦ï¼Œattention å¯è§†åŒ–\nä¸­å°è§„æ¨¡æ•°æ®ä¸æ¨¡å‹\nå¯è§†åŒ–ç‰¹å¾å­¦ä¹ ã€è¯­ä¹‰åˆ†å‰²èƒ½åŠ›ã€ä½æˆæœ¬è®­ç»ƒ\n\n\nDINO 2\nå¤§è§„æ¨¡è’¸é¦ã€FlashAttentionã€æ­£åˆ™åŒ–ã€è¶…å¼ºæ³›åŒ–èƒ½åŠ›\n1.42 äº¿å›¾åƒè®­ç»ƒæ•°æ®ã€å¤šä¸ª ViT æ¶æ„\nå¤šä»»åŠ¡é€šç”¨ï¼Œæ— éœ€å¾®è°ƒï¼Œä»»åŠ¡è¦†ç›–å¹¿æ³›\n\n\nDINO 3\nGram anchoringã€è½´å‘ RoPEã€å¤šåˆ†è¾¨ç‡é²æ£’ã€å¤šæ¨¡å‹ç‰ˆæœ¬\n7B å‚æ•°æ¨¡å‹ + è¶…å¤§æ•°æ®ï¼ˆ1.7B å›¾åƒï¼‰\næ›´å¼ºå¯†é›†ç‰¹å¾è´¨é‡ï¼Œå¤šä»»åŠ¡æ€§èƒ½è¾¾æ–° SOTAï¼Œé€‚åº”æ€§æ›´å¼º\n\n\n\n\n\nQuestion: Answer: å› ä¸ºç¼ºå°‘ CNN çš„ inductive biasï¼ŒViT åªèƒ½ä¾é å¤§æ•°æ®æ¥å­¦ä¹ ç©ºé—´ä¸å˜æ€§å’Œå±€éƒ¨æ¨¡å¼ã€‚\n\n\nAnswer: åˆ©ç”¨æ•™å¸ˆâ€“å­¦ç”Ÿè’¸é¦ + å›¾åƒå¢å¼ºè§†å›¾ï¼Œå­¦ç”Ÿæ¨¡ä»¿æ•™å¸ˆè¾“å‡ºï¼Œä»è€Œå­¦ä¹ é²æ£’ç‰¹å¾ã€‚\n\n\nAnswer: è®©å­¦ç”Ÿå­¦ä¹ å±€éƒ¨ä¸å…¨å±€çš„è¯­ä¹‰å¯¹é½ï¼Œä»è€Œè·å¾—æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚\n\n\nAnswer: ViT çš„ self-attention åœ¨è‡ªç›‘ç£ä¸­ä¼šè‡ªç„¶èšåˆè¯­ä¹‰ä¸€è‡´çš„åŒºåŸŸï¼Œä»è€Œè¡¨ç°å‡ºå¯¹è±¡åˆ†å‰²ç°è±¡ã€‚\n\n\nCentering prevents one dimension to dominate but encourages collapse to the uniform distribution, while the sharpening has the opposite effect.  Emerging Properties in Self-Supervised Vision Transformers, p.4 \n\n\nOutput sharpening is obtained by using a low value for the temperature Ï„t in the teacher softmax normalization.  Emerging Properties in Self-Supervised Vision Transformers, p.4"
  },
  {
    "objectID": "posts/PapersWithCode/05-dino/DINO.html#safe-softmax",
    "href": "posts/PapersWithCode/05-dino/DINO.html#safe-softmax",
    "title": "05: Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "8.1 Safe-Softmax",
    "text": "8.1 Safe-Softmax\ndef softmax(x: torch.Tensor, temp):\n    max ="
  },
  {
    "objectID": "posts/LearningNotes/DLFaC/index.html",
    "href": "posts/LearningNotes/DLFaC/index.html",
    "title": "Deep Learning Foundation and Concepts(DLFaC) Learning Notes",
    "section": "",
    "text": "DLFaC Chapter 01: Introduction to the Deep Learning\n\n\nChapter01 ä»‹ç»Deep Learningçš„åŸºæœ¬æ¦‚å¿µ\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html",
    "title": "Qwen Model Series",
    "section": "",
    "text": "åœ¨äº†è§£ Qwen ç­‰LLM æ¨¡å‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆäº†è§£ä»€ä¹ˆæ˜¯ Transformer, åœ¨è¿™é‡Œå°±ä¸å…·ä½“å±•å¼€äº†ï¼Œæœ‰å…´è¶£çš„åŒå­¦å‰å»æŸ¥çœ‹ è¿™ç¯‡æ–‡ç« ã€‚ å¯¹äºMulti-Modalityçš„Qwenï¼Œæˆ‘ä»¬éœ€è¦å…·å¤‡çš„æ˜¯ Vision-Transformer çš„çŸ¥è¯†ã€‚\nå‰ç½®çŸ¥è¯†ï¼š\n\nTransformer\nVision-Transformer\n\nQwenï¼ˆé€šä¹‰åƒé—®ï¼‰ æ˜¯é˜¿é‡Œäº‘æ——ä¸‹è¾¾æ‘©é™¢æ¨å‡ºçš„ä¸€ç³»åˆ—å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM) ä¸å¤šæ¨¡æ€æ¨¡å‹ï¼ˆM-LLMï¼‰ã€‚å®ƒç±»ä¼¼ OpenAI çš„ GPT ç³»åˆ—ï¼Œæ˜¯é˜¿é‡Œæ‰“é€ çš„å…¨æ ˆå¼ç»Ÿä¸€LLMä½“ç³»ã€‚ æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥æ²¿ç€Qwenæ¨¡å‹å‘å±•çš„æ—¶é—´çº¿ï¼Œæ¥æ„Ÿå—ä¸€ä¸‹LLMå‘å±•çš„è¿‡ç¨‹ã€‚"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#tokenization",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#tokenization",
    "title": "Qwen Model Series",
    "section": "2.1 Tokenization",
    "text": "2.1 Tokenization\nQwen ç”¨byte pair encoding (BPE)çš„Tokenizationçš„æ–¹æ³•ï¼Œè¿™ä¸GPT-3.5ï¼ŒGPT-4ç³»åˆ—æ˜¯ä¸€æ ·çš„ã€‚åœ¨è®­ç»ƒTokenizationä¹‹åï¼Œæœ€åçš„Vocabulary Size ç”±152Kã€‚ Qwençš„Tokenizationçš„æ–¹æ³•ï¼Œå®ç°äº†è¾ƒä½çš„Compression Ratioã€‚ä½Compression Ratioè¯´æ˜äº†Qwenåœ¨è¿™äº›è¯­è¨€çš„Training å’Œ Inference ä¸­ä¼šæ¯”è¾ƒé«˜æ•ˆã€‚"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#architecture",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#architecture",
    "title": "Qwen Model Series",
    "section": "2.2 Architecture",
    "text": "2.2 Architecture\nQwen-1 çš„æ¨¡å‹å€Ÿé‰´äº†LLaMAæ¨¡å‹ï¼Œä¹Ÿæ˜¯Decoder-Only çš„ Transformer (Vaswani et al. 2023) æ¨¡å‹ï¼Œåªä¸è¿‡æœ‰ç€ä»¥ä¸‹å‡ ç‚¹çš„æ”¹å˜ï¼š\n\nEmbedding and output projection: LLaMAå’ŒTransformer éƒ½ç”¨äº†Weight Tyingçš„æŠ€æœ¯ï¼Œè¿™ç§æ–¹å¼å¯ä»¥å‡å°‘æ¨¡å‹çš„å‚æ•°ï¼Œæé«˜è®­ç»ƒçš„æ•ˆç‡ã€‚ä¸è¿‡Qwenæ²¡æœ‰æ²¿ç”¨è¿™ç§æ–¹å¼ï¼Œè€Œæ˜¯è®©è¿™ä¸¤ä¸ªæœ‰å„è‡ªçš„parametersã€‚\nPositional Embeddingï¼š Qwenç”¨äº† RoPE (Su et al. 2023) æ¥Encoding Positionæ¶ˆæ¯ï¼ŒåŒæ—¶è¿ç”¨äº† FP32 çš„ç²¾åº¦ï¼Œæ¥ inverse frequency matrix.\nBias: å¯¹äºè®¸å¤šçš„Layerï¼Œç§»é™¤äº†biasçš„termï¼Œä¸è¿‡å¯¹äºQKV Layersï¼Œè¿˜æ˜¯åŠ äº†Biasã€‚\nPre-Norm & RMSNormï¼šQwen æ¨¡å‹ç”¨äº† Pre-Norm å’ŒRMSNormæ¥å½“ä½œNormalization çš„æ–¹æ³•\nActivation Function: ç”¨äº†SwiGLUå½“ä½œActivation Function, ä¸ºäº†ä¿æŒæ¨¡å‹å‚æ•°çš„ä¸å˜ï¼Œå‡å°‘äº†d_ffåˆ° \\(\\frac{8}{3}\\)"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#pre-training",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#pre-training",
    "title": "Qwen Model Series",
    "section": "2.3 Pre-Training",
    "text": "2.3 Pre-Training\nPre-Training éµå¾ªäº†æ ‡å‡†çš„Auto-Regressive LMçš„è®­ç»ƒç›®æ ‡ï¼ŒContext Lengthè®¾ä¸º2048ï¼Œ è¿ç”¨äº†Flash-Attentionã€‚ åˆ©ç”¨AdamW çš„optimizerã€‚ å’Œ Cosine Learning Rate schedule. å¹¶ä¸”è¿ç”¨äº† Mixed Precision Training ä¸ºäº†æé«˜æ¨¡å‹çš„Stability å’Œ è®­ç»ƒé€Ÿåº¦ã€‚"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#extend-context-length",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#extend-context-length",
    "title": "Qwen Model Series",
    "section": "2.4 Extend Context Length",
    "text": "2.4 Extend Context Length\næ¨¡å‹åœ¨è®­ç»ƒæ—¶çš„Context Lengthè®¾ä¸ºäº†2048ï¼Œä¸è¿‡è¿™åœ¨Inferenceæ—¶ï¼Œæ˜¯ä¸å¤Ÿçš„ã€‚äºæ˜¯Qwenåˆ©ç”¨äº†ä¸€ç§training-free techniques çš„æ–¹æ³•ï¼Œ NTK-aware interpolation\n\n2.4.1 NTK-aware Interpolation\nNTK-aware Interpolation ä¸æ™®é€šçš„Position Interpolationä¸åŒï¼ŒNTK-aware Interpolation adjust the base of RoPEã€‚ Qwençš„å›¢é˜Ÿåœ¨NTK-awareçš„åŸºç¡€ä¸Šï¼Œä¸ºäº†æ›´å¥½çš„å‹æ¦¨å‡ºNTKçš„æ€§èƒ½ï¼Œå®ç°äº†ä¸€ä¸ªNTKçš„extensionï¼Œå«åš dynamic NTK-aware interpolationã€‚\n\n\n2.4.2 Attention\né™¤äº†Position Encodingï¼Œattentionçš„è®¡ç®—æ•ˆç‡ä¹Ÿæ˜¯é˜»ç¢Context lengthçš„åŸå› ä¹‹ä¸€ã€‚ Qwençš„å›¢é˜Ÿç”¨äº†ä¸¤ä¸ªAttentionçš„æŠ€å·§ï¼š - LogN-Scaling - Layer-wise Window Attention: ä¸åŒçš„Layer æœ‰ä¸åŒçš„Window Size\n\nWe also observed that the long-context modeling ability of our model varies across layers, with lower layers being more sensitive in context length extension compared to the higher layers. To leverage this observation, we assign different window sizes to each layer, using shorter windows for lower layers and longer windows for higher layers.  QWEN TECHNICAL REPORT, p.8 \n\nè¿ç”¨äº†ä»¥ä¸Šå‡ ä¸ªæŠ€å·§ä¹‹åï¼ŒQwen æ¨¡å‹å°†context length ä»2048æå‡åˆ°äº†8192ï¼Œ åœ¨æ²¡æœ‰æŸå®³æ¨¡å‹èƒ½åŠ›çš„å‰æä¸‹ã€‚"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#alignment",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#alignment",
    "title": "Qwen Model Series",
    "section": "2.5 Alignment",
    "text": "2.5 Alignment\nåœ¨Pre-train Qwenä¹‹åï¼Œæˆ‘ä»¬ä¸èƒ½ç›´æ¥ä½¿ç”¨ï¼Œ\n\n2.5.1 Supervised Fine-Tuning\nSFTçš„è®­ç»ƒï¼Œå¯ä»¥è®©Qwenæ¨¡å‹éµå¾ªChatç±»å‹çš„å›ç­”ã€‚\n\n2.5.1.1 Data\n{\n  \"messages\": [\n    {\"role\": \"system\", \"content\": \"You are a helpful, harmless and honest assistant.\"},\n    {\"role\": \"user\", \"content\": \"å¸®æˆ‘å†™ä¸€æ®µå…³äºQwenæ¨¡å‹çš„ç®€ä»‹ã€‚\"},\n    {\"role\": \"assistant\", \"content\": \"Qwen æ˜¯é˜¿é‡Œäº‘æ¨å‡ºçš„ä¸€ç³»åˆ—å¤§è¯­è¨€æ¨¡å‹...\"}\n  ]\n}\n\n\n2.5.1.2 Training\nä¸Pre-Trainingç±»ä¼¼ï¼ŒSFT ç”¨äº†ç›¸åŒçš„è®­ç»ƒç›®æ ‡ Next-Token Predictionï¼Œä¸è¿‡ä¸Pre-Trainingä¸åŒçš„æ˜¯ï¼ŒSFTç”¨äº†ä¸€ä¸ªLoss Maskæ¥maskæ‰system å’Œ user inputsã€‚\nåŒæ ·è¿ç”¨äº† AdamW optimizer\n\n\n\n2.5.2 RLHF\nåœ¨SFTä¹‹åï¼Œæ¨¡å‹å¯èƒ½overfittingï¼Œå¹¶ä¸”ç¼ºå°‘generalization å’Œ creativityã€‚ ä¸ºäº†è®©æ¨¡å‹è·å¾—è¿™äº›èƒ½åŠ›ï¼Œåœ¨SFTä¹‹åï¼Œæˆ‘ä»¬éœ€è¦RLHFï¼Œè¿™ä¸ªè¿‡ç¨‹æ¶‰åŠåˆ°ä¸¤ä¸ªæ­¥éª¤ï¼š 1. Reward Model Training 2. Policy Training\n\n2.5.2.1 Reward Model Training\nReward Model Training ä¹Ÿå«åš Preference Model Pretraining (PMP)ï¼Œ è¿™ä¸ªåŒæ ·éœ€è¦pre- training ç„¶åFine-Tuniningã€‚ PMP çš„è®­ç»ƒæ•°æ®, ä¹Ÿæ˜¯ç”±ä¸€ç³»åˆ—çš„comparison data ç»„æˆã€‚\n{\n  \"prompt\": \"Explain why the Earth has seasons.\",\n  \"chosen\": \"The Earth has seasons because its axis is tilted about 23.5 degrees. As the planet orbits the Sun, this tilt causes different hemispheres to receive more or less direct sunlight throughout the year, creating seasonal temperature and daylight changes.\",\n  \"rejected\": \"The Earth has seasons because sometimes it randomly moves closer to the Sun and sometimes farther away.\"\n}\nè®­ç»ƒè¿™ä¸ªReward Modelã€‚ Qwençš„å›¢é˜Ÿç”¨äº†Pre-trained Language Model ä¹Ÿå°±æ˜¯Qwenï¼Œ æ¥å½“ä½œInitiate æƒé‡ã€‚ åœ¨è¿™ä¸ªQwenæ¨¡å‹ä¹‹ä¸Šï¼ŒåŠ ä¸Šäº†ä¸€å±‚Pooling Layeræ¥æå–å‡º Reward ï¼ˆä¸€ä¸ªScalar Valueï¼‰ #### Policy Training\nåœ¨è®­ç»ƒå®ŒReward Modelä¹‹åï¼Œä¸‹ä¸€æ­¥å°±æ˜¯è¿ç”¨Reinforcementçš„ç®—æ³•æ¥è®­ç»ƒLLMã€‚Qwençš„å›¢é˜Ÿè¿ç”¨äº†PPOçš„ç®—æ³•ï¼Œæ¥è®­ç»ƒï¼Œè¿™ä¸ªç®—æ³•ç”±4ä¸ªéƒ¨åˆ†ï¼š - Policy Model - Value Model - Reference Model - Reward Model\nè‡³æ­¤ï¼ŒQwençš„Foundation Modelï¼Œä»¥åŠè®­ç»ƒç»“æŸäº†ã€‚æ¥ä¸‹æ¥å¯ä»¥é€šè¿‡ä¸åŒçš„è®­ç»ƒæ•°æ®ï¼Œè®©Qwen è·å¾—ä¸åŒçš„èƒ½åŠ›ï¼Œæ¯”å¦‚Code-Qwenï¼Œä»¥åŠMath-Qwen"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#code-qwen",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#code-qwen",
    "title": "Qwen Model Series",
    "section": "2.6 Code Qwen",
    "text": "2.6 Code Qwen"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#math-qwen",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#math-qwen",
    "title": "Qwen Model Series",
    "section": "2.7 Math Qwen",
    "text": "2.7 Math Qwen"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#qwen-1-summary",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#qwen-1-summary",
    "title": "Qwen Model Series",
    "section": "2.8 Qwen 1 Summary",
    "text": "2.8 Qwen 1 Summary"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#model-architecture",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#model-architecture",
    "title": "Qwen Model Series",
    "section": "3.1 Model Architecture",
    "text": "3.1 Model Architecture\nQwen-VLçš„æ¨¡å‹ä¸»è¦åŒ…å«äº†ä»¥ä¸‹å‡ ä¸ªç»„ä»¶ï¼š - Large Language Modelï¼š Qwen-VLæ˜¯åŸºäºä¹‹å‰çš„ Qwen-7Bæ¥å½“ä½œå¤§è¯­è¨€ç»„ä»¶ - Visual Encoderï¼š Visual Encoderçš„ç»“æ„æ˜¯ï¼Œä¸ Vision-Transformer (Dosovitskiy et al. 2021) çš„ç»“æ„æ˜¯ä¸€æ ·çš„ã€‚é€šè¿‡åŠ è½½ OpenCLIPçš„æƒé‡, æ¥åˆå§‹åŒ–ViT - Position-aware Vision-Language Adapter: ä¸ºäº†å‡å°‘ image featureçš„é•¿åº¦ï¼ŒQwen-VL åˆ©ç”¨äº† Vision Language Adapter. è¿™ä¸ªæ˜¯ä¸€ç»„Cross-Attention Module éšæœºåˆå§‹åŒ–ï¼Œè¿™ç§æ–¹æ³•å°†Visual Featureçš„é•¿åº¦ï¼Œå‹ç¼©åˆ°äº†256. åœ¨è¿™ä¸ªAdapter ä¸­ï¼Œ2D absolute positional encoding ä¹Ÿæ·»åŠ äº†è¿›æ¥ï¼Œç”¨æ¥å‡å°‘å¯èƒ½æ¶ˆæ¯çš„ä¸¢å¤±ã€‚\n ## Inputs and Outputs æ·»åŠ äº†Visual Tokensä¹‹åï¼Œæ¨¡å‹éœ€è¦ä¸€ç§æ–¹æ³•ï¼Œæ¥è¾¨åˆ«å‡ºå“ªäº›æ˜¯Visual Tokens, å“ªäº›æ˜¯Text Tokens\n\nImages are processed through the visual encoder and adapter, yielding fixed-length sequences of image features. To differentiate between image feature input and text feature input, two special tokens ( and ) are appended to the beginning and end of the image feature sequence respectively, signifying the start and end of image content  Qwen-VL- A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond, p.4 \n\nå¯¹äºä¸åŒçš„è¾“å…¥å’Œè¦æ±‚ï¼Œæ¨¡å‹çš„è¾“å‡ºçš„å†…å®¹æ˜¯ä¸ä¸€æ ·çš„ã€‚"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#training-1",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#training-1",
    "title": "Qwen Model Series",
    "section": "3.2 Training",
    "text": "3.2 Training\nQwen-VL çš„è®­ç»ƒåˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n\n\n3.2.0.1 Pre-Training\nåœ¨è¿™ä¸ªé˜¶æ®µï¼Œæ¨¡å‹è®­ç»ƒå‡ºè®¤è¯†å›¾ç‰‡çš„èƒ½åŠ›ï¼Œå†»ç»“äº†LLMï¼Œè®­ç»ƒViTå’ŒVL adapterã€‚\n\n\n3.2.0.2 Multi-Task Pre-Training\nåœ¨è¿™ä¸ªé˜¶æ®µï¼Œæ¨¡å‹å·²ç»æœ‰äº†å¯¹Imageçš„åŸºæœ¬è®¤çŸ¥ï¼Œæ¥ä¸‹æ¥å°±æ˜¯è®­ç»ƒæ¨¡å‹å¯¹äºä¸åŒè¦æ±‚çš„è¾“å‡ºï¼Œä¹Ÿå°±æ˜¯æ‰€è°“çš„Multi-Taskã€‚ åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæ‰€æœ‰çš„æƒé‡ï¼Œéƒ½è¿›è¡Œå¾®è°ƒã€‚é€šè¿‡è¿™ä¸ªè®­ç»ƒï¼Œæ¨¡å‹è·å¾—äº†å®Œæˆä¸åŒä»»åŠ¡çš„èƒ½åŠ›ã€‚\n\n\n3.2.0.3 Supervised Finetuning\nåœ¨è¿™ä¸ªé˜¶æ®µä¸‹ï¼ŒQwenå›¢é˜Ÿè®­ç»ƒäº†Qwen-VLï¼Œä½¿å®ƒè·å¾—Instruction- Followingçš„èƒ½åŠ›ã€‚ ä¹Ÿå°±æ˜¯Qwen-VL-Chat Modelï¼Œ\nSupervised Finetuning çš„æ•°æ®å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š \nè‡³æ­¤ï¼ŒQwen-VLçš„è®­ç»ƒå·²ç»ç»“æŸäº†ï¼Œ"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#qwen1-vl-sumary",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#qwen1-vl-sumary",
    "title": "Qwen Model Series",
    "section": "3.3 Qwen1-VL Sumary",
    "text": "3.3 Qwen1-VL Sumary"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#tokenizer",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#tokenizer",
    "title": "Qwen Model Series",
    "section": "5.1 Tokenizer",
    "text": "5.1 Tokenizer\nQwen2 çš„Tokenizationçš„æ–¹æ³•ï¼Œä¸Qwen1 æ˜¯ä¸€æ ·çš„ï¼Œéƒ½é‡‡ç”¨äº†BPE Tokenization Algorithmsã€‚Vocabulary Size æœ‰151,643 Regular tokenså’Œ3 ä¸ªcontrol tokensã€‚"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#architecture-1",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#architecture-1",
    "title": "Qwen Model Series",
    "section": "5.2 Architecture",
    "text": "5.2 Architecture\nä¸Qwen1 ç›¸æ¯”ï¼ŒQwen2 çš„æ¨¡å‹æ”¹åŠ¨å¹¶æ²¡æœ‰å¾ˆå¤§ï¼Œä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢\n\n5.2.1 Dense Model\n\n5.2.1.1 Gouged Query Attention\nè¿ç”¨äº†Grouped Query Attention, è¿™ç§æ–¹å¼å¯ä»¥ä¼˜åŒ–KV Cache\n\n\n5.2.1.2 Dual Chunk Attention with YARN\nè¿ç”¨äº†Dual Chunk Attention å’Œ YARN æ¥æé«˜Context çš„é•¿åº¦ã€‚\n\n\n\n5.2.2 Mixture-Of-Expert Model"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#architecture-2",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#architecture-2",
    "title": "Qwen Model Series",
    "section": "6.1 Architecture",
    "text": "6.1 Architecture\n\n\n6.1.1 Naive Dynamic Resolution\nNaive Dynamic Resolution åœ¨ NaViT (Dehghani et al. 2023) ä¸­æå‡ºï¼Œå®ƒå¯ä»¥è®©Vision Transformer åœ¨ä¸åŒé•¿åº¦çš„å›¾ç‰‡ä¸­è®­ç»ƒã€‚\né™¤æ­¤ä¹‹å¤–ï¼ŒQwen2-VL æ‘’å¼ƒäº†Absolute Position Embeddingï¼Œè½¬ç”¨2D-RoPE (Su et al. 2023), é™¤æ­¤ä¹‹å¤–ï¼Œåœ¨ViT ç”ŸæˆRepresentationä¹‹åï¼ŒQwen2-VL è¿˜åœ¨ä¹‹åæ·»åŠ äº†ä¸€å±‚MLPLayerï¼Œå®ƒçš„ä½œç”¨æ˜¯å‡å°‘Tokensçš„æ•°é‡ï¼Œé€šè¿‡ä¸´è¿‘çš„ \\(2\\times 2\\) çš„tokensï¼ŒMLPè®²è¿™äº›tokens mergeåœ¨ä¸€èµ·ï¼Œå°†Tokensçš„æ•°é‡å‡å°‘äº† 4 å€ã€‚æ¯”å¦‚ä¸€å¼  \\(224 \\times 224\\) çš„å›¾ç‰‡, åˆ†æˆå¤§å°ä¸º14çš„patchesï¼Œå¾—åˆ°äº†256 ä¸ªtokensï¼Œå°†è¿™äº›tokens mergeåœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº† 64 ä¸ªtokensã€‚\n\n\n6.1.2 Multi-Modal Rotary Position Embedding (M-RoPE)\nQwen2-VL åˆ©ç”¨M-RoPEæ¥æå–å‡ºä¸åŒModalityä¹‹é—´çš„position çš„ä¿¡æ¯ã€‚ é€šè¿‡å°†Rotary Embedding åˆ†è§£æˆï¼š - Temporal - Height - Width ä¸‰ä¸ªéƒ¨åˆ†ã€‚ å¯¹äºText çš„éƒ¨åˆ†ï¼Œè¿™å‡ ä¸ªéƒ¨åˆ†ä¼šå¾—åˆ°ç›¸åŒçš„IDsï¼Œè¿™ä½¿å¾—å®ƒå¯ä»¥ç±»ä¼¼äº1D-RoPEçš„ä½œç”¨ã€‚å¯¹äºå›¾ç‰‡çš„è¾“å…¥ï¼ŒTemporal éƒ¨åˆ†çš„IDä¿æŒä¸å˜ï¼ŒHeight å’Œ Widthçš„éƒ¨åˆ†ï¼Œä¼šå¾—åˆ°ä¸åŒçš„IDsã€‚ å¯¹äºè§†é¢‘çš„è¾“å…¥ï¼ŒTemporalï¼ŒHeightï¼ŒWidthéƒ½ä¼šæ”¹å˜ã€‚\n\nM-RoPE not only enhances the modeling of positional information but also reduces the value of position IDs for images and videos, enabling the model to extrapolate to longer sequences during inference.  Qwen -VL- Enhancing Vision-Language Modelâ€™s Perception of the World at Any Resolution, p.5 \n\n\nFor text, these three use identical position IDs â†’ equivalent to standard 1D RoPE.\nFor images, temporal ID is constant; height & width IDs encode patch position.\nFor videos, temporal ID increases per frame; height & width same as image."
  },
  {
    "objectID": "posts/LearningNotes/CS336/Assignments/Ass01/ass01.html",
    "href": "posts/LearningNotes/CS336/Assignments/Ass01/ass01.html",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "",
    "text": "åœ¨æœ¬éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†å®ç°Byte Pair Encoding (BPE)ç®—æ³•ï¼Œç”¨äºæ–‡æœ¬çš„tokenizationã€‚åœ¨Lecture 01ä¸­ï¼Œæˆ‘ä»¬å·²ç»ä»‹ç»äº†BPEçš„åŸºæœ¬åŸç†å’Œå®ç°æ–¹æ³•ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†é€šè¿‡ä»£ç æ¥è¿›ä¸€æ­¥çš„å®ç°å¹¶ä¸”ä¼˜åŒ–BPEç®—æ³•ã€‚\n\n\nå›é¡¾ä¸€ä¸‹BPEç®—æ³•çš„åŸºæœ¬æ­¥éª¤ï¼š 1. Initialization: å°†è¾“å…¥æ–‡æœ¬è§†ä¸ºå­—èŠ‚åºåˆ—ï¼Œæ¯ä¸ªå­—èŠ‚ä½œä¸ºä¸€ä¸ªtokenã€‚åˆå§‹åŒ–è¯æ±‡è¡¨åŒ…å«æ‰€æœ‰å¯èƒ½çš„å­—èŠ‚ï¼ˆ0-255ï¼‰ã€‚ 2. Count Pairs: ç»Ÿè®¡æ–‡æœ¬ä¸­æ‰€æœ‰ç›¸é‚»å­—èŠ‚å¯¹çš„å‡ºç°é¢‘ç‡ã€‚ 3. Merge Pairs: æ‰¾åˆ°å‡ºç°é¢‘ç‡æœ€é«˜çš„å­—èŠ‚å¯¹ï¼Œå¹¶å°†å…¶åˆå¹¶ä¸ºä¸€ä¸ªæ–°çš„tokenï¼Œæ›´æ–°æ–‡æœ¬å’Œè¯æ±‡è¡¨ã€‚ 4. Repeat: é‡å¤æ­¥éª¤2å’Œ3ï¼Œç›´åˆ°è¾¾åˆ°é¢„å®šçš„åˆå¹¶æ¬¡æ•°"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "",
    "text": "Lecture 02 ä¸»è¦ä»‹ç»äº† PyTorch çš„åŸºç¡€çŸ¥è¯†å’Œä¸€äº›å®ç”¨çš„å·¥å…·åº“ï¼Œæ¯”å¦‚ einopsã€‚è¯¾ç¨‹å†…å®¹æ¶µç›–äº†å¼ é‡æ“ä½œã€æ•°æ®ç±»å‹ã€ä¼˜åŒ–å™¨ç­‰æ–¹é¢çš„å†…å®¹ã€‚å¹¶ä¸”æœ€é‡è¦çš„æ˜¯ï¼Œæå‡ºäº†ä¸€ä¸ª Resource Accountingï¼Œ å³è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦å¤šå¤§çš„å†…å­˜å’Œè®¡ç®—èµ„æºã€‚ é€šè¿‡ Resource Accountingï¼Œæˆ‘ä»¬å¯ä»¥æ›´å¥½åœ°ç†è§£æ¨¡å‹è®­ç»ƒçš„èµ„æºéœ€æ±‚ï¼Œä»è€Œä¼˜åŒ–æ¨¡å‹è®¾è®¡å’Œè®­ç»ƒè¿‡ç¨‹ã€‚\nè¯¾ç¨‹è§†é¢‘å¦‚ä¸‹æ‰€ç¤ºï¼š"
  },
  {
    "objectID": "posts/LearningNotes/DLFaC/Chapter01/Chapter01.html",
    "href": "posts/LearningNotes/DLFaC/Chapter01/Chapter01.html",
    "title": "DLFaC Chapter 01: Introduction to the Deep Learning",
    "section": "",
    "text": "Chapter01 ä»‹ç»Deep Learningçš„åŸºæœ¬æ¦‚å¿µ\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture01/lec01.html#character-level-tokenization",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture01/lec01.html#character-level-tokenization",
    "title": "Lecture 01: Introduction & Tokenization",
    "section": "2.1 Character-level Tokenization",
    "text": "2.1 Character-level Tokenization\nCharacter-level Tokenizationæ˜¯å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå•ä¸ªå­—ç¬¦ã€‚ ä¾‹å¦‚ï¼Œå¥å­ â€œHello, world!â€ ä¼šè¢«æ‹†åˆ†ä¸ºä»¥ä¸‹tokensï¼š\n['H', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!']\néœ€è¦çŸ¥é“çš„å°±æ˜¯ï¼Œæ¯ä¸ªCharacterå°±æ˜¯ä¸€ä¸ªtokenï¼Œåˆ©ç”¨Pythonï¼Œçš„ord()å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶è½¬æ¢ä¸ºå¯¹åº”çš„æ•´æ•°IDï¼š\ntext = \"Hello, world!\"\ntokens = [char for char in text]\ntoken_ids = [ord(char) for char in tokens]\nprint(tokens)\nprint(token_ids)\nè¿™ç§æ–¹å¼å¾ˆç®€å•ï¼Œä¹Ÿå¾ˆç›´è§‚ï¼Œä½†æ˜¯å®ƒæœ‰ä¸€äº›æ˜æ˜¾çš„ç¼ºç‚¹ï¼š\n\nVocabulary Sizeï¼šå¯¹äºæ‰€æœ‰å¯èƒ½çš„å­—ç¬¦ï¼ˆåŒ…æ‹¬å­—æ¯ã€æ•°å­—ã€æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦ï¼‰ï¼ŒVocabulary Sizeä¼šéå¸¸å¤§ï¼Œå¯¼è‡´æ¨¡å‹å‚æ•°é‡å¢åŠ ã€‚ï¼ˆå¤§çº¦æœ‰150K ä¸ªä¸åŒçš„å­—ç¬¦ï¼‰\n150Kä¸ªå­—ç¬¦ä¸­ï¼Œæœ‰å¾ˆå¤šå­—ç¬¦æ˜¯éå¸¸å°‘è§çš„ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥å­¦ä¹ åˆ°è¿™äº›å­—ç¬¦çš„è¡¨ç¤ºã€‚\nè¯­ä¹‰ä¿¡æ¯ç¼ºå¤±ï¼šå•ä¸ªå­—ç¬¦æ— æ³•æ•æ‰åˆ°è¯è¯­çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥ç†è§£ä¸Šä¸‹æ–‡ã€‚"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture01/lec01.html#word-level-tokenization",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture01/lec01.html#word-level-tokenization",
    "title": "Lecture 01: Introduction & Tokenization",
    "section": "2.2 Word-level Tokenization",
    "text": "2.2 Word-level Tokenization\nä¸Character-level Tokenizationä¸åŒï¼ŒWord-level Tokenizationæ˜¯å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå•è¯ã€‚ ä¾‹å¦‚ï¼Œå¥å­ â€œHello, world!â€ ä¼šè¢«æ‹†åˆ†ä¸ºä»¥ä¸‹tokensï¼š\n['Hello,', 'world!']\næ¯ä¸ªå•è¯å°±æ˜¯ä¸€ä¸ªtokenï¼ŒåŒæ ·åœ°ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ä¸ªç®€å•çš„Pythonä»£ç å°†å•è¯è½¬æ¢ä¸ºå¯¹åº”çš„æ•´æ•°IDï¼š\nimport regex\n\ntext= \"Hello, world!\"\ntokens = text.split()  # ç®€å•çš„ç©ºæ ¼æ‹†åˆ†\nvocab = {word: idx for idx, word in enumerate(set(tokens))}\ntoken_ids = [vocab[word] for word in tokens]\nprint(tokens)\nprint(token_ids)\né™¤äº†ç®€å•çš„æ ¹æ®ç©ºæ ¼æ‹†åˆ†å•è¯çš„æ–¹æ³•ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æœ‰ç¨å¾®å¤æ‚ä¸€ç‚¹çš„æ–¹æ³•ï¼Œæ¯”å¦‚ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¥å¤„ç†æ ‡ç‚¹ç¬¦å·ç­‰ã€‚ä¸¾ä¸ªä¾‹å­ï¼ŒGPT-2çš„tokenizerå°±æ˜¯ä½¿ç”¨äº†ä¸€ç§åŸºäºæ­£åˆ™è¡¨è¾¾å¼çš„æ–¹æ³•æ¥è¿›è¡ŒWord-level Tokenizationã€‚\nGPT2_TOKENIZER_REGEX = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\nå½“ç„¶ï¼Œè¿™ç§æ–¹æ³•ä¹Ÿæœ‰æ¯”è¾ƒæ˜æ˜¾çš„ç¼ºç‚¹ï¼š\n\nVocabulary Size ä¾ç„¶å¾ˆå¤§ï¼šå¯¹äºæ‰€æœ‰å¯èƒ½çš„å•è¯ï¼ŒVocabulary Sizeä¼šéå¸¸å¤§ï¼Œå¯¼è‡´æ¨¡å‹å‚æ•°é‡å¢åŠ ã€‚\næœªç™»å½•è¯é—®é¢˜ï¼ˆOut-of-Vocabulary, OOVï¼‰ï¼šå¯¹äºè®­ç»ƒé›†ä¸­æœªå‡ºç°çš„å•è¯ï¼Œæ¨¡å‹æ— æ³•å¤„ç†ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ å°½ç®¡æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€äº›æ–¹æ³•ï¼ˆå¦‚ä½¿ç”¨ç‰¹æ®Šçš„&lt;UNK&gt; tokenï¼‰æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†ä»ç„¶æ— æ³•å®Œå…¨è§£å†³ã€‚\nVocabulary Size çš„å¤§å°ä¸æ˜¯å›ºå®šçš„ï¼Œéšç€è®­ç»ƒæ•°æ®çš„å¢åŠ ï¼ŒVocabulary Sizeä¼šä¸æ–­å¢åŠ ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥æ‰©å±•ã€‚\nå¾ˆå¤šå•è¯æ˜¯éå¸¸å°‘è§çš„ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥å­¦ä¹ åˆ°è¿™äº›å•è¯çš„è¡¨ç¤ºã€‚"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture01/lec01.html#byte-based-tokenization",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture01/lec01.html#byte-based-tokenization",
    "title": "Lecture 01: Introduction & Tokenization",
    "section": "2.3 Byte-Based Tokenization",
    "text": "2.3 Byte-Based Tokenization\nåœ¨å­¦ä¹ Byte Pair Encoding (BPE)ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆä»‹ç»ä¸€ä¸‹Byte-Based Tokenizationã€‚ Byte-Based Tokenizationæ˜¯å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå­—èŠ‚å•å…ƒï¼ˆbyte-level tokensï¼‰ã€‚ ä¾‹å¦‚ï¼Œå¥å­ â€œHello, world!â€ ä¼šè¢«æ‹†åˆ†ä¸ºä»¥ä¸‹tokensï¼š\n['H', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!']\næ¯ä¸ªå­—èŠ‚å°±æ˜¯ä¸€ä¸ªtokenï¼ŒåŒæ ·åœ°ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ä¸ªç®€å•çš„Pythonä»£ç å°†å­—èŠ‚è½¬æ¢ä¸ºå¯¹åº”çš„æ•´æ•°IDï¼š\ntext = \"Hello, world!\"\ntokens = [char.encode('utf-8') for char in text]\ntoken_ids = [byte[0] for byte in tokens]\nprint(tokens)\nprint(token_ids)\nè¿™ç§æ–¹æ³•çš„ä¼˜ç‚¹æ˜¯ï¼š\n\nVocabulary Size å›ºå®šä¸”è¾ƒå°ï¼šç”±äºå­—èŠ‚çš„èŒƒå›´æ˜¯0-255ï¼ŒVocabulary Sizeå›ºå®šä¸º256ï¼Œæ¨¡å‹å‚æ•°é‡è¾ƒå°ã€‚\næ— OOVé—®é¢˜ï¼šç”±äºæ‰€æœ‰æ–‡æœ¬éƒ½å¯ä»¥è¡¨ç¤ºä¸ºå­—èŠ‚åºåˆ—ï¼Œä¸å­˜åœ¨OOVçš„é—®é¢˜ã€‚\né€‚ç”¨äºå¤šè¯­è¨€æ–‡æœ¬ï¼šå­—èŠ‚çº§åˆ«çš„è¡¨ç¤ºå¯ä»¥å¤„ç†å„ç§è¯­è¨€çš„æ–‡æœ¬ã€‚\nç®€å•é«˜æ•ˆï¼šå­—èŠ‚çº§åˆ«çš„è¡¨ç¤ºç®€å•ä¸”é«˜æ•ˆï¼Œé€‚åˆå¤§è§„æ¨¡æ–‡æœ¬å¤„ç†ã€‚\n\nä¸è¿‡ï¼Œè¿™ç§æ–¹æ³•æœ‰ä¸ªæ˜æ˜¾çš„ç¼ºç‚¹å°±æ˜¯Compression Ratioè¾ƒä½ã€‚ ç”±äºå­—èŠ‚çº§åˆ«çš„è¡¨ç¤ºè¿‡äºç»†ç²’åº¦ï¼Œå¯¼è‡´æ–‡æœ¬é•¿åº¦å¢åŠ ï¼Œå½±å“æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚å› ä¸ºTransformeræ¨¡å‹çš„è®¡ç®—å¤æ‚åº¦ä¸è¾“å…¥é•¿åº¦çš„å¹³æ–¹æˆæ­£æ¯”ï¼Œè¾“å…¥é•¿åº¦å¢åŠ ä¼šæ˜¾è‘—å¢åŠ è®¡ç®—èµ„æºçš„æ¶ˆè€—ã€‚"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture01/lec01.html#byte-pair-encoding-bpe",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture01/lec01.html#byte-pair-encoding-bpe",
    "title": "Lecture 01: Introduction & Tokenization",
    "section": "2.4 Byte Pair Encoding (BPE)",
    "text": "2.4 Byte Pair Encoding (BPE)\nä¸ºäº†å…‹æœCharacter-levelå’ŒWord-level Tokenizationçš„ç¼ºç‚¹ï¼ŒåŒæ—¶æé«˜Byte-Based Tokenizationçš„Compression Ratioï¼Œæˆ‘ä»¬å¼•å…¥äº†Byte Pair Encoding (BPE)ç®—æ³• (Sennrich, Haddow, and Birch 2016) ã€‚ BPEæ˜¯ä¸€ç§åŸºäºé¢‘ç‡çš„å­è¯å•å…ƒï¼ˆsubword unitï¼‰åˆ†è¯æ–¹æ³•ã€‚ å®ƒçš„åŸºæœ¬æ€æƒ³æ˜¯é€šè¿‡è¿­ä»£åœ°åˆå¹¶æœ€é¢‘ç¹å‡ºç°çš„å­—ç¬¦å¯¹ï¼ˆbyte pairsï¼‰æ¥æ„å»ºä¸€ä¸ªæ›´ç´§å‡‘çš„è¯æ±‡è¡¨ã€‚\n\n\n\nBPEç®—æ³•çš„æ­¥éª¤å¦‚ä¸‹ï¼š\n\nåˆå§‹åŒ–Vocabularyï¼šå°†æ–‡æœ¬ä¸­çš„æ‰€æœ‰å”¯ä¸€å­—ç¬¦ä½œä¸ºåˆå§‹çš„è¯æ±‡è¡¨ï¼ˆvocabularyï¼‰ã€‚\nç»Ÿè®¡é¢‘ç‡ get_statsï¼šè®¡ç®—æ–‡æœ¬ä¸­æ‰€æœ‰ç›¸é‚»å­—ç¬¦å¯¹çš„å‡ºç°é¢‘ç‡ã€‚\nåˆå¹¶å­—ç¬¦å¯¹å¹¶ä¸”æ›´æ–°Vocabularyï¼šé€‰æ‹©å‡ºç°é¢‘ç‡æœ€é«˜çš„å­—ç¬¦å¯¹ï¼Œå°†å…¶åˆå¹¶ä¸ºä¸€ä¸ªæ–°çš„tokenï¼Œå¹¶æ›´æ–°æ–‡æœ¬ä¸­çš„æ‰€æœ‰å‡ºç°è¯¥å­—ç¬¦å¯¹çš„åœ°æ–¹ï¼Œå¹¶ä¸”å°†æ–°tokenæ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ã€‚\né‡å¤æ­¥éª¤2-4ï¼šé‡å¤ä¸Šè¿°æ­¥éª¤ï¼Œç›´åˆ°è¾¾åˆ°é¢„å®šçš„è¯æ±‡è¡¨å¤§å°æˆ–æ»¡è¶³å…¶ä»–åœæ­¢æ¡ä»¶ã€‚\n\n\n\n\n\n\n\n\n\nFigureÂ 4: BPEç®—æ³•çš„ç¤ºæ„å›¾ï¼Œå±•ç¤ºäº†å­—ç¬¦å¯¹çš„åˆå¹¶è¿‡ç¨‹ã€‚\n\n\n\n\n\n\nä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„BPEç®—æ³•çš„Pythonå®ç°ç¤ºä¾‹ï¼š\ndef train_bpe(string: str, num_merges: int):\n    indices = list(map(int, string.encode(\"utf-8\"))) \n    merges: dict[tuple[int, int], int] = {}  \n    vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}  \n\n    for i in range(num_merges):\n        counts = defaultdict(int)\n        for index1, index2 in zip(indices, indices[1:]): \n            counts[(index1, index2)] += 1\n        \n        pair = max(counts, key=counts.get)  # @inspect pair\n        index1, index2 = pair\n\n        new_index = 256 + i\n        merges[pair] = new_index\n        vocab[new_index] = vocab[index1] + vocab[index2]\n        indices = merge(indices, pair, new_index)\n\n    return merges, vocab\nä»¥ä¸Šæ˜¯æœ€ç®€å•çš„BPEç®—æ³•çš„å®ç°ï¼Œæ˜¾ç„¶æœ‰å¾ˆå¤šçš„ä¸è¶³ï¼Œæˆ‘ä»¬åœ¨Assignment 01çš„ç¬¬ä¸€éƒ¨åˆ†ä¸­ï¼Œä¼šä¼˜åŒ–è¿™ä¸ªå®ç°ï¼š\n\nä¼˜åŒ– merge å‡½æ•°çš„æ•ˆç‡ã€‚\nåˆ©ç”¨pre-tokenizationæ¥åŠ é€ŸBPEçš„è®­ç»ƒè¿‡ç¨‹ã€‚\n\nç­‰\nå¯¹äºé‚£äº›æƒ³æ·±å…¥ç†è§£BPEç®—æ³•çš„åŒå­¦ï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹èµ„æºï¼š \nå…³äºBPEç®—æ³•ï¼Œè¿˜æœ‰å¾ˆå¤šå¯ä»¥ä¼˜åŒ–çš„åœ°æ–¹ï¼Œæ¯”å¦‚ï¼š\n\nåœ¨å¯»æ‰¾æœ€é¢‘ç¹å­—ç¬¦å¯¹æ—¶ï¼Œå¯ä»¥ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç»“æ„ï¼ˆå¦‚ä¼˜å…ˆé˜Ÿåˆ—ï¼‰æ¥åŠ é€ŸæŸ¥æ‰¾è¿‡ç¨‹ã€‚\nåœ¨åˆå¹¶å­—ç¬¦å¹¶ä¸”æ›´æ–°æ–‡æœ¬æ—¶ï¼Œå¯ä»¥ä½¿ç”¨æ›´é«˜æ•ˆçš„å­—ç¬¦ä¸²å¤„ç†æ–¹æ³•æ¥å‡å°‘æ—¶é—´å¤æ‚åº¦ã€‚è¿™äº›ä¼˜åŒ–å¯ä»¥æ˜¾è‘—æé«˜BPEç®—æ³•çš„è®­ç»ƒé€Ÿåº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®\n\nè¿™äº›æ–¹æ³•æˆ‘ä»¬å°†åœ¨ Assignment 01ä¸­è¿›è¡Œæ›´åŠ è¯¦ç»†çš„ä»‹ç»ã€‚"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Assignments/Ass01/ass01.html#bpe-algorithm-recap",
    "href": "posts/LearningNotes/CS336/Assignments/Ass01/ass01.html#bpe-algorithm-recap",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "",
    "text": "å›é¡¾ä¸€ä¸‹BPEç®—æ³•çš„åŸºæœ¬æ­¥éª¤ï¼š 1. Initialization: å°†è¾“å…¥æ–‡æœ¬è§†ä¸ºå­—èŠ‚åºåˆ—ï¼Œæ¯ä¸ªå­—èŠ‚ä½œä¸ºä¸€ä¸ªtokenã€‚åˆå§‹åŒ–è¯æ±‡è¡¨åŒ…å«æ‰€æœ‰å¯èƒ½çš„å­—èŠ‚ï¼ˆ0-255ï¼‰ã€‚ 2. Count Pairs: ç»Ÿè®¡æ–‡æœ¬ä¸­æ‰€æœ‰ç›¸é‚»å­—èŠ‚å¯¹çš„å‡ºç°é¢‘ç‡ã€‚ 3. Merge Pairs: æ‰¾åˆ°å‡ºç°é¢‘ç‡æœ€é«˜çš„å­—èŠ‚å¯¹ï¼Œå¹¶å°†å…¶åˆå¹¶ä¸ºä¸€ä¸ªæ–°çš„tokenï¼Œæ›´æ–°æ–‡æœ¬å’Œè¯æ±‡è¡¨ã€‚ 4. Repeat: é‡å¤æ­¥éª¤2å’Œ3ï¼Œç›´åˆ°è¾¾åˆ°é¢„å®šçš„åˆå¹¶æ¬¡æ•°"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#memory-accounting",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#memory-accounting",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "1.1 Memory Accounting",
    "text": "1.1 Memory Accounting\næ‰€æœ‰çš„æ•°æ®ï¼ˆåŒ…æ‹¬æ¨¡å‹å‚æ•°ã€æ¿€æ´»å€¼ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€ç­‰ï¼‰éƒ½æ˜¯ä»¥ Tensor çš„å½¢å¼å‚¨å­˜ã€‚æˆ‘ä»¬æœ‰å¾ˆå¤šç§æ–¹å¼åˆ›å»ºä¸€ä¸ª Tensorï¼Œæ¯”å¦‚ï¼š\nx = torch.tensor([[1., 2, 3], [4, 5, 6]])\nx= torch.randn(3, 4)\nx = torch.zeros(2, 5)\nx = torch.empty(10, 10)\næ¯ä¸ª Tensor éƒ½æœ‰ä¸€ä¸ªæ•°æ®ç±»å‹ (Data Type)ï¼Œé»˜è®¤çš„æ•°æ®ç±»å‹æ˜¯ float32 (ä¹Ÿç§°ä¸º FP32)ã€‚ä¸åŒçš„æ•°æ®ç±»å‹ä¼šå ç”¨ä¸åŒçš„å†…å­˜ç©ºé—´ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬æ¥çœ‹çœ‹å‡ ç§å¸¸è§çš„æ•°æ®ç±»å‹åŠå…¶å†…å­˜å ç”¨\n\n1.1.1 Common Data Types\nåœ¨äº†è§£ä¸åŒçš„Float Typesä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆæ¥äº†è§£ä¸€ä¸‹æµ®ç‚¹æ•°çš„è¡¨ç¤ºæ–¹æ³•ã€‚è®¡ç®—æœºä¸­çš„æµ®ç‚¹æ•°é€šå¸¸é‡‡ç”¨ IEEE 754 æ ‡å‡†è¿›è¡Œè¡¨ç¤ºã€‚æµ®ç‚¹æ•°ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼šç¬¦å·ä½ (Sign Bit)ã€æŒ‡æ•°ä½ (Exponent Bits) å’Œå°¾æ•°ä½ (Mantissa Bits), ä¹Ÿå«Fraction ã€‚\n\n\n\n\n\n\nFigureÂ 1: æµ®ç‚¹æ•°çš„è¡¨ç¤º\n\n\n\næµ®ç‚¹æ•°çš„å€¼å¯ä»¥é€šè¿‡ä»¥ä¸‹å…¬å¼è®¡ç®—ï¼š \\[\nnumber = (-1)^{Sign} \\times Base^{(Exponent - Bias)} \\times 1.Mantissa\n\\tag{1}\\]\nå…¶ä¸­ï¼ŒBase é€šå¸¸ä¸º2ï¼ŒBias æ˜¯ä¸€ä¸ªç”¨äºè°ƒæ•´æŒ‡æ•°çš„åç§»é‡ï¼Œå…·ä½“å–å†³äºæŒ‡æ•°ä½çš„é•¿åº¦, é€šå¸¸ä¸º2çš„æŒ‡æ•°ä½é•¿åº¦å‡1çš„å€¼ ä¸º127ã€‚\n = b_1Â·2^{-1} + b_2Â·2^{-2} + b_3Â·2^{-3} + â€¦ + b_{23}Â·2^{-23}\nå¯¹äº FigureÂ 1 ä¸­çš„æµ®ç‚¹æ•°è¡¨ç¤ºï¼š\n\nç¬¦å·ä½ (Sign) ä¸º 0ï¼Œè¡¨ç¤ºæ­£æ•°\næŒ‡æ•°ä½ (Exponent) ä¸º 01111100ï¼Œè½¬æ¢ä¸ºåè¿›åˆ¶ä¸º 124ï¼Œå‡å» Bias 127 å¾—åˆ° -3\nå°¾æ•°ä½ (Mantissa) ä¸º 01000000000000000000000 ï¼Œè½¬æ¢ä¸ºåè¿›åˆ¶ä¸º 0.25ï¼Œå› æ­¤ 1.Mantissa = 1 + 0.25 = 1.25\n\nå°†è¿™äº›å€¼ä»£å…¥å…¬å¼ EquationÂ 1 ä¸­ï¼Œå¯ä»¥è®¡ç®—å‡ºæµ®ç‚¹æ•°çš„å€¼ä¸ºï¼š \\[\nnumber = (-1)^0 \\times 2^{-3} \\times 1.25 = 0.15625\n\\]\næ¯”å¦‚ bias 10000000ï¼Œè½¬æ¢ä¸ºåè¿›åˆ¶ä¸º 128ï¼Œå‡å» Bias 127 å¾—åˆ° 1\n\n\n\n\n\n\nFigureÂ 2: é€šè¿‡è¿™ä¸ªåœ¨çº¿è®¡ç®—å™¨ï¼ŒéªŒè¯æµ®ç‚¹æ•°çš„è¡¨ç¤ºæ–¹æ³•ã€‚\n\n\n\næ˜ç™½äº†Float Numberçš„è®¡ç®—æ–¹æ³•ä¹‹åï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å‡ ç§å¸¸è§çš„æ•°æ®ç±»å‹åŠå…¶å†…å­˜å ç”¨ã€‚\n\n1.1.1.1 Float32\n\n\n\n\n\n\nFigureÂ 3: Float32 ä½¿ç”¨32ä½ (4å­—èŠ‚) æ¥è¡¨ç¤ºä¸€ä¸ªæµ®ç‚¹æ•°ã€‚å®ƒç”±1ä½ç¬¦å·ä½ã€8ä½æŒ‡æ•°ä½å’Œ23ä½å°¾æ•°ä½ç»„æˆï¼Œå¯ä»¥è¡¨ç¤ºå¤§çº¦7ä½åè¿›åˆ¶æœ‰æ•ˆæ•°å­—ã€‚\n\n\n\nFloat32 ä¹Ÿå« single precision æ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€å¸¸ç”¨çš„æ•°æ®ç±»å‹ï¼Œå‡ ä¹æ‰€æœ‰çš„æ·±åº¦å­¦ä¹ æ¡†æ¶éƒ½é»˜è®¤ä½¿ç”¨ Float32 ä½œä¸ºå¼ é‡çš„æ•°æ®ç±»å‹ã€‚ Float32 å¯ä»¥è¡¨ç¤ºçš„æ•°å€¼èŒƒå›´å¤§çº¦åœ¨ 1.18e-38 åˆ° 3.4e+38 ä¹‹é—´ï¼Œè¶³ä»¥æ»¡è¶³å¤§å¤šæ•°æ·±åº¦å­¦ä¹ ä»»åŠ¡çš„éœ€æ±‚ã€‚\nx = torch.tensor([1.0, 2.0, 3.0])\nprint(x.dtype)  # è¾“å‡º: torch.float32\nprint(x.element_size())  # è¾“å‡º: 4 (æ¯ä¸ªå…ƒç´ å ç”¨4å­—èŠ‚)\n\n\n\n\n\n\nFigureÂ 4: Float16 ä½¿ç”¨16ä½ (2å­—èŠ‚) æ¥è¡¨ç¤ºä¸€ä¸ªæµ®ç‚¹æ•°ã€‚å®ƒç”±1ä½ç¬¦å·ä½ã€5ä½æŒ‡æ•°ä½å’Œ10ä½å°¾æ•°ä½ç»„æˆï¼Œå¯ä»¥è¡¨ç¤ºå¤§çº¦3ä½åè¿›åˆ¶æœ‰æ•ˆæ•°å­—ã€‚\n\n\n\nFloat16 ä¹Ÿå« half precisionï¼Œä¸»è¦ç”¨äºå‡å°‘å†…å­˜å ç”¨å’ŒåŠ é€Ÿè®¡ç®—ã€‚ç›¸æ¯”äº Float32ï¼ŒFloat16 å¯ä»¥æ˜¾è‘—é™ä½å†…å­˜ä½¿ç”¨é‡ï¼Œä»è€Œå…è®¸æˆ‘ä»¬è®­ç»ƒæ›´å¤§çš„æ¨¡å‹æˆ–è€…ä½¿ç”¨æ›´å¤§çš„æ‰¹é‡å¤§å° (Batch Size)ã€‚\nFloat16 å¯ä»¥è¡¨ç¤ºçš„æ•°å€¼èŒƒå›´å¤§çº¦åœ¨ 6.1e-5 åˆ° 6.5e+4 ä¹‹é—´ï¼Œç›¸æ¯”äº Float32 æœ‰ä¸€å®šçš„é™åˆ¶ï¼Œå°¤å…¶æ˜¯åœ¨è¡¨ç¤ºéå¸¸å°æˆ–è€…éå¸¸å¤§çš„æ•°å€¼æ—¶å¯èƒ½ä¼šå‡ºç°æº¢å‡ºï¼ˆOverflowï¼‰æˆ–è€…ä¸‹æº¢ï¼ˆUnderflowï¼‰çš„é—®é¢˜ã€‚å½“è¿™ä¸ªé—®é¢˜å‡ºç°æ—¶ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå‡ºç°NaNçš„æƒ…å†µï¼Œç”±æ­¤å¯¼è‡´è®­ç»ƒå¤±è´¥ã€‚\n\n\n\n\n\n\nTip\n\n\n\nå½“æˆ‘ä»¬è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼ŒæŸå¤±å‡½æ•°å‡ºç°NaNçš„æƒ…å†µï¼Œé€šå¸¸æ˜¯å› ä¸ºæ•°å€¼æº¢å‡ºæˆ–è€…ä¸‹æº¢å¯¼è‡´çš„ã€‚\n\n\nIn [14]: x = torch.tensor([1e-8], dtype=torch.float16)\n\nIn [15]: x\nOut[15]: tensor([0.], dtype=torch.float16)\n\nIn [8]: x = torch.tensor([1e+8], dtype=torch.float16)\n\nIn [9]: x\nOut[9]: tensor([inf], dtype=torch.float16)\n\n\n1.1.1.2 BFloat16\n\n\n\n\n\n\nFigureÂ 5: BFloat16 ä½¿ç”¨16ä½ (2å­—èŠ‚) æ¥è¡¨ç¤ºä¸€ä¸ªæµ®ç‚¹æ•°ã€‚å®ƒç”±1ä½ç¬¦å·ä½ã€8ä½æŒ‡æ•°ä½å’Œ7ä½å°¾æ•°ä½ç»„æˆï¼Œå¯ä»¥è¡¨ç¤ºå¤§çº¦3ä½åè¿›åˆ¶æœ‰æ•ˆæ•°å­—ã€‚\n\n\n\nGoogle åœ¨2018å¹´æå‡ºäº† BFloat16 (Brain Float Point 16) æ•°æ®ç±»å‹ï¼Œä¸»è¦ç”¨äºæ·±åº¦å­¦ä¹ åŠ é€Ÿã€‚ç›¸æ¯”äº Float16ï¼ŒBFloat16 ä¿ç•™äº†ä¸ Float32 ç›¸åŒçš„æŒ‡æ•°ä½é•¿åº¦ï¼Œå› æ­¤å¯ä»¥è¡¨ç¤ºæ›´å¤§çš„æ•°å€¼èŒƒå›´ï¼Œä»è€Œå‡å°‘äº†æº¢å‡ºå’Œä¸‹æº¢çš„é£é™©ã€‚ç›¸å¯¹äºFloat32ï¼Œ BFloat16 çš„ç²¾åº¦è¾ƒä½ï¼Œä½†åœ¨è®¸å¤šæ·±åº¦å­¦ä¹ ä»»åŠ¡ä¸­ï¼ŒBFloat16 çš„ç²¾åº¦å·²ç»è¶³å¤Ÿä½¿ç”¨ã€‚\nIn [11]: x\nOut[11]: tensor([1.0014e+08], dtype=torch.bfloat16)\n\nIn [12]: x = torch.tensor([1e-8], dtype=torch.bfloat16)\n\nIn [13]: x\nOut[13]: tensor([1.0012e-08], dtype=torch.bfloat16)\nIn [16]: torch.finfo(torch.float32)\nOut[16]: finfo(resolution=1e-06, min=-3.40282e+38, max=3.40282e+38, eps=1.19209e-07, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=float32)\n\nIn [17]: torch.finfo(torch.float16)\nOut[17]: finfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16)\n\nIn [18]: torch.finfo(torch.bfloat16)\nOut[18]: finfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)\n\n\n1.1.1.3 FP8\nFP8 æ˜¯ä¸€ç§8ä½æµ®ç‚¹æ•°è¡¨ç¤ºæ–¹æ³•ï¼Œé€šå¸¸ç”¨äºæç«¯å†…å­˜å—é™çš„åœºæ™¯ï¼ˆæ¯”å¦‚å°†æ¨¡å‹éƒ¨ç½²åˆ°è¾¹ç¼˜è®¾å¤‡ï¼‰ã€‚FP8 æœ‰ä¸¤ç§ä¸»è¦æ ¼å¼ï¼šE4M3 å’Œ E5M2ã€‚\n\n\n\n\n\n\nFigureÂ 6: FP8 ä½¿ç”¨8ä½ (1å­—èŠ‚) æ¥è¡¨ç¤ºä¸€ä¸ªæµ®ç‚¹æ•°ã€‚E4M3 ç”±1ä½ç¬¦å·ä½ã€4ä½æŒ‡æ•°ä½å’Œ3ä½å°¾æ•°ä½ç»„æˆï¼›E5M2 ç”±1ä½ç¬¦å·ä½ã€5ä½æŒ‡æ•°ä½å’Œ2ä½å°¾æ•°ä½ç»„æˆã€‚\n\n\n\nE4M3 (range [-448, 448]) and E5M2 ([-57344, 57344]).\n\n\n1.1.1.4 Other Data Types\né™¤äº†ä¸Šè¿°å‡ ç§å¸¸è§çš„æ•°æ®ç±»å‹ä¹‹å¤–ï¼Œè¿˜æœ‰ä¸€äº›å…¶ä»–çš„æ•°æ®ç±»å‹ï¼Œæ¯”å¦‚ int8 å’Œ int4ã€‚è¿™äº›æ•°æ®ç±»å‹é€šå¸¸ç”¨äºé‡åŒ– (Quantization) æŠ€æœ¯ï¼Œé€šè¿‡å°†æµ®ç‚¹æ•°è½¬æ¢ä¸ºæ•´æ•°æ¥å‡å°‘å†…å­˜å ç”¨å’ŒåŠ é€Ÿè®¡ç®—ã€‚\n\n\n\nData Type\nDescription\nBits per Value\nBytes per Value\n\n\n\n\nfloat32\nSingle Precision\n32\n4\n\n\nfloat16\nHalf Precision\n16\n2\n\n\nbfloat16\nBrain Float\n16\n2\n\n\nint8\n8-bit Integer\n8\n1\n\n\nint4\n4-bit Integer\n4\n0.5\n\n\n\næˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œä¸åŒçš„æ•°æ®ç±»å‹æœ‰ä¸åŒçš„å†…å­˜å ç”¨ã€‚é€‰æ‹©åˆé€‚çš„æ•°æ®ç±»å‹å¯ä»¥å¸®åŠ©æˆ‘ä»¬åœ¨å†…å­˜å—é™çš„æƒ…å†µä¸‹è®­ç»ƒæ›´å¤§çš„æ¨¡å‹æˆ–è€…ä½¿ç”¨æ›´å¤§çš„æ‰¹é‡å¤§å°ï¼š\n\nç”¨ float32 è¿›è¡Œè®­ç»ƒï¼Œé€‚ç”¨äºå¤§å¤šæ•°ä»»åŠ¡ï¼Œä½†å†…å­˜å ç”¨è¾ƒé«˜ã€‚\nç”¨ float16 è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘å†…å­˜å ç”¨ï¼Œä½†éœ€è¦æ³¨æ„æ•°å€¼æº¢å‡ºå’Œä¸‹æº¢çš„é—®é¢˜ã€‚\nç”¨ bfloat16 è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥åœ¨å‡å°‘å†…å­˜å ç”¨çš„åŒæ—¶ï¼Œä¿æŒè¾ƒå¤§çš„æ•°å€¼èŒƒå›´ï¼Œé€‚ç”¨äºå¤§å¤šæ•°æ·±åº¦å­¦ä¹ ä»»åŠ¡ã€‚\nç”¨ int8 æˆ– int4 è¿›è¡Œé‡åŒ–è®­ç»ƒï¼Œå¯ä»¥æå¤§åœ°å‡å°‘å†…å­˜å ç”¨ï¼Œä½†éœ€è¦è¿›è¡Œé¢å¤–çš„é‡åŒ–å’Œåé‡åŒ–æ“ä½œï¼Œé€‚ç”¨äºéƒ¨ç½²é˜¶æ®µã€‚\n\nå› æ­¤åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸é‡‡ç”¨ä¸€ç§Mixed Precision Trainingçš„æ–¹æ³•ï¼Œæ¥å¹³è¡¡å†…å­˜å ç”¨å’Œæ•°å€¼ç²¾åº¦çš„é—®é¢˜ã€‚"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#section",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#section",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "1.2 ",
    "text": "1.2"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#compute-accounting",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#compute-accounting",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "1.2 Compute Accounting",
    "text": "1.2 Compute Accounting\nåœ¨äº†è§£äº†å†…å­˜å ç”¨ä¹‹åï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹è®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚è®¡ç®—èµ„æºä¸»è¦å–å†³äºæ¨¡å‹çš„å‚æ•°é‡å’Œè®­ç»ƒæ•°æ®çš„è§„æ¨¡ã€‚\n\n1.2.1 Tensor on GPUs\nå½“æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª Tensor æ—¶ï¼Œå®ƒé»˜è®¤æ˜¯åœ¨ CPU ä¸Šåˆ›å»ºçš„ã€‚å¦‚æœæˆ‘ä»¬æƒ³è¦åœ¨ GPU ä¸Šè¿›è¡Œè®¡ç®—ï¼Œéœ€è¦å°† Tensor ç§»åŠ¨åˆ° GPU ä¸Šï¼š\nx = torch.tensor([1.0, 2.0, 3.0])\nx.device  # è¾“å‡º: cpu\nx = x.cuda()  # å°† Tensor ç§»åŠ¨åˆ° GPU ä¸Š\nx.device  # è¾“å‡º: cuda:0\n\n\n\n\n\n\nFigureÂ 7: æˆ‘ä»¬å°†Tensorä»CPUçš„RAMä¸Šï¼Œç§»åŠ¨åˆ°DRAMä¸Šã€‚\n\n\n\næˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åˆ›å»ºä¸€ä¸ªç›´æ¥åœ¨ GPU ä¸Šçš„ Tensorï¼š\nx = torch.tensor([1.0, 2.0, 3.0], device='cuda')\nå¯ä»¥é€šè¿‡GPUçš„å†…å­˜æƒ…å†µæ¥æŸ¥çœ‹å½“å‰ GPU ä¸Šçš„å†…å­˜ä½¿ç”¨æƒ…å†µï¼š\nimport torch\nprint(torch.cuda.memory_summary())\nmemory_allocated = torch.cuda.memory_allocated() \nx = torch.tensor([1.0, 2.0, 3.0], device='cuda')\nmemory_allocated_after = torch.cuda.memory_allocated()\nprint(f\"Memory allocated before: {memory_allocated}, after: {memory_allocated_after}\")"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#tensor-operations",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#tensor-operations",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "1.3 Tensor Operations",
    "text": "1.3 Tensor Operations\nPyTorch Tensor æ˜¯ä¸€ä¸ªPointerï¼ŒæŒ‡å‘ä¸€å—è¿ç»­çš„å†…å­˜åŒºåŸŸã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡stridesæ¥è®¿é—®Tensorä¸­çš„æ•°æ®ã€‚\n\n\n\n\n\n\nFigureÂ 8\n\n\n\nTensor æœ‰å¾ˆå¤šæ“ä½œï¼Œæ¯”å¦‚ reshape, permute, transpose ç­‰ç­‰ã€‚è¿™äº›æ“ä½œé€šå¸¸ä¸ä¼šæ”¹å˜æ•°æ®çš„å­˜å‚¨æ–¹å¼ï¼Œè€Œæ˜¯é€šè¿‡ä¿®æ”¹ strides æ¥å®ç°å¯¹æ•°æ®çš„ä¸åŒè§†å›¾ (View)ã€‚è¿™æ˜¯ååˆ†é«˜æ•ˆçš„ï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦è¿›è¡Œæ•°æ®çš„å¤åˆ¶ (Copy)ï¼Œåªéœ€è¦ä¿®æ”¹ Tensor çš„å…ƒæ•°æ® (Metadata)ã€‚ä¸è¿‡éœ€è¦å°å¿ƒçš„æ˜¯ï¼Œå½“æˆ‘ä»¬ä¿®æ”¹ Tensor çš„æ•°æ®æ—¶ï¼Œå¯èƒ½ä¼šå½±å“åˆ°åŸå§‹æ•°æ®ï¼Œå› ä¸ºå®ƒä»¬å…±äº«åŒä¸€å—å†…å­˜åŒºåŸŸã€‚\næœ‰ä¸€äº›æ“ä½œä¼šå¯¼è‡´æ•°æ®å˜å¾—ä¸è¿ç»­ (Non-Contiguous)ï¼Œæ¯”å¦‚ transpose å’Œ permuteã€‚è¿™äº›æ“ä½œä¼šæ”¹å˜æ•°æ®çš„å­˜å‚¨é¡ºåºï¼Œä»è€Œå¯¼è‡´æ•°æ®åœ¨å†…å­˜ä¸­ä¸å†æ˜¯è¿ç»­å­˜å‚¨çš„ã€‚è¿™æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ contiguous() æ–¹æ³•æ¥åˆ›å»ºä¸€ä¸ªæ–°çš„è¿ç»­å­˜å‚¨çš„ Tensorã€‚\n\n\n\n\n\n\nNote\n\n\n\n.transpose().contiguous() çš„æ“ä½œæˆ‘ä»¬åœ¨ä¸€ Attention çš„è®¡ç®—ä¸­ç»å¸¸ä¼šç”¨åˆ°ã€‚\n\n\nElement-wise æ“ä½œ (æ¯”å¦‚åŠ æ³•ã€ä¹˜æ³•ç­‰) é€šå¸¸è¦æ±‚è¾“å…¥çš„ Tensor æ˜¯è¿ç»­å­˜å‚¨çš„ã€‚å¦‚æœè¾“å…¥çš„ Tensor æ˜¯ä¸è¿ç»­çš„ï¼ŒPyTorch ä¼šè‡ªåŠ¨è°ƒç”¨ contiguous() æ–¹æ³•æ¥åˆ›å»ºä¸€ä¸ªæ–°çš„è¿ç»­å­˜å‚¨çš„ Tensorï¼Œä»è€Œä¿è¯æ“ä½œçš„æ­£ç¡®æ€§ã€‚ï¼Œæ¯”å¦‚ï¼š\nx = torch.tensor([1, 4, 9])\nx.pow(2)\nx.sqrt()\nx.rsqrt()\nx + x \nx * 2\nx.triu()\n\n1.3.1 Matrix Multiplication\næœ€é‡è¦çš„ä¹Ÿæ˜¯æœ€å¸¸ç”¨çš„æ“ä½œä¹‹ä¸€æ˜¯çŸ©é˜µä¹˜æ³• (Matrix Multiplication)ï¼Œåœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼ŒçŸ©é˜µä¹˜æ³•è¢«å¹¿æ³›åº”ç”¨äºç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­è¿‡ç¨‹ä¸­ã€‚\nA = torch.randn(3, 4)\nB = torch.randn(4, 5)\nC = torch.matmul(A, B)  # C çš„å½¢çŠ¶ä¸º (3,5)\nC = A @ B  # å¦ä¸€ç§çŸ©é˜µä¹˜æ³•çš„å†™æ³•\nC = A.mm(B)  # å¦ä¸€ç§çŸ©é˜µä¹˜æ³•çš„å†™æ³•"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#tensor-flops",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#tensor-flops",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "1.6 Tensor Flops",
    "text": "1.6 Tensor Flops\näº†è§£äº†å†…å­˜å ç”¨å’Œè®¡ç®—èµ„æºä¹‹åï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•è®¡ç®— Tensor çš„ FLOPs (Floating Point Operations)ã€‚FLOPs æ˜¯è¡¡é‡è®¡ç®—å¤æ‚åº¦çš„ä¸€ä¸ªé‡è¦æŒ‡æ ‡ï¼Œè¡¨ç¤ºæ¯ç§’é’Ÿå¯ä»¥æ‰§è¡Œå¤šå°‘æ¬¡æµ®ç‚¹è¿ç®—ã€‚å…¶ä¸­ä¸¤ä¸ªå¸¸è§çš„æŒ‡æ ‡æ˜¯ï¼š\n\nFLOPs: æ€»å…±éœ€è¦æ‰§è¡Œçš„æµ®ç‚¹è¿ç®—æ¬¡æ•°\nFLOP/s ä¹Ÿå†™ä½œFLOPSï¼š æ¯ç§’é’Ÿå¯ä»¥æ‰§è¡Œçš„æµ®ç‚¹è¿ç®—æ¬¡æ•°\n\nå¯¹äºä¸¤ä¸ª çŸ©é˜µ A (å½¢çŠ¶ä¸º m x n) å’Œ B (å½¢çŠ¶ä¸º n x p) çš„çŸ©é˜µä¹˜æ³• C = A @ Bï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å‡º FLOPs å¦‚ä¸‹ï¼š \\[\nFLOPs = 2 * m * n * p\n\\]\nå…¶ä¸­ï¼Œä¹˜æ³•æ“ä½œéœ€è¦ m * n * p æ¬¡ï¼Œ åŠ æ³•æ“ä½œä¹Ÿéœ€è¦ m * n * p æ¬¡ï¼Œå› æ­¤æ€»å…±éœ€è¦ 2 * m * n * p æ¬¡æµ®ç‚¹è¿ç®—ã€‚\nå¯¹äºå…¶ä»–çš„å¼ é‡æ“ä½œï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç±»ä¼¼åœ°è®¡ç®— FLOPsã€‚äº†è§£ FLOPs å¯ä»¥å¸®åŠ©æˆ‘ä»¬è¯„ä¼°æ¨¡å‹çš„è®¡ç®—å¤æ‚åº¦ï¼Œä»è€Œä¼˜åŒ–æ¨¡å‹è®¾è®¡å’Œè®­ç»ƒè¿‡ç¨‹ã€‚æ¯”å¦‚ï¼š\n\nElement-wise æ“ä½œ (æ¯”å¦‚åŠ æ³•ã€ä¹˜æ³•ç­‰) çš„ FLOPs é€šå¸¸ä¸å¼ é‡çš„å…ƒç´ æ•°é‡æˆæ­£æ¯” \\(\\mathcal{O}(m \\times n)\\)\nAddition of two matrices of shape (m, n): FLOPs = m * n\n\nç”±æ­¤å¯è§ï¼ŒçŸ©é˜µä¹˜æ³•çš„è®¡ç®—å¤æ‚åº¦è¿œé«˜äº Element-wise æ“ä½œï¼Œå› æ­¤åœ¨è®¾è®¡æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šå°½é‡å‡å°‘çŸ©é˜µä¹˜æ³•çš„æ¬¡æ•°ï¼Œä»è€Œé™ä½è®¡ç®—å¤æ‚åº¦ã€‚"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#gradient-calculation",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#gradient-calculation",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "1.4 Gradient Calculation",
    "text": "1.4 Gradient Calculation\näº†è§£äº†çŸ©é˜µä¹˜æ³•ä¹‹åï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•è®¡ç®—æ¢¯åº¦ (Gradient)ã€‚åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæ¢¯åº¦æ˜¯ç”¨æ¥æ›´æ–°æ¨¡å‹å‚æ•°çš„å…³é”®ã€‚PyTorch æä¾›äº†è‡ªåŠ¨å¾®åˆ† (Autograd) åŠŸèƒ½ï¼Œå¯ä»¥è‡ªåŠ¨è®¡ç®—å¼ é‡çš„æ¢¯åº¦ã€‚\nå‡è®¾æˆ‘ä»¬æœ‰ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œå±‚ï¼š\n\\[\nY = 0.5 * (X  W - 5)^2\n\\]\nåœ¨å‰ç½®çš„ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—è¾“å‡º Yï¼š\nx = torch.randn([1., 2, 3])  # è¾“å…¥å¼ é‡ X\nw = torch.randn([1., 2, 3], requires_grad=True)  # æƒé‡å¼ \ny = 0.5 * (x @ w - 5) ** 2  # å‰å‘ä¼ æ’­è®¡ç®—è¾“å‡º Y\npred_y = x @ w\nloss = 0.5 * (pred_y - 5) ** 2\nåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—æ¢¯åº¦ï¼š\nloss.backward()  # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦\nprint(w.grad)  # è¾“å‡ºæƒé‡ w çš„æ¢¯åº¦"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#summary",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#summary",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "1.5 Summary",
    "text": "1.5 Summary\nForward Pass: 2 * Number of data points * Number of parameters Backward Pass: 4 * Number of data points * Number of parameters\nTotal: 6 * Number of data points * Number of parameters\n\n1.5.1 Einops Library\nåœ¨å¤„ç†é«˜ç»´å¼ é‡æ—¶ï¼Œå¼ é‡çš„é‡æ’ (Rearrangement) å’Œå˜å½¢ (Reshaping) æ˜¯éå¸¸å¸¸è§çš„æ“ä½œã€‚ä¼ ç»Ÿçš„æ–¹æ³•é€šå¸¸éœ€è¦å¤šè¡Œä»£ç ï¼Œå¹¶ä¸”å®¹æ˜“å‡ºé”™ã€‚einops æ˜¯ä¸€ä¸ªå¼ºå¤§çš„åº“ï¼Œå¯ä»¥ç®€åŒ–è¿™äº›æ“ä½œï¼Œä½¿ä»£ç æ›´åŠ ç®€æ´å’Œæ˜“è¯»ã€‚\n\n\n\n\n\n\n\nTip\n\n\n\nå¦‚æœä»¥ä¸Šçš„å†…å®¹æ¯”è¾ƒéš¾ä»¥ç†è§£çš„è¯ï¼Œå¯ä»¥è®¿é—®è¿™ä¸ªé“¾æ¥ã€‚ é‡Œé¢æœ‰éå¸¸è¯¦ç»†çš„einopsæ•™ç¨‹ï¼ŒåŒ…å«äº†å¾ˆå¤šä¾‹å­å’Œå¯è§†åŒ–çš„å›¾ç¤ºã€‚\n\n\nåœ¨è¿™é‡Œå°±ä¸å…·ä½“å±•å¼€äº†ã€‚"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#parameter",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#parameter",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "2.1 Parameter",
    "text": "2.1 Parameter\nè®¸å¤šåŠ é€Ÿå™¨ï¼ˆæ¯”å¦‚ GPU å’Œ TPUï¼‰éƒ½å¯¹çŸ©é˜µä¹˜æ³•è¿›è¡Œäº†é«˜åº¦ä¼˜åŒ–ï¼Œåˆ©ç”¨å¹¶è¡Œè®¡ç®—å’Œä¸“ç”¨ç¡¬ä»¶å•å…ƒæ¥åŠ é€ŸçŸ©é˜µä¹˜æ³•çš„è®¡ç®—è¿‡ç¨‹ã€‚å› æ­¤ï¼Œåœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå°½é‡å°†è®¡ç®—ä»»åŠ¡è½¬åŒ–ä¸ºçŸ©é˜µä¹˜æ³•ï¼Œå¯ä»¥æ˜¾è‘—æå‡è®¡ç®—æ•ˆç‡ã€‚\n\n2.1.1 Einops Library\nåœ¨å¤„ç†é«˜ç»´å¼ é‡æ—¶ï¼Œå¼ é‡çš„é‡æ’ (Rearrangement) å’Œå˜å½¢ (Reshaping) æ˜¯éå¸¸å¸¸è§çš„æ“ä½œã€‚ä¼ ç»Ÿçš„æ–¹æ³•é€šå¸¸éœ€è¦å¤šè¡Œä»£ç ï¼Œå¹¶ä¸”å®¹æ˜“å‡ºé”™ã€‚einops æ˜¯ä¸€ä¸ªå¼ºå¤§çš„åº“ï¼Œå¯ä»¥ç®€åŒ–è¿™äº›æ“ä½œï¼Œä½¿ä»£ç æ›´åŠ ç®€æ´å’Œæ˜“è¯»ã€‚\n\n\n\n\n\n\n\nTip\n\n\n\nå¦‚æœä»¥ä¸Šçš„å†…å®¹æ¯”è¾ƒéš¾ä»¥ç†è§£çš„è¯ï¼Œå¯ä»¥è®¿é—®è¿™ä¸ªé“¾æ¥ã€‚ é‡Œé¢æœ‰éå¸¸è¯¦ç»†çš„einopsæ•™ç¨‹ï¼ŒåŒ…å«äº†å¾ˆå¤šä¾‹å­å’Œå¯è§†åŒ–çš„å›¾ç¤ºã€‚\n\n\nåœ¨è¿™é‡Œå°±ä¸å…·ä½“å±•å¼€äº†ã€‚"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#parameter-initialization",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#parameter-initialization",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "2.2 Parameter initialization",
    "text": "2.2 Parameter initialization\nå®šä¹‰å¥½æ¨¡å‹ä¹‹åï¼Œæˆ‘ä»¬éœ€è¦åˆå§‹åŒ–æ¨¡å‹çš„å‚æ•°ã€‚å‚æ•°çš„åˆå§‹åŒ–æ–¹å¼ä¼šå½±å“æ¨¡å‹çš„è®­ç»ƒæ•ˆæœå’Œæ”¶æ•›é€Ÿåº¦ã€‚å¸¸è§çš„åˆå§‹åŒ–æ–¹æ³•æœ‰éšæœºåˆå§‹åŒ– (Random Initialization)ã€Xavier åˆå§‹åŒ– (Xavier Initialization) å’Œ He åˆå§‹åŒ– (He Initialization) ç­‰ç­‰ã€‚\nè®¸å¤šåŠ é€Ÿå™¨ï¼ˆæ¯”å¦‚ GPU å’Œ TPUï¼‰éƒ½å¯¹çŸ©é˜µä¹˜æ³•è¿›è¡Œäº†é«˜åº¦ä¼˜åŒ–ï¼Œåˆ©ç”¨å¹¶è¡Œè®¡ç®—å’Œä¸“ç”¨ç¡¬ä»¶å•å…ƒæ¥åŠ é€ŸçŸ©é˜µä¹˜æ³•çš„è®¡ç®—è¿‡ç¨‹ã€‚å› æ­¤ï¼Œåœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå°½é‡å°†è®¡ç®—ä»»åŠ¡è½¬åŒ–ä¸ºçŸ©é˜µä¹˜æ³•ï¼Œå¯ä»¥æ˜¾è‘—æå‡è®¡ç®—æ•ˆç‡ã€‚"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#model-definition",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#model-definition",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "2.1 Model definition",
    "text": "2.1 Model definition\né¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªæ¨¡å‹ã€‚æ¨¡å‹é€šå¸¸ç”±å¤šä¸ªå±‚ (Layer) ç»„æˆï¼Œæ¯ä¸ªå±‚éƒ½æœ‰è‡ªå·±çš„å‚æ•° (Parameters)ã€‚ç°ä»£çš„LLMæ¨¡å‹é€šå¸¸æ˜¯åŸºäº Transformer (Vaswani et al. 2023) æ¶æ„æ„å»ºçš„ã€‚ å…·ä½“çš„å†…å®¹ï¼Œä¼šåœ¨Lecture 03ä¸­è¯¦ç»†ä»‹ç»ï¼Œåœ¨è¿™é‡Œå°±å…ˆä¸å±•å¼€äº†ã€‚\n\n\n\n\n\n\nNote\n\n\n\nå¯¹äºTransformeræ¯”è¾ƒé™Œç”Ÿçš„åŒå­¦ï¼Œå¯ä»¥å‚è€ƒæˆ‘è¿™ä¸€ç¯‡ç¬”è®° 100-Paper with Code: 01 Transformer"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#optimizer-selection",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#optimizer-selection",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "2.3 Optimizer Selection",
    "text": "2.3 Optimizer Selection\nåœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬éœ€è¦é€‰æ‹©ä¸€ä¸ªä¼˜åŒ–å™¨ (Optimizer) æ¥æ›´æ–°æ¨¡å‹çš„å‚æ•°ã€‚ å¸¸è§çš„ä¼˜åŒ–å™¨æœ‰éšæœºæ¢¯åº¦ä¸‹é™ (SGD)ã€åŠ¨é‡æ³• (Momentum)ã€Adam å’Œ AdamW ç­‰ç­‰ã€‚ä¸åŒçš„ä¼˜åŒ–å™¨æœ‰ä¸åŒçš„æ›´æ–°è§„åˆ™å’Œè¶…å‚æ•° (Hyperparameters)ï¼Œé€‰æ‹©åˆé€‚çš„ä¼˜åŒ–å™¨å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¿«åœ°æ”¶æ•›åˆ°æœ€ä¼˜è§£ã€‚"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#loss-function",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#loss-function",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "2.4 Loss Function",
    "text": "2.4 Loss Function\nåœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªæŸå¤±å‡½æ•° (Loss Function) æ¥è¡¡é‡æ¨¡å‹çš„é¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®è·ã€‚å¸¸è§çš„æŸå¤±å‡½æ•°æœ‰å‡æ–¹è¯¯å·® (Mean Squared Error, MSE)ã€äº¤å‰ç†µæŸå¤± (Cross Entropy Loss) ç­‰ç­‰ã€‚é€‰æ‹©åˆé€‚çš„æŸå¤±å‡½æ•°å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ä¼˜åŒ–"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#training",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#training",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "2.5 Training",
    "text": "2.5 Training\nåœ¨ GPU ä¸Šè¿›è¡Œè®¡ç®—æ—¶ï¼Œæ•°æ®ä¼ è¾“çš„é€Ÿåº¦é€šå¸¸æ˜¯ä¸€ä¸ªç“¶é¢ˆã€‚ä¸ºäº†æé«˜æ•°æ®ä¼ è¾“çš„æ•ˆç‡ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Pinned Memory (ä¹Ÿå« Page-Locked Memory)ã€‚Pinned Memory æ˜¯ä¸€ç§ç‰¹æ®Šçš„å†…å­˜åŒºåŸŸï¼Œå¯ä»¥åŠ é€Ÿä¸»æœº (Host) å’Œè®¾å¤‡ (Device) ä¹‹é—´çš„æ•°æ®ä¼ è¾“ã€‚\n\n\n\n\n\n\nFigureÂ 9: å¦‚å›¾æ‰€ç¤ºï¼ŒPinned memory å¯ä»¥ä½œä¸ºè®¾å¤‡(Device)åˆ°ä¸»æœº(Host)æ‹·è´çš„ä¸­è½¬åŒºï¼Œç›´æ¥åœ¨ pinned memory ä¸­åˆ†é…ä¸»æœºæ•°ç»„ï¼Œå°±èƒ½é¿å… pageable å†…å­˜ä¸ pinned å†…å­˜ä¹‹é—´çš„é¢å¤–æ‹·è´å¼€é”€ï¼Œä»è€Œæå‡æ•°æ®ä¼ è¾“æ•ˆç‡ã€‚\n\n\n\nè¿™ç¯‡æ–‡ç« ä¸­ä»‹ç»äº†4ç§å¸¸è§çš„åŠ é€Ÿæ•°æ®ä¼ è¾“çš„æ–¹æ³•ï¼š\n\nåˆ©ç”¨ Numpy Memmap å¤„ç†å¤§æ•°æ®é›†: é€šè¿‡å†…å­˜æ˜ å°„æŠ€æœ¯ï¼Œåªå°†æ•°æ®é›†çš„ä¸€éƒ¨åˆ†åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨ï¼Œæé«˜æ•°æ®åŠ è½½é€Ÿåº¦ã€‚\nå¤šåˆ©ç”¨ torch.from_numpy å‡½æ•°: ç›´æ¥å°† NumPy æ•°ç»„è½¬æ¢ä¸º PyTorch å¼ é‡ï¼Œé¿å…ä¸å¿…è¦çš„æ•°æ®å¤åˆ¶ï¼Œæé«˜æ•°æ®ä¼ è¾“æ•ˆç‡ã€‚\nå°† num_workers è®¾ç½®ä¸ºå¤§äº0: é€šè¿‡å¤šçº¿ç¨‹æ•°æ®åŠ è½½ï¼Œæé«˜æ•°æ®é¢„å¤„ç†å’ŒåŠ è½½çš„å¹¶è¡Œåº¦ï¼Œå‡å°‘æ•°æ®åŠ è½½æ—¶é—´ã€‚\nä½¿ç”¨ Pinned Memory åŠ é€Ÿä¸»æœºä¸è®¾å¤‡ä¹‹é—´çš„æ•°æ®ä¼ è¾“: é€šè¿‡å°†æ•°æ®å­˜å‚¨åœ¨å›ºå®šå†…å­˜ä¸­ï¼Œå‡å°‘æ•°æ®ä¼ è¾“çš„å»¶è¿Ÿï¼Œæé«˜\n\nfrom torch.utils.data import DataLoader\n\n# some code\n\nloader = DataLoader(your_dataset, ..., pin_memory=True)\ndata_iter = iter(loader)\n\nnext_batch = data_iter.next() # start loading the first batch\nnext_batch = [ _.cuda(non_blocking=True) for _ in next_batch ]  # with pin_memory=True and non_blocking=True, this will copy data to GPU non blockingly\n\nfor i in range(len(loader)):\n    batch = next_batch \n    if i + 2 != len(loader): \n        # start copying data of next batch\n        next_batch = data_iter.next()\n        next_batch = [ _.cuda(async=True) for _ in next_batch]\nè¿™å‡ ä¸ªæ–¹æ³•å¯ä»¥æ˜¾è‘—æå‡æ•°æ®ä¼ è¾“å’ŒåŠ è½½çš„æ•ˆç‡ï¼Œå°¤å…¶åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶æ•ˆæœå°¤ä¸ºæ˜æ˜¾ã€‚åœ¨æˆ‘ä»¬å®ŒæˆAssignment 01æ—¶ï¼Œä¼šç”¨åˆ°è¿™äº›æŠ€å·§æ¥ä¼˜åŒ–æ•°æ®åŠ è½½è¿‡ç¨‹ã€‚"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#training-loop",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#training-loop",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "2.5 Training Loop",
    "text": "2.5 Training Loop\nå½“æˆ‘ä»¬å®šä¹‰å¥½æ¨¡å‹ã€åˆå§‹åŒ–å‚æ•°ã€é€‰æ‹©ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ã€‚è®­ç»ƒè¿‡ç¨‹é€šå¸¸åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š\nåŠ è½½æ•°æ® (Data Loading): ä»æ•°æ®é›†ä¸­åŠ è½½è®­ç»ƒæ•°æ®ï¼Œé€šå¸¸ä½¿ç”¨æ‰¹é‡ (Batch) çš„æ–¹å¼è¿›è¡ŒåŠ è½½ã€‚ å‰å‘ä¼ æ’­ (Forward Pass): å°†è¾“å…¥æ•°æ®ä¼ é€’ç»™æ¨¡å‹ï¼Œè®¡ç®—æ¨¡å‹çš„è¾“å‡ºã€‚ è®¡ç®—æŸå¤± (Loss Calculation): ä½¿ç”¨æŸå¤±å‡½æ•°è®¡ç®—æ¨¡å‹è¾“å‡ºä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®è·ã€‚ åå‘ä¼ æ’­ (Backward Pass): è®¡ç®—æŸå¤±å‡½æ•°ç›¸å¯¹äºæ¨¡å‹å‚æ•°çš„æ¢¯åº¦ã€‚ å‚æ•°æ›´æ–° (Parameter Update): ä½¿ç”¨ä¼˜åŒ–å™¨æ ¹æ®è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°ã€‚\né‡å¤ä»¥ä¸Šæ­¥éª¤ï¼Œç›´åˆ°æ¨¡å‹æ”¶æ•›æˆ–è€…è¾¾åˆ°é¢„å®šçš„è®­ç»ƒè½®æ•° (Epochs)ã€‚\n\n2.5.1 Randomness Control\nåœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œéšæœºæ€§ (Randomness) æ˜¯ä¸å¯é¿å…çš„ã€‚æ¯”å¦‚ï¼Œå‚æ•°çš„åˆå§‹åŒ–ã€æ•°æ®çš„æ‰“ä¹± (Shuffling) å’Œæ‰¹é‡çš„é€‰æ‹© (Batch Selection)ç­‰æ“ä½œéƒ½æ¶‰åŠåˆ°éšæœºæ€§ã€‚ä¸ºäº†ä¿è¯å®éªŒçš„å¯é‡å¤æ€§ (Reproducibility)ï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦æ§åˆ¶éšæœºæ•°ç”Ÿæˆå™¨ (Random Number Generator, RNG) çš„ç§å­ (Seed)ã€‚\nimport random\nimport numpy as np\nimport torch\n\ndef seed_everything(seed):    \n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\nseed_everything(42)\n\n\n2.5.2 Data Loading\n\n\n2.5.3 Checkpointing\nåœ¨è®­ç»ƒå¤§å‹æ¨¡å‹æ—¶ï¼Œè®­ç»ƒè¿‡ç¨‹å¯èƒ½ä¼šéå¸¸è€—æ—¶ï¼Œå¹¶ä¸”å®¹æ˜“å—åˆ°å„ç§æ„å¤–æƒ…å†µçš„å½±å“ï¼Œæ¯”å¦‚æ–­ç”µã€ç³»ç»Ÿå´©æºƒç­‰ã€‚ä¸ºäº†é¿å…è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ•°æ®ä¸¢å¤±ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šä½¿ç”¨æ£€æŸ¥ç‚¹ (Checkpointing) æŠ€æœ¯æ¥ä¿å­˜æ¨¡å‹çš„çŠ¶æ€ã€‚\nModel Checkpointing é€šå¸¸ä¿å­˜ä»¥ä¸‹å‡ ä¸ªæ–¹é¢çš„ä¿¡æ¯ï¼š\n\næ¨¡å‹å‚æ•° (Model Parameters): ä¿å­˜æ¨¡å‹çš„æƒé‡å’Œåç½®ç­‰å‚æ•°ã€‚\nä¼˜åŒ–å™¨çŠ¶æ€ (Optimizer State): ä¿å­˜ä¼˜åŒ–å™¨çš„çŠ¶æ€ï¼Œæ¯”å¦‚åŠ¨é‡ (Momentum ) å’Œå­¦ä¹ ç‡ (Learning Rate) ç­‰ä¿¡æ¯ã€‚\nå­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€ (Learning Rate Scheduler State): ä¿å­˜å­¦ä¹ ç‡è°ƒåº¦å™¨çš„çŠ¶æ€ã€‚\nè®­ç»ƒè¿›åº¦ (Training Progress): ä¿å­˜å½“å‰çš„è®­ç»ƒè½®æ•° (Epochs) å’Œæ‰¹é‡ç´¢å¼• (Batch Index) ç­‰ä¿¡æ¯ã€‚\n\nåœ¨ GPU ä¸Šè¿›è¡Œè®¡ç®—æ—¶ï¼Œæ•°æ®ä¼ è¾“çš„é€Ÿåº¦é€šå¸¸æ˜¯ä¸€ä¸ªç“¶é¢ˆã€‚ä¸ºäº†æé«˜æ•°æ®ä¼ è¾“çš„æ•ˆç‡ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Pinned Memory (ä¹Ÿå« Page-Locked Memory)ã€‚Pinned Memory æ˜¯ä¸€ç§ç‰¹æ®Šçš„å†…å­˜åŒºåŸŸï¼Œå¯ä»¥åŠ é€Ÿä¸»æœº (Host) å’Œè®¾å¤‡ (Device) ä¹‹é—´çš„æ•°æ®ä¼ è¾“ã€‚\n\n\n\n\n\n\nFigureÂ 9: å¦‚å›¾æ‰€ç¤ºï¼ŒPinned memory å¯ä»¥ä½œä¸ºè®¾å¤‡(Device)åˆ°ä¸»æœº(Host)æ‹·è´çš„ä¸­è½¬åŒºï¼Œç›´æ¥åœ¨ pinned memory ä¸­åˆ†é…ä¸»æœºæ•°ç»„ï¼Œå°±èƒ½é¿å… pageable å†…å­˜ä¸ pinned å†…å­˜ä¹‹é—´çš„é¢å¤–æ‹·è´å¼€é”€ï¼Œä»è€Œæå‡æ•°æ®ä¼ è¾“æ•ˆç‡ã€‚\n\n\n\nè¿™ç¯‡æ–‡ç« ä¸­ä»‹ç»äº†4ç§å¸¸è§çš„åŠ é€Ÿæ•°æ®ä¼ è¾“çš„æ–¹æ³•ï¼š\n\nåˆ©ç”¨ Numpy Memmap å¤„ç†å¤§æ•°æ®é›†: é€šè¿‡å†…å­˜æ˜ å°„æŠ€æœ¯ï¼Œåªå°†æ•°æ®é›†çš„ä¸€éƒ¨åˆ†åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨ï¼Œæé«˜æ•°æ®åŠ è½½é€Ÿåº¦ã€‚\nå¤šåˆ©ç”¨ torch.from_numpy å‡½æ•°: ç›´æ¥å°† NumPy æ•°ç»„è½¬æ¢ä¸º PyTorch å¼ é‡ï¼Œé¿å…ä¸å¿…è¦çš„æ•°æ®å¤åˆ¶ï¼Œæé«˜æ•°æ®ä¼ è¾“æ•ˆç‡ã€‚\nå°† num_workers è®¾ç½®ä¸ºå¤§äº0: é€šè¿‡å¤šçº¿ç¨‹æ•°æ®åŠ è½½ï¼Œæé«˜æ•°æ®é¢„å¤„ç†å’ŒåŠ è½½çš„å¹¶è¡Œåº¦ï¼Œå‡å°‘æ•°æ®åŠ è½½æ—¶é—´ã€‚\nä½¿ç”¨ Pinned Memory åŠ é€Ÿä¸»æœºä¸è®¾å¤‡ä¹‹é—´çš„æ•°æ®ä¼ è¾“: é€šè¿‡å°†æ•°æ®å­˜å‚¨åœ¨å›ºå®šå†…å­˜ä¸­ï¼Œå‡å°‘æ•°æ®ä¼ è¾“çš„å»¶è¿Ÿï¼Œæé«˜\n\nfrom torch.utils.data import DataLoader\n\n# some code\n\nloader = DataLoader(your_dataset, ..., pin_memory=True)\ndata_iter = iter(loader)\n\nnext_batch = data_iter.next() # start loading the first batch\nnext_batch = [ _.cuda(non_blocking=True) for _ in next_batch ]  # with pin_memory=True and non_blocking=True, this will copy data to GPU non blockingly\n\nfor i in range(len(loader)):\n    batch = next_batch \n    if i + 2 != len(loader): \n        # start copying data of next batch\n        next_batch = data_iter.next()\n        next_batch = [ _.cuda(async=True) for _ in next_batch]\nè¿™å‡ ä¸ªæ–¹æ³•å¯ä»¥æ˜¾è‘—æå‡æ•°æ®ä¼ è¾“å’ŒåŠ è½½çš„æ•ˆç‡ï¼Œå°¤å…¶åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶æ•ˆæœå°¤ä¸ºæ˜æ˜¾ã€‚åœ¨æˆ‘ä»¬å®ŒæˆAssignment 01æ—¶ï¼Œä¼šç”¨åˆ°è¿™äº›æŠ€å·§æ¥ä¼˜åŒ–æ•°æ®åŠ è½½è¿‡ç¨‹ã€‚\n\n\n2.5.4 Mixed Precision Training\nåœ¨ä¹‹å‰çš„å†…å®¹ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸åŒçš„æ•°æ®ç±»å‹åŠå…¶å†…å­˜å ç”¨ã€‚åœ¨å®é™…çš„æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šé‡‡ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (Mixed Precision Training) çš„æ–¹æ³•ï¼Œæ¥å¹³è¡¡å†…å­˜å ç”¨å’Œæ•°å€¼ç²¾åº¦çš„é—®é¢˜ã€‚\né‚£é‚£äº›éœ€è¦æ··åˆç²¾åº¦è®­ç»ƒå‘¢ï¼Ÿé€šå¸¸åœ¨ä»¥ä¸‹å‡ ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¼šè€ƒè™‘ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼š\n\nbfloat16 æˆ–è€… fp8 ä½œä¸ºå‰å‘çš„è®¡ç®—æ•°æ®ç±»å‹ï¼ˆactivationsï¼‰\nfloat32 ä½œä¸ºæ¢¯åº¦è®¡ç®—çš„æ•°æ®ç±»å‹, å¹¶ä¸”æ˜¯ç”¨ float32 æ¥æ›´æ–°å‚æ•°\nä¼˜åŒ–å™¨çŠ¶æ€ (Optimizer States) ä½¿ç”¨ float32 æ¥å­˜å‚¨\n\nMicikevicius et al. (2018) æå‡ºäº†ä¸€ç§æ··åˆç²¾åº¦è®­ç»ƒçš„æ–¹æ³•ï¼Œç§°ä¸º Loss Scalingã€‚Loss Scaling çš„åŸºæœ¬æ€æƒ³æ˜¯é€šè¿‡æ”¾å¤§æŸå¤±å‡½æ•°çš„å€¼ï¼Œæ¥é¿å…åœ¨ä½¿ç”¨ä½ç²¾åº¦æ•°æ®ç±»å‹æ—¶å‡ºç°æ•°å€¼ä¸‹æº¢ (Underflow) çš„é—®é¢˜ã€‚\nLoss Scaling çš„å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š\n\nåœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œè®¡ç®—æŸå¤±å‡½æ•°çš„å€¼ï¼Œå¹¶å°†å…¶ä¹˜ä»¥ä¸€ä¸ªæ”¾å¤§å› å­ (Scaling Factor)ã€‚\nåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œè®¡ç®—æ¢¯åº¦ï¼Œå¹¶å°†å…¶é™¤ä»¥æ”¾å¤§å› å­ã€‚\nä½¿ç”¨ä¼˜åŒ–å™¨æ›´æ–°æ¨¡å‹å‚æ•°ã€‚ ä¸è¿‡ï¼Œå½“æˆ‘ä»¬ç”¨ bfloat16 è¿›è¡Œå‰å‘è®¡ç®—æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä¸éœ€è¦ä½¿ç”¨ Loss Scalingï¼Œå› ä¸º bfloat16 å·²ç»æœ‰è¶³å¤Ÿçš„æ•°å€¼èŒƒå›´æ¥é¿å…ä¸‹æº¢çš„é—®é¢˜ã€‚"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture03/lec03.html",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture03/lec03.html",
    "title": "Lecture 03: LM Model Architecture & Hyperparameters",
    "section": "",
    "text": "Back to top"
  }
]