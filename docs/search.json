[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Yuyang",
    "section": "",
    "text": "I am a recent graduate from Singapore Management University in MITB (AI Track). With a strong foundation in computer science and big data, I have hands-on experience in machine learning and deep learning, and I’m passionate about building impactful AI-driven solutions. I am currently seeking full-time opportunities in the AI field where I can contribute, grow, and make a meaningful impact. Open to Work – feel free to reach out at zhangyuyang1211@gmail.com!"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About Yuyang",
    "section": "Education",
    "text": "Education\n\n  \n    \n    \n      2023 - 2025\n      Master of IT in Business\n      Singapore Management University\n    \n  \n  \n    \n    \n      2020 - 2023\n      Bachelor of Computer Science\n      University of Wollongong"
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "About Yuyang",
    "section": "Work Experience",
    "text": "Work Experience\n\n  \n    \n    \n      Feb 2025  -  Apr 2025\n      Research Assistant Intern\n      SMART"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "About Yuyang",
    "section": "Projects",
    "text": "Projects\n\n  \n    \n    \n       File-Tuning Large Visual Language Model\n          [GitHub]\n          [Blog]\n      \n        Wrting the"
  },
  {
    "objectID": "index.html#technical-skills",
    "href": "index.html#technical-skills",
    "title": "About Yuyang",
    "section": "Technical Skills",
    "text": "Technical Skills\n\n  \n    Python Development\n    Algorithm Design\n    Deep Learning\n    Machine Learning\n    Computer Vision\n    Natural Language Processing\n    Deep Reinforcement Learning\n    Graph Neural Networks\n    Large Language Model\n    Model Compression\n    Convex Optimization\n    Probabilistic Graph Model\n    Meta Learning\n    Deep Generative Model"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture01/lec01.html#character-level-tokenization",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture01/lec01.html#character-level-tokenization",
    "title": "Lecture 01: Introduction & Tokenization",
    "section": "2.1 Character-level Tokenization",
    "text": "2.1 Character-level Tokenization\nCharacter-level Tokenization是将文本拆分为单个字符。 例如，句子 “Hello, world!” 会被拆分为以下tokens：\n['H', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n需要知道的就是，每个Character就是一个token，利用Python，的ord()函数，我们可以将其转换为对应的整数ID：\ntext = \"Hello, world!\"\ntokens = [char for char in text]\ntoken_ids = [ord(char) for char in tokens]\nprint(tokens)\nprint(token_ids)\n这种方式很简单，也很直观，但是它有一些明显的缺点：\n\nVocabulary Size：对于所有可能的字符（包括字母、数字、标点符号和特殊字符），Vocabulary Size会非常大，导致模型参数量增加。（大约有150K 个不同的字符）\n150K个字符中，有很多字符是非常少见的，导致模型难以学习到这些字符的表示。\n语义信息缺失：单个字符无法捕捉到词语的语义信息，导致模型难以理解上下文。"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture01/lec01.html#word-level-tokenization",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture01/lec01.html#word-level-tokenization",
    "title": "Lecture 01: Introduction & Tokenization",
    "section": "2.2 Word-level Tokenization",
    "text": "2.2 Word-level Tokenization\n与Character-level Tokenization不同，Word-level Tokenization是将文本拆分为单词。 例如，句子 “Hello, world!” 会被拆分为以下tokens：\n['Hello,', 'world!']\n每个单词就是一个token，同样地，我们可以通过一个简单的Python代码将单词转换为对应的整数ID：\nimport regex\n\ntext= \"Hello, world!\"\ntokens = text.split()  # 简单的空格拆分\nvocab = {word: idx for idx, word in enumerate(set(tokens))}\ntoken_ids = [vocab[word] for word in tokens]\nprint(tokens)\nprint(token_ids)\n除了简单的根据空格拆分单词的方法，我们还可以有稍微复杂一点的方法，比如使用正则表达式来处理标点符号等。举个例子，GPT-2的tokenizer就是使用了一种基于正则表达式的方法来进行Word-level Tokenization。\nGPT2_TOKENIZER_REGEX = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n当然，这种方法也有比较明显的缺点：\n\nVocabulary Size 依然很大：对于所有可能的单词，Vocabulary Size会非常大，导致模型参数量增加。\n未登录词问题（Out-of-Vocabulary, OOV）：对于训练集中未出现的单词，模型无法处理，导致性能下降。 尽管我们可以通过一些方法（如使用特殊的&lt;UNK&gt; token）来缓解这个问题，但仍然无法完全解决。\nVocabulary Size 的大小不是固定的，随着训练数据的增加，Vocabulary Size会不断增加，导致模型难以扩展。\n很多单词是非常少见的，导致模型难以学习到这些单词的表示。"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture01/lec01.html#byte-based-tokenization",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture01/lec01.html#byte-based-tokenization",
    "title": "Lecture 01: Introduction & Tokenization",
    "section": "2.3 Byte-Based Tokenization",
    "text": "2.3 Byte-Based Tokenization\n在学习Byte Pair Encoding (BPE)之前，我们先介绍一下Byte-Based Tokenization。 Byte-Based Tokenization是将文本拆分为字节单元（byte-level tokens）。 例如，句子 “Hello, world!” 会被拆分为以下tokens：\n['H', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n每个字节就是一个token，同样地，我们可以通过一个简单的Python代码将字节转换为对应的整数ID：\ntext = \"Hello, world!\"\ntokens = [char.encode('utf-8') for char in text]\ntoken_ids = [byte[0] for byte in tokens]\nprint(tokens)\nprint(token_ids)\n这种方法的优点是：\n\nVocabulary Size 固定且较小：由于字节的范围是0-255，Vocabulary Size固定为256，模型参数量较小。\n无OOV问题：由于所有文本都可以表示为字节序列，不存在OOV的问题。\n适用于多语言文本：字节级别的表示可以处理各种语言的文本。\n简单高效：字节级别的表示简单且高效，适合大规模文本处理。\n\n不过，这种方法有个明显的缺点就是Compression Ratio较低。 由于字节级别的表示过于细粒度，导致文本长度增加，影响模型的训练效率和性能。因为Transformer模型的计算复杂度与输入长度的平方成正比，输入长度增加会显著增加计算资源的消耗。"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture01/lec01.html#byte-pair-encoding-bpe",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture01/lec01.html#byte-pair-encoding-bpe",
    "title": "Lecture 01: Introduction & Tokenization",
    "section": "2.4 Byte Pair Encoding (BPE)",
    "text": "2.4 Byte Pair Encoding (BPE)\n为了克服Character-level和Word-level Tokenization的缺点，同时提高Byte-Based Tokenization的Compression Ratio，我们引入了Byte Pair Encoding (BPE)算法 (Sennrich, Haddow, and Birch 2016) 。 BPE是一种基于频率的子词单元（subword unit）分词方法。 它的基本思想是通过迭代地合并最频繁出现的字符对（byte pairs）来构建一个更紧凑的词汇表。\n\n\n\nBPE算法的步骤如下：\n\n初始化Vocabulary：将文本中的所有唯一字符作为初始的词汇表（vocabulary）。\n统计频率 get_stats：计算文本中所有相邻字符对的出现频率。\n合并字符对并且更新Vocabulary：选择出现频率最高的字符对，将其合并为一个新的token，并更新文本中的所有出现该字符对的地方，并且将新token添加到词汇表中。\n重复步骤2-4：重复上述步骤，直到达到预定的词汇表大小或满足其他停止条件。\n\n\n\n\n\n\n\n\n\nFigure 4: BPE算法的示意图，展示了字符对的合并过程。\n\n\n\n\n\n\n下面是一个简单的BPE算法的Python实现示例：\ndef train_bpe(string: str, num_merges: int):\n    indices = list(map(int, string.encode(\"utf-8\"))) \n    merges: dict[tuple[int, int], int] = {}  \n    vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}  \n\n    for i in range(num_merges):\n        counts = defaultdict(int)\n        for index1, index2 in zip(indices, indices[1:]): \n            counts[(index1, index2)] += 1\n        \n        pair = max(counts, key=counts.get)  # @inspect pair\n        index1, index2 = pair\n\n        new_index = 256 + i\n        merges[pair] = new_index\n        vocab[new_index] = vocab[index1] + vocab[index2]\n        indices = merge(indices, pair, new_index)\n\n    return merges, vocab\n以上是最简单的BPE算法的实现，显然有很多的不足，我们在Assignment 01的第一部分中，会优化这个实现：\n\n优化 merge 函数的效率。\n利用pre-tokenization来加速BPE的训练过程。\n\n等\n对于那些想深入理解BPE算法的同学，可以参考以下资源： \n关于BPE算法，还有很多可以优化的地方，比如：\n\n在寻找最频繁字符对时，可以使用更高效的数据结构（如优先队列）来加速查找过程。\n在合并字符并且更新文本时，可以使用更高效的字符串处理方法来减少时间复杂度。这些优化可以显著提高BPE算法的训练速度，特别是在处理大规模文本数据\n\n这些方法我们将在 Assignment 01中进行更加详细的介绍。"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture03/lec03.html",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture03/lec03.html",
    "title": "Lecture 03: LM Model Architecture & Hyperparameters",
    "section": "",
    "text": "Lecture 03 介绍了现代大模型的核心架构与不同的超参数设计。这节课是理解大模型的基础，内容比较多，建议多看几遍。 本节课的目标是了解下图中的内容:"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture03/lec03.html#normalization",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture03/lec03.html#normalization",
    "title": "Lecture 03: LM Model Architecture & Hyperparameters",
    "section": "2.1 Normalization",
    "text": "2.1 Normalization\n\n为什么需要 Normalization？\nNormalization 技术在深度学习中起到了稳定训练过程和加速收敛的作用。 它通过调整神经网络层的输入分布，减少了内部协变量偏移 (Internal Covariate Shift)，从而使得模型在训练过程中更加稳定。此外，Normalization 还可以帮助缓解梯度消失和梯度爆炸问题，提高模型的泛化能力。\n\n\n2.1.1 Position of Normalization\n\n2.1.1.1 Post-Norm\n在Transformer中，Normalization放置在Sublayer之后，也就是所谓的 Post-Norm 结构。用数学公式表示为:\n\\[\n\\text{Post-Norm: } \\quad \\text{Norm}(x + \\text{Sublayer}(x))\n\\tag{1}\\]\n然而，(Ba, Kiros, and Hinton 2016) 指出，Post-Norm存在以下的问题:\n\n梯度不稳定，特别是在初始化阶段： 论文使用平均场理论分析指出，在 Post-Norm 结构 下（即 LayerNorm 放在残差连接之后），靠近输出层的参数在初始化时梯度期望值很大。 这会导致训练初期梯度爆炸，从而影响模型的稳定性和收敛速度。\n依赖复杂的 warm-up 超参数调优： 由于 Post-Norm 结构在训练初期容易出现梯度不稳定的问题，因此需要使用复杂的学习率 warm-up 策略来缓解这一问题。 这增加了模型训练的复杂性和调优难度。\n\n\nSpecifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem.  On Layer Normalization in the Transformer Architecture P.1 \n\n\n\n2.1.1.2 Pre-Norm\n为了缓解 Post-Norm 的问题，(Ba, Kiros, and Hinton 2016) 提出了 Pre-Norm 结构，即将 Normalization 放置在 Sublayer 之前。 用数学公式表示为:\n\\[\n\\text{Pre-Norm: } \\quad x + \\text{Sublayer}(\\text{Norm}(x))\n\\tag{2}\\]\n下图展示了 Post-Norm 与 Pre-Norm 结构的对比:\n\n\n\n\n\n\nFigure 2: Post Norm 与 Pre-Norm 结构对比。\n\n\n\nPre-Norm 有以下优点:\n\n提高训练稳定性： Pre-Norm 结构通过在每个子层之前进行归一化，减少了梯度爆炸和梯度消失的风险，从而提高了训练的稳定性。\n简化超参数调优： 由于 Pre-Norm 结构在训练过程中更加稳定，因此不再需要复杂的学习率 warm-up 策略，简化了模型的训练过程和超参数调优。\n\n\nWe show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.  On Layer Normalization in the Transformer Architecture P.1 \n\n接下来我们来看一下为什么Pre-Norm能有这么多的好处：\n\nGradient Attenuation\n\n\n\n\n\n\n\nFigure 3: 上图展示了 Pre-Norm 与 Post-Norm Transformer 在梯度行为上的差异。可以看到，在初始化阶段，Post-Norm 会导致靠近输出层的梯度期望值显著增大，而底层梯度被明显削弱，呈现出严重的梯度失衡（gradient attenuation across layers），从而使训练过程极不稳定，必须依赖学习率 warm-up 进行缓解。相比之下，Pre-Norm 在初始化时各层梯度规模保持一致且稳定，避免了梯度衰减与爆炸问题。\n\n\n\n\nGradient Spike / Noise\n\n实验表明 (Nguyen and Salazar 2019)\n\n\n\n\n\n\nFigure 4\n\n\n\n\nPost-norm produces noisy gradients with many sharp spikes, even towards the end of training. On the other hand, Pre-norm has fewer noisy gradients with smaller sizes, even without warmup.  Transformers without Tears: Improving the Normalization of Self-Attention P.6 \n\n\n\n2.1.1.3 Other Positions\n除了 Pre-Norm 和 Post-Norm 之外，还有一些其他的 Normalization 位置设计，例如:\n\nSandwich Norm: 将 Normalization 放置在每个子层的输入和输出之间。\nDouble Norm: 在每个子层的输入和输出都进行归一化。\n\n\n\n\n2.1.2 Normalization Types\n除了 Normalization 的位置设计之外，Normalization 的形式也是一个重要的设计选择。 常见的 Normalization 形式包括:\n\nLayerNorm(Ba, Kiros, and Hinton 2016): 对每个样本的特征维度进行归一化，适用于序列数据。\nRMSNorm(RootMeanSquareLayer2020zhang?): 只使用均方根（Root Mean Square）来进行归一化，省略了均值的计算，减少了计算开销。 RMSNorm 在某些情况下可以提供与 LayerNorm 相似的性能，但计算更高效。\n\n原始的Transformer中使用的是LayerNorm， 但是现代大语言模型中，RMSNorm被广泛采用， 例如在 LLaMA(Llama2023touvron?)、GPT-3(LanguageModelsAre2020brown?)、PaLM(ScalingLanguageModels2022chowdhery?) 等模型中都使用了 RMSNorm。"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture03/lec03.html#activations",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture03/lec03.html#activations",
    "title": "Lecture 03: LM Model Architecture & Hyperparameters",
    "section": "2.2 Activations",
    "text": "2.2 Activations"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture03/lec03.html#serial-parallel-mlp",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture03/lec03.html#serial-parallel-mlp",
    "title": "Lecture 03: LM Model Architecture & Hyperparameters",
    "section": "2.3 Serial & Parallel MLP",
    "text": "2.3 Serial & Parallel MLP"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture03/lec03.html#position-encoding",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture03/lec03.html#position-encoding",
    "title": "Lecture 03: LM Model Architecture & Hyperparameters",
    "section": "2.4 Position Encoding",
    "text": "2.4 Position Encoding"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Assignments/Ass01/ass01.html",
    "href": "posts/LearningNotes/CS336/Assignments/Ass01/ass01.html",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "",
    "text": "在本部分中，我们将实现Byte Pair Encoding (BPE)算法，用于文本的tokenization。在Lecture 01中，我们已经介绍了BPE的基本原理和实现方法。现在，我们将通过代码来进一步的实现并且优化BPE算法。\n\n\n回顾一下BPE算法的基本步骤： 1. Initialization: 将输入文本视为字节序列，每个字节作为一个token。初始化词汇表包含所有可能的字节（0-255）。 2. Count Pairs: 统计文本中所有相邻字节对的出现频率。 3. Merge Pairs: 找到出现频率最高的字节对，并将其合并为一个新的token，更新文本和词汇表。 4. Repeat: 重复步骤2和3，直到达到预定的合并次数"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Assignments/Ass01/ass01.html#bpe-algorithm-recap",
    "href": "posts/LearningNotes/CS336/Assignments/Ass01/ass01.html#bpe-algorithm-recap",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "",
    "text": "回顾一下BPE算法的基本步骤： 1. Initialization: 将输入文本视为字节序列，每个字节作为一个token。初始化词汇表包含所有可能的字节（0-255）。 2. Count Pairs: 统计文本中所有相邻字节对的出现频率。 3. Merge Pairs: 找到出现频率最高的字节对，并将其合并为一个新的token，更新文本和词汇表。 4. Repeat: 重复步骤2和3，直到达到预定的合并次数"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html",
    "title": "Qwen Model Series",
    "section": "",
    "text": "在了解 Qwen 等LLM 模型前，我们需要先了解什么是 Transformer, 在这里就不具体展开了，有兴趣的同学前去查看 这篇文章。 对于Multi-Modality的Qwen，我们需要具备的是 Vision-Transformer 的知识。\n前置知识：\n\nTransformer\nVision-Transformer\n\nQwen（通义千问） 是阿里云旗下达摩院推出的一系列大语言模型（LLM) 与多模态模型（M-LLM）。它类似 OpenAI 的 GPT 系列，是阿里打造的全栈式统一LLM体系。 接下来，我们来沿着Qwen模型发展的时间线，来感受一下LLM发展的过程。"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#tokenization",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#tokenization",
    "title": "Qwen Model Series",
    "section": "2.1 Tokenization",
    "text": "2.1 Tokenization\nQwen 用byte pair encoding (BPE)的Tokenization的方法，这与GPT-3.5，GPT-4系列是一样的。在训练Tokenization之后，最后的Vocabulary Size 由152K。 Qwen的Tokenization的方法，实现了较低的Compression Ratio。低Compression Ratio说明了Qwen在这些语言的Training 和 Inference 中会比较高效。"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#architecture",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#architecture",
    "title": "Qwen Model Series",
    "section": "2.2 Architecture",
    "text": "2.2 Architecture\nQwen-1 的模型借鉴了LLaMA模型，也是Decoder-Only 的 Transformer (Vaswani et al. 2023) 模型，只不过有着以下几点的改变：\n\nEmbedding and output projection: LLaMA和Transformer 都用了Weight Tying的技术，这种方式可以减少模型的参数，提高训练的效率。不过Qwen没有沿用这种方式，而是让这两个有各自的parameters。\nPositional Embedding： Qwen用了 RoPE (Su et al. 2023) 来Encoding Position消息，同时运用了 FP32 的精度，来 inverse frequency matrix.\nBias: 对于许多的Layer，移除了bias的term，不过对于QKV Layers，还是加了Bias。\nPre-Norm & RMSNorm：Qwen 模型用了 Pre-Norm 和RMSNorm来当作Normalization 的方法\nActivation Function: 用了SwiGLU当作Activation Function, 为了保持模型参数的不变，减少了d_ff到 \\(\\frac{8}{3}\\)"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#pre-training",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#pre-training",
    "title": "Qwen Model Series",
    "section": "2.3 Pre-Training",
    "text": "2.3 Pre-Training\nPre-Training 遵循了标准的Auto-Regressive LM的训练目标，Context Length设为2048， 运用了Flash-Attention。 利用AdamW 的optimizer。 和 Cosine Learning Rate schedule. 并且运用了 Mixed Precision Training 为了提高模型的Stability 和 训练速度。"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#extend-context-length",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#extend-context-length",
    "title": "Qwen Model Series",
    "section": "2.4 Extend Context Length",
    "text": "2.4 Extend Context Length\n模型在训练时的Context Length设为了2048，不过这在Inference时，是不够的。于是Qwen利用了一种training-free techniques 的方法， NTK-aware interpolation\n\n2.4.1 NTK-aware Interpolation\nNTK-aware Interpolation 与普通的Position Interpolation不同，NTK-aware Interpolation adjust the base of RoPE。 Qwen的团队在NTK-aware的基础上，为了更好的压榨出NTK的性能，实现了一个NTK的extension，叫做 dynamic NTK-aware interpolation。\n\n\n2.4.2 Attention\n除了Position Encoding，attention的计算效率也是阻碍Context length的原因之一。 Qwen的团队用了两个Attention的技巧： - LogN-Scaling - Layer-wise Window Attention: 不同的Layer 有不同的Window Size\n\nWe also observed that the long-context modeling ability of our model varies across layers, with lower layers being more sensitive in context length extension compared to the higher layers. To leverage this observation, we assign different window sizes to each layer, using shorter windows for lower layers and longer windows for higher layers.  QWEN TECHNICAL REPORT, p.8 \n\n运用了以上几个技巧之后，Qwen 模型将context length 从2048提升到了8192， 在没有损害模型能力的前提下。"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#alignment",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#alignment",
    "title": "Qwen Model Series",
    "section": "2.5 Alignment",
    "text": "2.5 Alignment\n在Pre-train Qwen之后，我们不能直接使用，\n\n2.5.1 Supervised Fine-Tuning\nSFT的训练，可以让Qwen模型遵循Chat类型的回答。\n\n2.5.1.1 Data\n{\n  \"messages\": [\n    {\"role\": \"system\", \"content\": \"You are a helpful, harmless and honest assistant.\"},\n    {\"role\": \"user\", \"content\": \"帮我写一段关于Qwen模型的简介。\"},\n    {\"role\": \"assistant\", \"content\": \"Qwen 是阿里云推出的一系列大语言模型...\"}\n  ]\n}\n\n\n2.5.1.2 Training\n与Pre-Training类似，SFT 用了相同的训练目标 Next-Token Prediction，不过与Pre-Training不同的是，SFT用了一个Loss Mask来mask掉system 和 user inputs。\n同样运用了 AdamW optimizer\n\n\n\n2.5.2 RLHF\n在SFT之后，模型可能overfitting，并且缺少generalization 和 creativity。 为了让模型获得这些能力，在SFT之后，我们需要RLHF，这个过程涉及到两个步骤： 1. Reward Model Training 2. Policy Training\n\n2.5.2.1 Reward Model Training\nReward Model Training 也叫做 Preference Model Pretraining (PMP)， 这个同样需要pre- training 然后Fine-Tunining。 PMP 的训练数据, 也是由一系列的comparison data 组成。\n{\n  \"prompt\": \"Explain why the Earth has seasons.\",\n  \"chosen\": \"The Earth has seasons because its axis is tilted about 23.5 degrees. As the planet orbits the Sun, this tilt causes different hemispheres to receive more or less direct sunlight throughout the year, creating seasonal temperature and daylight changes.\",\n  \"rejected\": \"The Earth has seasons because sometimes it randomly moves closer to the Sun and sometimes farther away.\"\n}\n训练这个Reward Model。 Qwen的团队用了Pre-trained Language Model 也就是Qwen， 来当作Initiate 权重。 在这个Qwen模型之上，加上了一层Pooling Layer来提取出 Reward （一个Scalar Value） #### Policy Training\n在训练完Reward Model之后，下一步就是运用Reinforcement的算法来训练LLM。Qwen的团队运用了PPO的算法，来训练，这个算法由4个部分： - Policy Model - Value Model - Reference Model - Reward Model\n至此，Qwen的Foundation Model，以及训练结束了。接下来可以通过不同的训练数据，让Qwen 获得不同的能力，比如Code-Qwen，以及Math-Qwen"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#code-qwen",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#code-qwen",
    "title": "Qwen Model Series",
    "section": "2.6 Code Qwen",
    "text": "2.6 Code Qwen"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#math-qwen",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#math-qwen",
    "title": "Qwen Model Series",
    "section": "2.7 Math Qwen",
    "text": "2.7 Math Qwen"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#qwen-1-summary",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#qwen-1-summary",
    "title": "Qwen Model Series",
    "section": "2.8 Qwen 1 Summary",
    "text": "2.8 Qwen 1 Summary"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#model-architecture",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#model-architecture",
    "title": "Qwen Model Series",
    "section": "3.1 Model Architecture",
    "text": "3.1 Model Architecture\nQwen-VL的模型主要包含了以下几个组件： - Large Language Model： Qwen-VL是基于之前的 Qwen-7B来当作大语言组件 - Visual Encoder： Visual Encoder的结构是，与 Vision-Transformer (Dosovitskiy et al. 2021) 的结构是一样的。通过加载 OpenCLIP的权重, 来初始化ViT - Position-aware Vision-Language Adapter: 为了减少 image feature的长度，Qwen-VL 利用了 Vision Language Adapter. 这个是一组Cross-Attention Module 随机初始化，这种方法将Visual Feature的长度，压缩到了256. 在这个Adapter 中，2D absolute positional encoding 也添加了进来，用来减少可能消息的丢失。\n ## Inputs and Outputs 添加了Visual Tokens之后，模型需要一种方法，来辨别出哪些是Visual Tokens, 哪些是Text Tokens\n\nImages are processed through the visual encoder and adapter, yielding fixed-length sequences of image features. To differentiate between image feature input and text feature input, two special tokens ( and ) are appended to the beginning and end of the image feature sequence respectively, signifying the start and end of image content  Qwen-VL- A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond, p.4 \n\n对于不同的输入和要求，模型的输出的内容是不一样的。"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#training-1",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#training-1",
    "title": "Qwen Model Series",
    "section": "3.2 Training",
    "text": "3.2 Training\nQwen-VL 的训练分为以下几个步骤，如下图所示：\n\n\n3.2.0.1 Pre-Training\n在这个阶段，模型训练出认识图片的能力，冻结了LLM，训练ViT和VL adapter。\n\n\n3.2.0.2 Multi-Task Pre-Training\n在这个阶段，模型已经有了对Image的基本认知，接下来就是训练模型对于不同要求的输出，也就是所谓的Multi-Task。 在这个阶段，所有的权重，都进行微调。通过这个训练，模型获得了完成不同任务的能力。\n\n\n3.2.0.3 Supervised Finetuning\n在这个阶段下，Qwen团队训练了Qwen-VL，使它获得Instruction- Following的能力。 也就是Qwen-VL-Chat Model，\nSupervised Finetuning 的数据如下图所示： \n至此，Qwen-VL的训练已经结束了，"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#qwen1-vl-sumary",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#qwen1-vl-sumary",
    "title": "Qwen Model Series",
    "section": "3.3 Qwen1-VL Sumary",
    "text": "3.3 Qwen1-VL Sumary"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#tokenizer",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#tokenizer",
    "title": "Qwen Model Series",
    "section": "5.1 Tokenizer",
    "text": "5.1 Tokenizer\nQwen2 的Tokenization的方法，与Qwen1 是一样的，都采用了BPE Tokenization Algorithms。Vocabulary Size 有151,643 Regular tokens和3 个control tokens。"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#architecture-1",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#architecture-1",
    "title": "Qwen Model Series",
    "section": "5.2 Architecture",
    "text": "5.2 Architecture\n与Qwen1 相比，Qwen2 的模型改动并没有很大，主要体现在以下几个方面\n\n5.2.1 Dense Model\n\n5.2.1.1 Gouged Query Attention\n运用了Grouped Query Attention, 这种方式可以优化KV Cache\n\n\n5.2.1.2 Dual Chunk Attention with YARN\n运用了Dual Chunk Attention 和 YARN 来提高Context 的长度。\n\n\n\n5.2.2 Mixture-Of-Expert Model"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#architecture-2",
    "href": "posts/LearningNotes/LLM-Series/Qwen/Qwen.html#architecture-2",
    "title": "Qwen Model Series",
    "section": "6.1 Architecture",
    "text": "6.1 Architecture\n\n\n6.1.1 Naive Dynamic Resolution\nNaive Dynamic Resolution 在 NaViT (Dehghani et al. 2023) 中提出，它可以让Vision Transformer 在不同长度的图片中训练。\n除此之外，Qwen2-VL 摒弃了Absolute Position Embedding，转用2D-RoPE (Su et al. 2023), 除此之外，在ViT 生成Representation之后，Qwen2-VL 还在之后添加了一层MLPLayer，它的作用是减少Tokens的数量，通过临近的 \\(2\\times 2\\) 的tokens，MLP讲这些tokens merge在一起，将Tokens的数量减少了 4 倍。比如一张 \\(224 \\times 224\\) 的图片, 分成大小为14的patches，得到了256 个tokens，将这些tokens merge在一起，我们就得到了 64 个tokens。\n\n\n6.1.2 Multi-Modal Rotary Position Embedding (M-RoPE)\nQwen2-VL 利用M-RoPE来提取出不同Modality之间的position 的信息。 通过将Rotary Embedding 分解成： - Temporal - Height - Width 三个部分。 对于Text 的部分，这几个部分会得到相同的IDs，这使得它可以类似于1D-RoPE的作用。对于图片的输入，Temporal 部分的ID保持不变，Height 和 Width的部分，会得到不同的IDs。 对于视频的输入，Temporal，Height，Width都会改变。\n\nM-RoPE not only enhances the modeling of positional information but also reduces the value of position IDs for images and videos, enabling the model to extrapolate to longer sequences during inference.  Qwen -VL- Enhancing Vision-Language Model’s Perception of the World at Any Resolution, p.5 \n\n\nFor text, these three use identical position IDs → equivalent to standard 1D RoPE.\nFor images, temporal ID is constant; height & width IDs encode patch position.\nFor videos, temporal ID increases per frame; height & width same as image."
  },
  {
    "objectID": "posts/LearningNotes/DLFaC/index.html",
    "href": "posts/LearningNotes/DLFaC/index.html",
    "title": "Deep Learning Foundation and Concepts(DLFaC) Learning Notes",
    "section": "",
    "text": "DLFaC Chapter 01: Introduction to the Deep Learning\n\n\nChapter01 介绍Deep Learning的基本概念\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "posts/PapersWithCode/05-dino/DINO.html",
    "href": "posts/PapersWithCode/05-dino/DINO.html",
    "title": "05: Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "",
    "text": "1 Preliminaries\n  \n  1.1 Self-Supervised Learning(SSL)\n  1.2 Knowledge Distillation\n  1.3 Exponential Moving Average (EMA)\n  \n  2 DINO\n  3 Summary\n  4 Key Concepts\n  5 Q & A\n  6 扩展\n  7 DINO\n  \n  7.1 数据处理\n  7.2 DINO V2\n  7.3 DINO V3\n  7.4 DINO V1 vs. DINO V2 vs. DINO V3\n  \n  8 Implementation\n  \n  8.1 Safe-Softmax\n在我们PwC的第二篇，我们学习了什么是Vision Transformer 以及它的基本架构和工作原理。我们还讨论了 ViT 相对于传统卷积神经网络（CNN）的优势和劣势。 但是ViT存在的主要问题是需要大量标注数据：\n我们在训练CIFAR-10 Classification问题时也可以发现，ViT在其准确度只有 55% 左右。 其主要的原因是：ViT 没有 CNN 的 inductive bias（卷积的平移不变性、局部感受野），需要更多标注来“学会”这些先验。\n而DINO (Caron et al. 2021) 的提出，旨在解决“在没有标签数据情况下，训练 Vision Transformer（ViT）提取有区分性、语义丰富的视觉表示”的问题。该方法旨在利用Self-Supervised Learning方式，通过模型自身蒸馏实现有效预训练。\n在我们了解DINO之前，我们需要先了解几个基本概念，以便我们更好的理解DINO。"
  },
  {
    "objectID": "posts/PapersWithCode/05-dino/DINO.html#self-supervised-learningssl",
    "href": "posts/PapersWithCode/05-dino/DINO.html#self-supervised-learningssl",
    "title": "05: Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "1.1 Self-Supervised Learning(SSL)",
    "text": "1.1 Self-Supervised Learning(SSL)\nSelf-Supervised Learning是一种Un-supervised learning的方法，通过利用未标记数据中的结构信息来进行特征学习。它通常通过设计预文本任务（pretext tasks）来实现。预文本任务指的是：在没有人工标注的情况下，人为设计一个“伪任务”，让模型通过解决这个任务来学习有用的特征表示。举个例子，在 Language Model中，GPT-2(Radford et al., n.d.) 通过预测下一个单词（Next Token Prediction）来学习语言表示，而BERT(Devlin et al. 2019) 通过掩码语言模型任务（Masked Language Modeling）来学习上下文表示。而对于图片，我们可以通过\n\n遮挡图片的区域（Inpainting）， 来进行自监督学习。\n拼图任务（Jigsaw Puzzle）：把图片分割成小块，打乱顺序，让模型重新排列。\n\n\n一句话总结Self-Supervised Learning就是: 让模型先玩一些自带标签的小任务，从而学会理解数据。"
  },
  {
    "objectID": "posts/PapersWithCode/05-dino/DINO.html#knowledge-distillation",
    "href": "posts/PapersWithCode/05-dino/DINO.html#knowledge-distillation",
    "title": "05: Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "1.2 Knowledge Distillation",
    "text": "1.2 Knowledge Distillation\n知识蒸馏(Hinton, Vinyals, and Dean 2015)是一种模型压缩技术，通过将一个大型模型（教师模型）的知识转移到一个较小的模型（学生模型）中，从而提高学生模型的性能。这两个模型，通常有以下几个特点：\n\nTeacher 模型通常容量大、性能好，但计算开销高。\nStudent 模型较小，但通过学习 Teacher 的知识，可以在低成本下接近 Teacher 的性能。\n\n知识蒸馏核心思想是，Teacher 输出概率分布 (soft targets) 往往包含着比硬标签（Hard Label）更多的信息，比如类间相似性。其损失函数定义为\n\\[\n\\mathcal{L}_{KD} =\n(1-\\lambda)\\,\n\\underbrace{\\mathcal{L}_{CE}(y, p_s)}_{\\text{Hard Label Loss}} +\n\\lambda \\, T^2 \\,\n\\underbrace{\\mathcal{L}_{KL}(p_t^T, p_s^T)}_{\\text{Soft Label Loss}}\n\\tag{1}\\]\n其中：\n\n\\(p_s, p_t\\): 是 student/teacher 的输出概率分布。\n\\(T\\): 是温度参数，用于控制softmax的平滑程度。\n\\(\\lambda\\): 是一个超参数，用于平衡两种损失的贡献。\n\\(\\mathcal{L}_{CE}\\): 是交叉熵损失函数, 用于衡量学生模型输出与真实标签之间的差距。 \\[\n\\mathcal{L}_{CE}(y, p_s) = -\\sum_{i} y_i \\log(p_{s,i})\n\\tag{2}\\]\n\\(\\mathcal{L}_{KL}\\): 是Kullback-Leibler散度损失函数，用于衡量教师模型输出与学生模型输出之间的差距。\n\n\\[\n\\mathcal{L}_{KL}(p_t, p_s) = \\sum_{i} p_t(i) \\log\\frac{p_t(i)}{p_s(i)}\n\\tag{3}\\]\n\nTakeaway 2 知识蒸馏(Knowledge Distillation)是一种模型压缩(Model Compression)方法，通过让小模型（student）学习大模型（teacher）的输出分布，从而继承其知识并提升小模型性能。"
  },
  {
    "objectID": "posts/PapersWithCode/05-dino/DINO.html#exponential-moving-average-ema",
    "href": "posts/PapersWithCode/05-dino/DINO.html#exponential-moving-average-ema",
    "title": "05: Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "1.3 Exponential Moving Average (EMA)",
    "text": "1.3 Exponential Moving Average (EMA)\n指数移动平均（EMA）是一种 Weight Average 的方法。在DINO中，EMA被用来更新教师网络的参数，使其在训练过程中更加稳定。具体来说，教师网络的参数是学生网络参数的指数加权平均，这样可以避免训练过程中的剧烈波动，从而提高模型的鲁棒性。\n\\[\nx^{\\text{EMA}}_{t+1} = \\alpha x^{\\text{EMA}}_t + (1 - \\alpha) x_{t+1}, \\quad \\text{where} \\ \\alpha \\in [0, 1]\n\\tag{4}\\]\nEMA 通常用来提升模型的generalization能力。\n\nTakeaway 3 指数移动平均(Exponential Moving Average)通过给新数据更高权重、旧数据指数衰减来平滑更新，从而获得更稳定的参数或信号。\n\n有了这些储备知识，接下来让我们看看DINO是如何利用这些知识来训练模型的。"
  },
  {
    "objectID": "posts/PapersWithCode/05-dino/DINO.html#数据处理",
    "href": "posts/PapersWithCode/05-dino/DINO.html#数据处理",
    "title": "05: Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "7.1 数据处理",
    "text": "7.1 数据处理\nDINO 在数据处理上采用了多视图增强策略。具体来说，对于每一张输入图像，DINO 会生成多个不同的视图，这些视图通过不同的数据增强技术获得，例如随机裁剪、颜色抖动等。这些增强视图将作为 Student 网络的输入。\n同时，Teacher 网络则只使用全局视图，即对整个图像进行一次前向传播，得到全局特征表示。通过这种方式，Student 网络可以学习到更丰富的局部特征，同时也能与 Teacher 网络的全局特征进行对齐。\n\n\n\n对于一张照片，DINO进行以下几个操作：\n\n颜色抖动（Color Jitter）\n高斯模糊（Gaussian Blur）\n太阳化（Solarization）\n\n并且对每个视图，进行Multi-Crop的策略：\n\n生成 两个 global crop（大视野），\n加若干个 local crop（小视野），\n然后所有裁剪视图都输入到 学生网络\n只有 global views 输入到 教师网络\n\n\n\n\n\n\n \n\n\nFigure 2: DINO Image Pre-process Steps\n\n\n\n\n\n\n下面是几个Data Augmentation的例子\n\n\n\n\n\n\n\n\n\n\n\n(a) Original Image\n\n\n\n\n\n\n\n\n\n\n\n(b) Gaussian Blur\n\n\n\n\n\n\n\n\n\n\n\n(c) Solarized\n\n\n\n\n\n\n\nFigure 3: 两个不同的Data Augmentation Examples\n\n\n\nThe full augmentation pipeline\n\n\n\n\n\n\nFigure 4: The Data Augmentation Pipeline"
  },
  {
    "objectID": "posts/PapersWithCode/05-dino/DINO.html#dino-v2",
    "href": "posts/PapersWithCode/05-dino/DINO.html#dino-v2",
    "title": "05: Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "7.2 DINO V2",
    "text": "7.2 DINO V2\n\n\n\n\n\n\nFigure 5\n\n\n\n• DINO 1：构建基础框架，开启自监督学习在视觉 Transformer 的应用。 • DINO 2：全面扩展训练规模与技术，使模型成为真正“开箱即用”的视觉基础模型。 • DINO 3：进一步提升规模与技术，解决密集特征退化问题，增强多场景适应性，并推动性能至新高度。"
  },
  {
    "objectID": "posts/PapersWithCode/05-dino/DINO.html#dino-v3",
    "href": "posts/PapersWithCode/05-dino/DINO.html#dino-v3",
    "title": "05: Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "7.3 DINO V3",
    "text": "7.3 DINO V3"
  },
  {
    "objectID": "posts/PapersWithCode/05-dino/DINO.html#dino-v1-vs.-dino-v2-vs.-dino-v3",
    "href": "posts/PapersWithCode/05-dino/DINO.html#dino-v1-vs.-dino-v2-vs.-dino-v3",
    "title": "05: Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "7.4 DINO V1 vs. DINO V2 vs. DINO V3",
    "text": "7.4 DINO V1 vs. DINO V2 vs. DINO V3\n\n\nSummary of 4 Tokenization Algorithms\n\n\n\n\n\n\n\n\nDINO Version\n核心创新与改进\n数据规模与模型规模\n应用特性与优势\n\n\n\n\nDINO 1\nstudent–teacher 自监督蒸馏，attention 可视化\n中小规模数据与模型\n可视化特征学习、语义分割能力、低成本训练\n\n\nDINO 2\n大规模蒸馏、FlashAttention、正则化、超强泛化能力\n1.42 亿图像训练数据、多个 ViT 架构\n多任务通用，无需微调，任务覆盖广泛\n\n\nDINO 3\nGram anchoring、轴向 RoPE、多分辨率鲁棒、多模型版本\n7B 参数模型 + 超大数据（1.7B 图像）\n更强密集特征质量，多任务性能达新 SOTA，适应性更强\n\n\n\n\n\nQuestion: Answer: 因为缺少 CNN 的 inductive bias，ViT 只能依靠大数据来学习空间不变性和局部模式。\n\n\nAnswer: 利用教师–学生蒸馏 + 图像增强视图，学生模仿教师输出，从而学习鲁棒特征。\n\n\nAnswer: 让学生学习局部与全局的语义对齐，从而获得更丰富的上下文表示。\n\n\nAnswer: ViT 的 self-attention 在自监督中会自然聚合语义一致的区域，从而表现出对象分割现象。\n\n\nCentering prevents one dimension to dominate but encourages collapse to the uniform distribution, while the sharpening has the opposite effect.  Emerging Properties in Self-Supervised Vision Transformers, p.4 \n\n\nOutput sharpening is obtained by using a low value for the temperature τt in the teacher softmax normalization.  Emerging Properties in Self-Supervised Vision Transformers, p.4"
  },
  {
    "objectID": "posts/PapersWithCode/05-dino/DINO.html#safe-softmax",
    "href": "posts/PapersWithCode/05-dino/DINO.html#safe-softmax",
    "title": "05: Emerging Properties in Self-Supervised Vision Transformers(DINO)",
    "section": "8.1 Safe-Softmax",
    "text": "8.1 Safe-Softmax\ndef softmax(x: torch.Tensor, temp):\n    max ="
  },
  {
    "objectID": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html",
    "href": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html",
    "title": "Vision-Transformer",
    "section": "",
    "text": "1 Preliminary\n  2 Vision-Transformer\n  3 Summary\n  4 Key Concepts\n  5 Q & A\n  6 Related resource & Further Reading\n  7 Preliminary\n  8 Vision-Transformer\n  \n  8.1 Patch Embedding\n  8.2 Position Encoding\n  \n  8.2.1 Extending Position Encoding\n  \n  8.3 [CLS] Tokens & MLP Head\n  8.4 Transformer Encoder Block\n  8.5 CNN vs. ViT： Inductive bias\n  8.6 ViT Model Variants\n  \n  9 Q&A\n  10 扩展\n  \n  10.1 减少Tokens\n  10.2 Vision Language Model\n  \n  11 Appendix\n  \n  11.1 Axial Attention（轴向注意力）\n  \n  12 Summary\n  13 Key Concepts\n  14 Q & A\n  15 Related resource & Further Reading"
  },
  {
    "objectID": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#patch-embedding",
    "href": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#patch-embedding",
    "title": "Vision-Transformer",
    "section": "8.1 Patch Embedding",
    "text": "8.1 Patch Embedding\n\nThe standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image \\(x \\in \\mathbb{R}^{H \\times W \\times C}\\) into a sequence of flattened 2D patches\\(x \\in \\mathbb{R}^{N \\times (P^{2} \\times C)}\\), where (\\(H, W\\)) is the resolution of the original image, \\(C\\) is the number of channels, (\\(P, P\\)) is the resolution of each image patch, and \\(N = HW/P^{2}\\) is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size \\(D\\) through all of its layers, so we flatten the patches and map to \\(D\\) dimensions with a trainable linear projection. We refer to the output of this projection as the patch embeddings.  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.3 \n\n在Transformer 这一篇，我们了解到，它是作用于Sequence Modeling的，很显然，Image 不是 Sequence的。很直观的第一种想法就是，将图片直接展开，从二维 (\\(3, H, W\\)) 展开成一维的 (\\(3, H \\times W\\)). 这样我们就得到的图片的Sequence Model。如下图@fig-flat-image所示\n\n\n\n\n\n\nFigure 2\n\n\n\n这种方法有一种明显的问题就是：Sequence的长度太长，举个例子，对于 \\(3\\times 256 \\times 256\\) 的图片，我们有 \\(256 \\times 256 = 65,336\\) 个tokens，通过这种方法，所需要的训练时长很长。并且它没有用到图片的一个特性：相邻的pixel 之间，是有很高的correlation的。所以我们很自然的想到：如果把相邻的pixels和在一组，组成一个patch，这样不就既减少了tokens的数量，又用到了pixel之间的correlation。这就是Vision Transformer 的Patch Embedding。 这样我们就得到了。 接下来我们只需要用，一个MLP，将我们展开的patch，映射到 \\(D\\)- dimension的空间，这样我们就可以传入Transformer 模型了。\n接下来我们来看看代码怎么实现：\n# Load Image and resize it to certain size\nimage_path = IMAGE_PATN\nimg_bgr = cv2.imread(image_path)\nimg_resized = cv2.resize(img_bgr, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA)\nimg = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB) \n\n# Patchify \npatches = einops.rearrange(\n     img, \"(h ph) (w pw) c -&gt; (h w) ph pw c\", ph=PATCH_SIZE, pw=PATCH_SIZE\n)\n通过这个Patchify只有，我们得到将图片Patch到了\n\n\n分成了不同的小Patch。\n接下来我们要做的就是，将这些Patch 展开，然后传入一个MLP，\nflat_patch = einops.rearrange(\n     patches, \"n ph pw c -&gt; n (ph pw c)\"\n)\n\n\nmlp = nn.Linear(PATCH_SIZE * PATCH_SIZE * 3, d_model)\npatch_embedding = mlp(flat_patch)\n同这种方式，我们就可以见图片转化为Transformer可以接受的vector。不过在实际操作中，我们并不会用以上的方式，因为上面的方式实现起来比较慢，我们可以将Patch 和 Linear Project和在一起。\n\n\n\n\n\n\nTip\n\n\n\n将几个tensor 的operation操作合成一个的方法，叫做kernel fusion，这是一种提高训练和推理素的方法\n\n\n在实际的代码中，我们用Convolution Layer 代替 Patch + Flatten+ Linear 的方法. 如果我们用一个 卷积层，参数设置为： • kernel_size = PATCH_SIZE （卷积核覆盖一个 patch） • stride = PATCH_SIZE （不重叠地移动，相当于切 patch） • in_channels = 3（RGB） • out_channels = d_model\n那么卷积会： 1. 把输入图片分成 PATCH_SIZE x PATCH_SIZE 的不重叠块（因为 stride = kernel_size）。 2. 对每个 patch 做一次线性映射（因为卷积本质上就是对局部区域做加权求和，相当于 Linear）。 3. 输出的 shape 自动就是 (batch, num_patches, d_model)。\n这正好等价于 切 patch + flatten + Linear 的组合。\n代码如下：\nclass PatchEmbedding(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.conv = nn.Conv2d(\n            in_channels=config.num_channels,\n            out_channels=config.hidden_dim,\n            kernel_size=config.patch_size,\n            stride=config.patch_size,\n            padding=\"valid\" if config.patch_size == 16 else \"same\",\n        )\n\n    def forward(self, imgs: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        imgs: (batch_size, num_channels, height, width)\n        Returns: (batch_size,  num_patches_height, num_patches_width, hidden_dim)\n        \"\"\"\n        # (B, C, H, W) -&gt; (B, hidden_dim, H', W')\n        x = self.conv(imgs)\n\n        # (B, hidden_dim, H', W') -&gt; (B, hidden_dim, H' * W')\n        x = x.flatten(2)\n\n        # (B, hidden_dim, H' * W') -&gt; (B, H' * W', hidden_dim)\n        x = x.transpose(1, 2)\n        return x\n用卷积的好处，除了可以更高效的实现Patch Embedding，代码更加简洁之外，我们还可以通过改变 stride 来使一些Patch overlapping，获得一个多尺度的结构，"
  },
  {
    "objectID": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#position-encoding",
    "href": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#position-encoding",
    "title": "Vision-Transformer",
    "section": "8.2 Position Encoding",
    "text": "8.2 Position Encoding\n将图片转化为 Transformer 的输入之后，接下来Transformer中的另一个组件就是传入 Position Information。我们知道在Transformer 中，他们用的是 sine-cosine position embedding，在那篇文章中，我们也提到了，还存在其他不同的Position Encoding的办法，ViT 用的就是另一种办法，Learned Position Embedding。Learned Position Embedding的方法很简单，也很好理解，对于每一个位置，我们给他一个index，将这个index传入一个 Embedding Matrix， 我们就得到一个Position Embedding。不过与Token Embedding不同的是，我们会用到所有的Position，也整个matrix， 所以我们不用定index，直接定义整个Embedding，然后将它传入Transformer中。\nclass PositionalEncoding(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.positional_embedding = nn.Parameter(\n            torch.randn(\n                1,\n                (config.image_size // config.patch_size) ** 2 + 1,\n                config.hidden_dim,\n            )\n        )\n\n        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_dim))\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        x: (batch_size, num_patches, hidden_dim)\n        Returns: (batch_size, num_patches, hidden_dim)\n        \"\"\"\n        # Add positional encoding to the input tensor\n        batch_size = x.size(0)\n\n        pos_embedding = self.positional_embedding.expand(batch_size, -1, -1)\n        cls_token = self.cls_token.expand(batch_size, -1, -1)\n\n        x = torch.cat((cls_token, x), dim=1)\n        return x + pos_embedding\n为什么ViT要用Learned Position Embedding呢？在ViT这篇文章中，他们尝试过不同的Position Embedding，比如：\n\nNo Positional Information\n1-dimensional Positional Embedding\n2-dimensional Positional Embedding\nRelative Positional Embedding\n\n发现，除了No Positional Information之外，其余3种在Image Classification中的表现，都是差不多的。\n\n\n\n\n\n\nFigure 3\n\n\n\n论文中表示，可能是因为所需要的 Position的信息较小，对于不同种类的Position Embedding的方法，学习这个Position Information的能力，都是差不多的。\n\nWe speculate that since our Transformer encoder operates on patch-level inputs, as opposed to pixel-level, the differences in how to encode spatial information is less important. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original pixel-level inputs, e.g., \\(14 \\times 14\\) as opposed to \\(224 \\times 224\\), and learning to represent the spatial relations in this resolution is equally easy for these different positional encoding strategies.  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.18 \n\n不过，尽管Position的方法不重要，但是不同的训练参数，还是会影响到学习到的Position Information, 下图所示：\n\n\n\n\n\n\nFigure 4\n\n\n\n\n8.2.1 Extending Position Encoding\n当我们有了一个Pre-Training的模型，我们想用它Fine-Tuning到一个不同图片大小的数据库，我们改怎么做呢，第一个方法当然是，Resize 我们的图片，到ViT Pre-training的图片大小，但是，这个能导致较大的图片，失去很多细节。如果我们想保持图片的大小不变，同时让模型训练，我们就需要Extend Position Encoding，因为当Patch Size不变，图片大小变了的话，产生的Number of Patches 也是会改变的，这样，就是损失一些信息。我们需要做的是，找到一种方法，增大或者减小Position的数量。 这就是所谓的Position Interpolation。\n2D interpolation of the pre-trained position embeddings • ViT 在预训练时，通常用固定输入分辨率（比如 224×224） → 生成固定数量的 patch（比如 16×16 patch → 196 个 patch）。 • 但在 fine-tuning 时，输入图片可能大小不一样，比如 384×384，这时 patch 数量就变了。 • 这会导致原本的 位置编码 (position embeddings) 和新的 patch 数量对不上。 • 解决办法：对预训练好的位置编码做 二维插值 (2D interpolation)，根据 patch 在原图中的空间位置，把位置编码拉伸/缩放到新的分辨率。\n\nThe Vision Transformer can handle arbitrary sequence lengths (up to memory constraints), however, the pre-trained position embeddings may no longer be meaningful. We therefore perform 2D interpolation of the pre-trained position embeddings, according to their location in the original image  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.4 \n\ndef interpolate_pos_encoding(self, x, w, h):\n    npatch = x.shape[1] - 1\n    N = self.pos_embed.shape[1] - 1\n    if npatch == N and w == h:\n        return self.pos_embed\n    class_pos_embed = self.pos_embed[:, 0]\n    patch_pos_embed = self.pos_embed[:, 1:]\n    dim = x.shape[-1]\n    w0 = w // self.patch_embed.patch_size\n    h0 = h // self.patch_embed.patch_size\n    \n    patch_pos_embed = F.interpolate(\n        patch_pos_embed.reshape(\n            1, \n            int(math.sqrt(N)), \n            int(math.sqrt(N)), \n            dim\n        ).permute(0, 3, 1, 2),\n        scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n        mode='bicubic',\n    )\n    patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n    return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)"
  },
  {
    "objectID": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#cls-tokens-mlp-head",
    "href": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#cls-tokens-mlp-head",
    "title": "Vision-Transformer",
    "section": "8.3 [CLS] Tokens & MLP Head",
    "text": "8.3 [CLS] Tokens & MLP Head\n在 Transformer 这一节，我们了解到：每输入一个token，Transformer会输出对应的token。这就是说，对于每个patch，Transformer会输出对应的Tokens，那么，我们应该选择哪一个token作为我们图片的表示呢。 BERT (Devlin et al. 2019)， 用了一个 [CLS], 来表示一个句子。同理，我们也可以添加一个 [CLS] token, 来表示一张图片。同时，对于 [CLS] token, 我们也要在给他一个表示位置的信息。这就是为什么在Position Encoding上，我们有 (config.image_size // config.patch_size) ** 2 + 1, 位置信息，其中 +1 就是 [CLS] 的位置信息。 总结一下 [CLS] token 的作用就是用来聚合所有的Patch的消息，然后用来Image 的Representation。\n我们想一下，除了加一个 [CLS] token，之外，我们还有其他办法来表示图片吗。有一种很自然的方法就是，将所有的patch的消息收集起来，然后去一个平均值来表示这个图片。类似于传统的ConvNet(e.g. ResNet) 我们可以通过 AvgPooling 来实现。 不过论文中提到， 对于两种不同的Image Representation，需要有不同的Learning Rate 来训练这个网络。\nOther content \n有了Image Represent之后，我们只需要将这个传入一个简单的MLP，我们就可以得到一个Classifier。MLP的输入是hidden dim，输入则是我们Number of Classes。不同的Index 表示不同的Classses。\n\nAn initial attempt at using only image-patch embeddings, globally average-pooling (GAP) them, followed by a linear classifier—just like ResNet’s final feature map—performed very poorly. However, we found that this is neither due to the extra token, nor to the GAP operation. Instead, the difference in performance is fully explained by the requirement for a different learning-rate,  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.17 \n\n\nBoth during pre-training and fine-tuning, a classification head is attached to \\(\\mathrm{z}_{L}^{0}\\). The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time.  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.3 \n\nclass ClassifierHead(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n        \n        self.fc1 = nn.Linear(config.hidden_dim, config.mlp_dim)\n        self.fc2 = nn.Linear(config.mlp_dim, config.num_classes)\n        self.dropout = nn.Dropout(config.dropout_rate)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        x: (batch_size, num_patches, hidden_dim)\n        Returns: (batch_size, num_classes)\n        \"\"\"\n        # Use the CLS token for classification\n        cls_token = x[:, 0, :]\n        x = F.relu(self.fc1(cls_token))\n        \n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x"
  },
  {
    "objectID": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#transformer-encoder-block",
    "href": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#transformer-encoder-block",
    "title": "Vision-Transformer",
    "section": "8.4 Transformer Encoder Block",
    "text": "8.4 Transformer Encoder Block\n至此，我们已经讲完了ViT与Transformer的主要不同之处。接下来，就是Transformer的Encoder。 \n这部分，和Transformer原本的Encoder很类似，只不过有几处不同：\n\nPre-Norm: 在ViT同，输入先进行一个LayerNorm，然后在传入MHA或者MLP中，反观在Transformer原本的Encoder中，我们是先将MHA或者MLP的输出与输入加在一起，之后再进行一个Normalization。这叫做Post-Norm\nMLP的实现：在Transformer Encoder中，用的是 ReLU, 而在ViT中，用的是 GELU\n\n除此之外，其他部分都是一样的。一下是ViT Encoder的实现：\nclass EncoderBlock(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n        \n        self.norm1 = LayerNorm(config.hidden_dim)\n        self.mha = MHA(config)\n        \n        self.norm2 = LayerNorm(config.hidden_dim)\n        self.ffn = FFN(config)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        x: (batch_size, num_patches, hidden_dim)\n        Returns: (batch_size, num_patches, hidden_dim)\n        \"\"\"\n        # Multi-head attention\n        redisual = x\n        x = self.norm1(x)\n        x = redisual + self.mha(x)\n\n        # Feed-forward network\n        redisual = x\n        x = self.norm2(x)\n        x = x + self.ffn(x)\n\n        return x"
  },
  {
    "objectID": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#cnn-vs.-vit-inductive-bias",
    "href": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#cnn-vs.-vit-inductive-bias",
    "title": "Vision-Transformer",
    "section": "8.5 CNN vs. ViT： Inductive bias",
    "text": "8.5 CNN vs. ViT： Inductive bias\n至此，我们已经介绍完了Vision Transformer，我们来从Inductive Bias 的方面，看看 CNN 和 ViT 有什么不同\n\n\n\n\n\n\n什么是Inductive Bias\n\n\n\n在深度学习里，Inductive Bias（归纳偏置）是指模型在学习之前，因结构或设计而自带的假设或先验。\n\n\n对于图像来说，常见的先验就是：\n\n局部像素是相关的（locality）\n相邻区域的模式有规律（2D neighborhood）\n物体无论出现在图像哪里，识别方式应该一样（translation equivariance）\n\n🔹 2. CNN 的结构怎么体现这些偏置？ 1. 局部性 (Locality) • 卷积核（例如 3×3）只和局部像素打交道，而不是全图。 • 这意味着模型“相信”图像的重要特征来自局部邻域，而不是遥远区域。 2. 二维邻域结构 (2D structure) • 卷积操作是沿着 图像的二维网格进行的，天然利用了图像的行列结构。 • 这和文本（序列 1D）不一样，CNN 明确知道输入是 2D 排列的。 3. 平移等变性 (Translation equivariance) • 卷积核的参数在整张图共享。 • 所以猫在左上角还是右下角，卷积核都能检测到“猫耳朵”。 • 这让 CNN 自动具有“识别位置无关”的能力。\n这些性质不是模型通过训练学出来的，而是因为 卷积操作本身的数学结构就带来的： • kernel 的局部连接 → 局部性 • kernel 滑动覆盖全图 → 平移等变性 • 操作在二维空间定义 → 邻域结构 • 所以，哪怕你不给 CNN 喂太多数据，它也会利用这些偏置去学习特征。\n而对于 ViT 来说： ViT 的归纳偏置非常弱，几乎完全依赖数据和训练来学习。\n\nPatch 切分 (Patchification) • ViT 唯一的“图像先验”之一就是把输入图片切成 patch。 • 这一操作隐含了：图像是一个二维结构，可以被分块处理。\n位置编码 (Positional Embeddings) • Transformer 本身只处理序列，没有空间结构的概念。 • ViT 通过加位置编码告诉模型 patch 在图像中的相对位置。 • 在输入分辨率变化时，会做 二维插值 (2D interpolation) 来适配，这也是一种人工引入的 2D 先验。\n其他部分 • 除了以上两点，ViT 的注意力机制是 全局的 (global)，没有局部性约束。 • 没有像 CNN 那样内置的平移等变性或局部邻域结构。\n\n这样就是为什么ViT需要更多数据和计算才能学到同样的空间归纳规律。"
  },
  {
    "objectID": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#vit-model-variants",
    "href": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#vit-model-variants",
    "title": "Vision-Transformer",
    "section": "8.6 ViT Model Variants",
    "text": "8.6 ViT Model Variants\nViT 有3种不同的基本变形， 如下图所示 \nViT的名字通常表示为: ViT-L/16: 意思是，ViT-Large，然后用的16 Patch Size。 需要注意的是，Patch Size越大，我们得到的tokens就越少，也就是需要更少的训练时实现。"
  },
  {
    "objectID": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#减少tokens",
    "href": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#减少tokens",
    "title": "Vision-Transformer",
    "section": "10.1 减少Tokens",
    "text": "10.1 减少Tokens\n\nPatch Merge\nPatch Shuffle"
  },
  {
    "objectID": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#vision-language-model",
    "href": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#vision-language-model",
    "title": "Vision-Transformer",
    "section": "10.2 Vision Language Model",
    "text": "10.2 Vision Language Model\n我们以及学习了ViT for computer Vision， Transformer for NLP， 接下来有什么办法让这两种模型结合起来呢？ CLIP (2021): 将 ViT 融合到 vision-language 预训练中。"
  },
  {
    "objectID": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#axial-attention轴向注意力",
    "href": "posts/PapersWithCode/02-vision-transformer/Vision-Transformer.html#axial-attention轴向注意力",
    "title": "Vision-Transformer",
    "section": "11.1 Axial Attention（轴向注意力）",
    "text": "11.1 Axial Attention（轴向注意力）\n在处理 图像或视频 这类高维输入时，如果直接对所有像素做 全局 self-attention，复杂度是 \\(\\mathcal{O}(H^2 W^2)\\) （\\(H, W\\) 是高和宽）。当图像很大时，这个代价太高。 核心想法：把二维 attention 拆成两次一维 attention（沿着图像的两个“轴”分别做）。 1. Row-wise Attention（行注意力） • 沿着水平方向（宽度轴 W）做注意力，每一行的像素互相关注。 • 复杂度：\\(\\mathcal{O}(H \\cdot W^2)\\)。 2. Column-wise Attention（列注意力） • 沿着垂直方向（高度轴 H）做注意力，每一列的像素互相关注。 • 复杂度： \\(\\mathcal{O}(W \\cdot H^2)\\)。\n组合起来，相当于在 H 和 W 两个轴上都做了全局依赖建模。\n\n\n\n\n\n\nFigure 5"
  },
  {
    "objectID": "posts/PapersWithCode/17 VQ-VAE/VQ_VAE.html",
    "href": "posts/PapersWithCode/17 VQ-VAE/VQ_VAE.html",
    "title": "VQ-VAE",
    "section": "",
    "text": "1 Preliminary\n  \n  1.1 Vector Quantization\n  1.2 Straight Through Estimator\n  \n  2 VQ_VAE\n  3 Summary\n  4 Key Concepts\n  5 Q & A\n  6 Related resource & Further Reading\n  7 VQ_VAE\n  8 Summary\n  9 Key Concepts\n  10 Q & A\n  11 Related resource & Further Reading"
  },
  {
    "objectID": "posts/PapersWithCode/17 VQ-VAE/VQ_VAE.html#vector-quantization",
    "href": "posts/PapersWithCode/17 VQ-VAE/VQ_VAE.html#vector-quantization",
    "title": "VQ-VAE",
    "section": "1.1 Vector Quantization",
    "text": "1.1 Vector Quantization\n向量量化（Vector Quantization）是一种把连续的向量转换为离散的“索引” 的方法。通过这个索引，在字典（codebook）中找到一个与其最相近的一个向量，这个字典也叫做： - CodeBook - Embedding Table - Centroids 这时，每个向量就变成了一个离散的编号（Index） 用数学表达就是： 我们有: - A vector \\(z \\in \\mathbb{R}^{d}\\) - A codebook \\(E \\in \\mathbb{R}^{K \\times d}\\), 其中 有 \\(K\\) 个索引\n我们通过比较 \\(z\\) 和 \\(K\\) 个向量中，找到最近的一个向量\n\\[\n\\text{quantized}(z) = e_{k} \\quad \\text{where}\\ k = \\underset{j}{\\operatorname{arg\\min}}   \\|z - e_{j} \\|^{2}\n\\]\nimport torch\n\ndef quantize(embedding_table: torch.Tensor, z: torch.Tensor):\n    \"\"\"\n    embedding_table: (K, D)\n    z: (B, D)\n    \"\"\"\n    # (B, 1, D) - (1, K, D) → (B, K, D)\n    diff = z.unsqueeze(1) - embedding_table.unsqueeze(0)\n\n    # (B, K, D) -&gt; (B, K)\n    distances = torch.linalg.norm(diff, dim=2)\n\n    #  (B, K) -&gt; (B,)\n    indices = distances.argmin(dim=1)\n\n    # Gather quantized embeddings → (B, D)\n    q = embedding_table[indices]\n\n    return q, indices\n\n\nK, D = 512, 64\nB = 8\n\ncodebook = torch.randn(K, D)\nz = torch.randn(B, D)\n\nq, idx = quantize(codebook, z)\nassert q.shape == (B, D)\nassert idx.shape == (B,)"
  },
  {
    "objectID": "posts/PapersWithCode/17 VQ-VAE/VQ_VAE.html#straight-through-estimator",
    "href": "posts/PapersWithCode/17 VQ-VAE/VQ_VAE.html#straight-through-estimator",
    "title": "VQ-VAE",
    "section": "1.2 Straight Through Estimator",
    "text": "1.2 Straight Through Estimator\nK, D = 512, 64\nB = 8\n\nembedding_table = torch.randn(K, D)\nz_enc = torch.randn(B, D, requires_grad=True)\nassert z_enc.grad is None\n\nz_k, _ = quantize(embedding_table, z_enc)\n\n# STE\nz_k = z_enc + (z_k - z_enc).detach()\nz_k.retain_grad()\n\nloss = (z_k**2).mean()\nloss.backward()\n\nassert z_k.grad is not None\nassert z_enc.grad is not None\nassert torch.allclose(z_enc.grad, z_k.grad)"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "",
    "text": "1 Swin-Transformer\n  \n  1.1 Window Multi-Head-Attention (W-MHA)\n  1.2 Shifted Window Multi-Head-Attention (SW-MHA)\n  1.3 Consecutive Swin Transformer Block\n  1.4 Patch Merge\n  1.5 Relative Position Encoding\n  \n  1.5.1 Fine-Tuning in different image size\n  \n  1.6 Others\n  1.7 Downstream Tasks\n  \n  1.7.1 Image Classification\n  \n  1.8 Object Detection & Semantic segmentation\n  1.9 Training Details\n  \n  1.9.1 DropPath\n  1.9.2 Gradient Checkpoint\n  \n  1.10 Swin V2\n  \n  1.10.1 Post normalization\n  1.10.2 Scaled cosine attention\n  1.10.3 Log-spaced Continuous Position Bias(Log-CPB)\n  \n  \n  2 Summary\n  3 Key Concept Check Table\n  4 Q & A\n  5 Related resource & Further Reading"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#window-multi-head-attention-w-mha",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#window-multi-head-attention-w-mha",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "1.1 Window Multi-Head-Attention (W-MHA)",
    "text": "1.1 Window Multi-Head-Attention (W-MHA)\nW-MHA 的核心思想是：\n\n把图像划分成固定大小的窗口（window），比如 7×7 patch 的窗口。\n在窗口内的 token 之间做局部自注意力，而不是在整张图像的所有 token 之间做全局注意力。\n每个窗口独立计算 Multi-Head Attention → 降低计算量，并且我们可以并行的计算\n\n这样一来：\n\n单个窗口 token 数量固定 = \\(M^{2}\\)（如 7×7=49）。\n注意力计算复杂度从 \\(\\mathcal{O}((hw)^{2}C)\\) 降低为 \\(\\mathcal{O}(M^{2}hwC)\\)，其中 \\(M \\ll \\sqrt{ N }\\)。\n\n除了降低计算复杂度之外，W-MHA，还有保留CNN 在图像处理中强大的一点是 局部感受野 和 平移不变性。\n\nW-MHA 通过窗口限制，使得注意力机制也具备类似的局部归纳偏置（inductive bias），适合图像建模。\n\n\nFor efficient modeling, we propose to compute self-attention within local windows. The windows are arranged to evenly partition the image in a non-overlapping manner.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.4"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#shifted-window-multi-head-attention-sw-mha",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#shifted-window-multi-head-attention-sw-mha",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "1.2 Shifted Window Multi-Head-Attention (SW-MHA)",
    "text": "1.2 Shifted Window Multi-Head-Attention (SW-MHA)\nW-MHA 很好，但是它存在的一个问题就是：\n\n窗口之间是相互独立的，缺少跨窗口的信息交流。这会导致，模型只能看见局部，不能获得全局的信息。\n\n\nThe window-based self-attention module lacks connections across windows, which limits its modeling power. To introduce cross-window connections while maintaining the efficient computation of non-overlapping windows, we propose a shifted window partitioning approach which alternates between two partitioning configurations in consecutive Swin Transformer blocks.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.4 \n\n为了解决这个问题，Swin- Transformer提出来 Shifted Window Mulit-Head-Attention (SW-MHA) 窗口位置相对前一层平移，比如 7×7 窗口 → 平移 3 个 patch。 这样，新的窗口会跨越原来的边界，token 会和相邻窗口的 token 一起计算注意力。 相当于强制跨窗口交互，让信息可以在不同区域之间流动。\n 如上如所示，我们将Window通过向左上角移动，通过给图片增加Padding来，但是这种办法显然会增加计算的复杂度。Swin Transformer用了一种很聪明的办法，叫做 Cycling Shift，这种方法就是将将一个张量或图像在某个维度上做 平移，但不是把移出去的部分丢掉，而是 重新从另一边补回来。就像“环形队列”或“钟表走一圈又回到起点”。 如下图所示 \n可以看到，通过Cycling Shift，我们得到的每个window的内容，和之前是一样的，但是所需要的Window的数量，小了很多，这也就意味着，所需要的时间复杂度，也小了很多。\n\n不过Cycling Shift也有一个问题，就是同一个窗口里面，可能有来自不同图片的信息，这些信息在原图片上不是相邻的，自然不应该相互交流信息。我们可以将图片，抽象成下图的形式。组织Attention交流，很自然的一种方法是利用Mask，就像Transformer里的Causal Mask一样。但是，这个Mask长什么样子呢\n\n我们可以看一下Mask，如下图所示，有颜色的区域表示Mask == 1， 在此为了更好的"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#consecutive-swin-transformer-block",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#consecutive-swin-transformer-block",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "1.3 Consecutive Swin Transformer Block",
    "text": "1.3 Consecutive Swin Transformer Block\n\nSwin Transformer is built by replacing the standard multi-head self attention (MSA) module in a Transformer block by a module based on shifted windows (described in Section 3.2), with other layers kept the same. As illustrated in Figure 3(b), a Swin Transformer block consists of a shifted window based MSA module, followed by a 2-layer MLP with GELU nonlinearity in between. A LayerNorm (LN) layer is applied before each MSA module and each MLP, and a residual connection is applied after each module  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.4 \n\n\n\\[\n\\begin{split}\n\\hat{z}^l &= \\text{W-MSA} \\left( \\text{LN} \\left( z^{l-1} \\right) \\right) + z^{l-1} \\\\\nz^l &= \\text{MLP} \\left( \\text{LN} \\left( \\hat{z}^l \\right) \\right) + \\hat{z}^l \\\\\n\\hat{z}^{l+1} &= \\text{SW-MSA} \\left( \\text{LN} \\left( z^l \\right) \\right) + z^l \\\\\nz^{l+1} &= \\text{MLP} \\left( \\text{LN} \\left( \\hat{z}^{l+1} \\right) \\right) + \\hat{z}^{l+1}\n\\end{split}\n\\]\n将W-MSA 和 SW-MSA叠在一起，就得到了Transformer Block，当然，还有一个MLP，Layer Normalization，在此就不赘述了。"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#patch-merge",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#patch-merge",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "1.4 Patch Merge",
    "text": "1.4 Patch Merge\n讲完了W-MHA，和SW-MHA，我们就理解了Swin- Transformer中最难理解，也是最终的部分，接下来我们看看其他简单的部分。 Patch Merge , 图中绿色的部分，逐步降低 token 数量（降采样），同时增加特征维度的操作。这类似于CNN中的操作，随着层数的增加，分辨率逐步降低、通道数逐步增加，这样既减少了计算量，又能提取层级特征。具体的实现：\n\n分组：将相邻的 2×2 patch 合并成一个新的 patch。\n\n假设输入特征大小为 (H, W, C)。\n每 2×2 的 patch → 合并为 1 个新 token。\n新特征图大小变为 (H/2, W/2, 4C)。\n\n线性变换:\n\n将合并后的 4C 维特征通过一个 线性层 (Linear Projection)，降到 2C 维。\n输出维度翻倍（2C），以补偿分辨率减半带来的信息损失。 🔹 为什么提出 Patch Merging\n\n\n分层表示 (Hierarchical Representation) • 模仿 CNN 的金字塔结构，从局部细节逐步聚合到全局语义。 • 有利于下游任务（检测、分割）中不同尺度的目标建模。\n计算效率 • token 数量逐层减少 → Attention 的复杂度大幅下降。 • 保证模型可扩展到大分辨率图像。\n语义信息聚合 • 通过合并相邻 patch，模型能把更大感受野的信息整合到新的 token 中。\n\n\nx = x.view(B, H, W, C)\n\nx0 = x[:, 0::2, 0::2, :]  # (B, H/2, W/2, C)\nx1 = x[:, 1::2, 0::2, :]  # (B, H/2, W/2, C)\nx2 = x[:, 0::2, 1::2, :]  # (B, H/2, W/2, C)\nx3 = x[:, 1::2, 1::2, :]  # (B, H/2, W/2, C)\n\nx = torch.cat([x0, x1, x2, x3], -1)  # (B, H/2, W/2, 4*C)\nx = x.view(B, -1, 4 * C)  # (B, H/2*W/2, 4*C)\n\nx = self.reduction(x)  # (B, H/2*W/2, 2*C)"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#relative-position-encoding",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#relative-position-encoding",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "1.5 Relative Position Encoding",
    "text": "1.5 Relative Position Encoding\n与Transformer 和 Vision-Transformer 中不同的是，Swin Transformer利用的是Relative Position Encoding。\n\\[\n\\text{Attention}(Q, K, V) = \\text{Softmax}\\left( \\frac{QK^{T}}{\\sqrt{ d }} +B\\right) V\n\\]\n1.  定义偏置表 (relative_position_bias_table)\n•   大小是 (2*Wh-1) * (2*Ww-1, num_heads)\n•   意味着窗口内的任意两个 token 的相对位置 (dx, dy)，都有一个可学习的偏置值（每个 head 一份）。\n•   例如窗口是 7×7 → 相对位置范围是 [-6,6]，所以表大小是 13×13=169，每个位置存一组偏置\n\n\n2.  计算相对位置索引 (relative_position_index)\n•   首先生成窗口内每个 token 的坐标。\n•   然后做差，得到任意两个 token 的相对坐标 (dx, dy)。\n•   再映射成表的索引（通过移位和哈希成一个整数 index）。\n•   结果是一个 (Wh*Ww, Wh*Ww) 的矩阵，每个元素存两个 token 之间在 bias 表里的索引。\n\n\n•   在图像里，相对位置比绝对位置更重要：\n•   比如一个像素的左邻和右邻很相似，无论这个像素在图像的哪个地方。\n\\[\n\\begin{tabular}\n\\Xhline{1.0pt}\n& \\multicolumn{2}{c|}{ImageNet} & \\multicolumn{2}{c|}{COCO} & \\multicolumn{1}{c}{ADE20k} \\\\\n& top-1 & top-5  & AP$^\\text{box}$ & AP$^\\text{mask}$ & mIoU \\\\\n\\hline\nno pos. & 80.1 & 94.9 & 49.2 & 42.6  & 43.8 \\\\\nabs. pos. & 80.5 & 95.2 & 49.0 & 42.4  & 43.2 \\\\\nabs.+rel. pos. & 81.3 & 95.6 & 50.2 & 43.4 & 44.0\\\\\nrel. pos. w/o app. & 79.3 & 94.7 & 48.2 & 41.9 & 44.1 \\\\\nrel. pos. & \\textbf{81.3} & \\textbf{95.6} & \\textbf{50.5} & \\textbf{43.7} & \\textbf{46.1} \\\\\n\\Xhline{1.0pt}\n\\end{tabular}\n\\]\n\n1.5.1 Fine-Tuning in different image size\n和 Vision-Transformer 一样，当输入的图片和训练时不一样，我们可以通过 bi-cubic interpolation 来增大Relative Position\n\nThe learnt relative position bias in pre-training can be also used to initialize a model for fine-tuning with a different window size through bi-cubic interpolation  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.5 \n\n\\[\n\\begin{table}\n    \\centering\n\\small\n\\addtolength{\\tabcolsep}{-4.0pt}\n\\begin{tabular}\n\\Xhline{1.0pt}\n\\multirow{2}{*}{method} & \\multicolumn{4}{c|}{MSA in a stage (ms)} & \\multicolumn{3}{c}{Arch. (FPS)} \\\\\n& S1 & S2 & S3 & S4 & T & S & B \\\\\n\\hline\nsliding window (naive) & 122.5 & 38.3 & 12.1 & 7.6 & 183 & 109 & 77 \\\\\nsliding window (kernel)  & 7.6 & 4.7 & 2.7 & 1.8 & 488 & 283 & 187 \\\\\n\\hline\nPerformer~\\cite{choromanski2020performer} & 4.8 & 2.8 & 1.8 & 1.5 & 638 & 370 & 241 \\\\\n\\hline\nwindow (w/o shifting) & 2.8 & 1.7 & 1.2 & 0.9 & 770 & 444 & 280 \\\\\n\\hline\nshifted window (padding) & 3.3 & 2.3 & 1.9 & 2.2 & 670 & 371 & 236 \\\\\nshifted window (cyclic)  & 3.0 & 1.9 & 1.3 & 1.0 & 755 & 437 & 278 \\\\\n\\Xhline{1.0pt}\n\\end{tabular}\n    \\caption{Real speed of different self-attention computation methods and implementations on a V100 GPU. }\n    \\label{tab:ablation-selfatt-efficient}\n\\normalsize\n\\end{table}\n\\]"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#others",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#others",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "1.6 Others",
    "text": "1.6 Others\n除了以上几个，Swin Transformer 中还有其他Component，比如 ：\n\nPatch Embedding\nLinear Projection\nFeedForward\nLayer Normalization 在此，就不赘述了，有需要的同学，请参考前一篇 Vision-Transformer， 或者 Transformer"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#downstream-tasks",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#downstream-tasks",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "1.7 Downstream Tasks",
    "text": "1.7 Downstream Tasks\n当一场图片传入Swin Transformer， 它可以提取出图片的特征。 \\[\n\\mathrm{z} = f_{\\theta}(\\mathrm{x}), \\quad \\text{where}\\ \\mathrm{x} \\in \\mathbb{R}^{3\\times H \\times W}, \\mathrm{z} \\in \\mathbb{R}^{H'W' \\times C}\n\\]\n一张图片转化成了 \\(H'W'\\) 个特征，每个特征的大小为 $C。\nSwin Transformer 可以有当作基本的backbone，在此基础上，我们可以对下游进行不同的任务，比如：\n\nImage Classification\nObject Detection\nSemantic segmentation\n\n接下来，我们将如何用Swin Transformer在不同的任务中\n\n\n1.7.1 Image Classification\n对于 \\(\\mathrm{z}\\) 的 hidden states，我们可以进行一个Average Pooling，对于每一个特征求均值，然后再将这个传入一个分类头，就可以得到我们Classification了。与 Vision-Transformer 不同的是，Swin Transformer 没有 [CLS] token 来当收集全部的信息。"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#object-detection-semantic-segmentation",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#object-detection-semantic-segmentation",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "1.8 Object Detection & Semantic segmentation",
    "text": "1.8 Object Detection & Semantic segmentation\nBackbone (Swin Transformer)：\n\nStage 1: [N, C1, H/4, W/4]\nStage 2: [N, C2, H/8, W/8]\nStage 3: [N, C3, H/16, W/16]\nStage 4: [N, C4, H/32, W/32]\n\n可以得到 FPN(Lin et al. 2017) to create a pyramid of feature maps suitable for detection.\n{P2: [N, C, H/4, W/4], \n P3: [N, C, H/8, W/8], \n P4: [N, C, H/16, W/16], \n P5: [N, C, H/32, W/32]}\n有了这些FPN 之后，我们可以结合不同的算法，来进行不同的任务，比如\n例子 1：目标检测 (Object Detection)\n以 Swin Transformer + Faster R-CNN (Ren et al. 2016) 为例： 1. 输入图像：一张 800×1333 的 COCO 数据集图片。\n3.  FPN (特征金字塔网络)：将多尺度特征融合，形成统一的金字塔特征。\n4.  RPN (Region Proposal Network)：在特征图上生成候选区域。\n5.  RoI Head：对候选区域进行分类 (车、人、狗…) 和边框回归。\n6.  输出：预测结果，例如：\n•   “一辆车” → 边框 (x1,y1,x2,y2) + 类别 “car”\n•   “一个人” → 边框 + 类别 “person”\n 👉 在 COCO 数据集上，Swin-T + Faster R-CNN比 ResNet-50 + Faster R-CNN 的 mAP 提高约 5~6 个点。\n语义分割 (Semantic Segmentation)  以 Swin Transformer + UPerNet(Xiao et al. 2018)为例： 1. 输入图像：一张 512×512 的 ADE20K 数据集图片。 2. Backbone (Swin Transformer)：同样输出 1/4, 1/8, 1/16, 1/32 四个尺度特征。 3. FPN/UPerNet Head： • 将多层特征融合，对应不同语义层级。 • 利用融合后的特征生成像素级预测。 4. 预测图 (segmentation map)：大小 512×512，每个像素属于一个类别。 • [0,0] 像素 → “sky” • [100,150] 像素 → “building” • [200,300] 像素 → “road” 5. 输出：完整的语义分割图，每个像素都有类别标签。\n👉 在 ADE20K 上，Swin-L + UPerNet 的 mIoU 达到 53.5+，比传统 CNN backbone 提升显著。 具体的实现细节，等到以后我们阅读到关于Segmentation的内容在，再来实现"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#training-details",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#training-details",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "1.9 Training Details",
    "text": "1.9 Training Details\n\nWe employ an AdamW optimizer for 300 epochs using a cosine decay learning rate scheduler and 20 epochs of linear warm-up. A batch size of 1024, an initial learning rate of 0.001, and a weight decay of 0.05 are used. We include most of the augmentation and regularization strategies of in training, except for repeated augmentation and EMA, which do not enhance performance.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.5 \n\n\n1.9.1 DropPath\n论文中还用到了 DropPath 来当作一种 Regularization。 DropPath 也称之为 Stochastic Depth (Huang et al. 2016) , 它是一种应用在Residual Network， 在训练过程中，随机丢弃整个 残差分支 (residual branch) 或 整个路径 (path)。减少过拟合，同时让模型学会依赖不同深度的路径，提升训练稳定性。 \n与Dropout 不同的是， Dropout 丢弃的是单个神经元的输出， 而DropPath 丢弃的是整个残差分支 / 整层 Block\n\n\n\n\n\n\n\n\n特性\nDropout (经典)\nDropPath (Stochastic Depth)\n\n\n\n\n丢弃对象\n单个神经元的输出\n整个残差分支 / 整层 Block\n\n\n应用粒度\n逐元素 (element-wise)\n层级 (layer-wise)\n\n\n使用场景\n全连接层、CNN、RNN 等\n残差网络、Transformer 等\n\n\n推理阶段效果\n不丢弃，使用缩放补偿\n不丢弃，保留完整路径\n\n\n作用\n减少神经元过拟合\n防止深层网络过拟合、提升稳定性\n\n\n\ndef drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    output = x.div(keep_prob) * random_tensor\n    return output\n\n\nclass DropPath(nn.Module):\n    def __init__(self, drop_prob=None):\n        super().__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n        \n\nclass SwinTransformerBlock(nn.Module):\n    def __init__(self):\n        ...\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else nn.Identity()\n        \n    \n    def forward(self, x):\n        shortcut = x \n        ... # Attention \n        x = shortcut + self.drop_path(x)\n        \n        shortcut = x \n        ... # FFN\n        x = shortcut + self.drop_path(x)\n        \n        ...\n        \n        return x \n\n📝 TAKEAWAY DropPath（也叫 Stochastic Depth）是一种正则化方法，它在训练时随机跳过（丢弃）整个网络层或分支的计算，以减少过拟合并提高模型的泛化能力。\n\n\n\n1.9.2 Gradient Checkpoint\n在此，我们在介绍一个训练方法，用于加速训练，叫做Gradient Checkpoint又叫做Activation Checkpoint， 用PyTorh实现，是很容易的 的，我们只需要call utils.checkpoint\n正常训练流程： 在前向传播（forward）时，每一层的中间激活值（activation）都会保存下来，以便反向传播（backward）时用来计算梯度。 问题是：保存所有中间激活值会消耗大量显存（GPU memory）。 • Gradient Checkpoint 的思路： 并不是保存所有激活值，而是只在部分关键节点（checkpoint）保存激活。 对于未保存的激活值，在反向传播时重新再跑一次前向计算来得到它们，从而节省显存。\n换句话说：用计算换显存。\n🔹 工作机制 1. 在前向传播时： • 模型被切分成若干块（segments）。 • 只保存每一块的输入，丢弃中间的激活。 2. 在反向传播时： • 需要用到梯度时，重新对那一块做一次 forward 来恢复激活。 • 然后正常计算梯度。\n•   增加计算开销：因为要在 backward 时重新做一次 forward。\n•   一般会带来 20%～30% 额外的训练时间。\nimport torch\nfrom torch import nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(1024, 1024)\n        self.layer2 = nn.Linear(1024, 1024)\n\n    def forward(self, x):\n        def custom_forward(*inputs):\n            return self.layer2(self.layer1(*inputs))\n        \n        # 对这部分使用 checkpoint\n        x = checkpoint(custom_forward, x)\n        return x\n\n📝 TAKEAWAY Gradient Checkpointing 是一种 用额外计算换显存 的方法，通过在前向传播时少存激活，反向传播时重算，能让大模型在有限显存下完成训练。"
  },
  {
    "objectID": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#swin-v2",
    "href": "posts/PapersWithCode/03-swin-transformer/Swin-Transformer.html#swin-v2",
    "title": "03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)",
    "section": "1.10 Swin V2",
    "text": "1.10 Swin V2\n“Swin V2” (Liu et al. 2022) 是在原始 Swin Transformer 的基础上，为了更好地 扩展模型容量（更多参数）、处理高分辨率输入 以及 提高训练稳定性 所做的一系列改进。 在视觉任务中，Transformer 模型若要变得更强（更多参数、更高分辨率输入、更多层数）就会遇到几个挑战： 1. 训练不稳定：随着模型变深、通道变宽，内部激活的幅度可能急剧增长，导致梯度、数值不稳定。 2. 分辨率迁移问题：模型在低分辨率下预训练（例如 224×224）后，用在高分辨率（例如 1,536×1,536）或不同窗口尺寸时表现会下降。 3. 对标注数据的过度依赖：大模型需要大量标注数据才能训练得好。\nSwin V2 就是为了克服这些障碍，支持训练超大模型（如 30 亿参数级别），同时能处理大尺寸输入 \n\n1.10.1 Post normalization\nclass SwinTransformerBlock(nn.Module):\n    def __init__(self):\n        ...\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else nn.Identity()\n        \n    \n    def forward(self, x):\n        shortcut = x \n        ... # Attention \n        x = shortcut + self.drop_path(self.norm1(x))\n        \n        shortcut = x \n        ... # FFN\n        x = x + self.drop_path(self.norm2(self.mlp(x)))\n        \n        ...\n        \n        return x \n\n\n1.10.2 Scaled cosine attention\nclass WindowAttention:\n    def __init__(self,):\n    \n        ...\n        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)\n        ...\n    \n    \n    def forward(self, x):\n        attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1))\n        logit_scale = torch.clamp(self.logit_scale, max=torch.log(torch.tensor(1. / 0.01))).exp()\n        attn = attn * logit_scale\n\n\n1.10.3 Log-spaced Continuous Position Bias(Log-CPB)\nlog-spaced continuous position bias approach to address the issue in transferring across window resolutions\n\\[\n\\begin{split}\n\\widehat{\\Delta x} &= \\operatorname{sign}(x) \\cdot \\log(1 + |\\Delta x|) \\\\\n\\widehat{\\Delta y} &= \\operatorname{sign}(y) \\cdot \\log(1 + |\\Delta y|)\n\\end{split}\n\\]\nself.cpb_mlp = nn.Sequential(nn.Linear(2, 512, bias=True),\n                             nn.ReLU(inplace=True),\n                             nn.Linear(512, num_heads, bias=False))\n                             \ndef forward(self, x):\n    relative_position_bias_table = self.cpb_mlp(self.relative_coords_table).view(-1, self.num_heads)"
  },
  {
    "objectID": "posts/PapersWithCode/06-vae/VAE.html",
    "href": "posts/PapersWithCode/06-vae/VAE.html",
    "title": "06: Auto-Encoding Variational Bayes(VAE)",
    "section": "",
    "text": "1 Preliminary\n  \n  1.1 Latent Variable Models\n  1.2 AutoEncoder\n  1.3 KL-Divergence\n  1.4 Variational Inference\n  \n  2 VAE\n  \n  2.1 Re-parameterization Trick\n  2.2 Amortized Inference\n  2.3 Experiements\n  \n  3 Summary\n  4 Key Concepts\n  5 Q & A\n  6 Related resource & Further Reading\n介绍了Vision Model 和 Language Model， 以及如何通过Self-Supervised Learning 来学习Vision的 Representation DINO, 接下来我们将会更新一系列的生成式模型（Generative Models）。生成式模型在当下是很热门的话题，比如Stable Diffusion， Nana Banana 这些模型都是基本思想都是基于Latent Variable Models，接下来，我们就来看看Variable Autoencoder这一篇，Latent Variable 模型。"
  },
  {
    "objectID": "posts/PapersWithCode/06-vae/VAE.html#latent-variable-models",
    "href": "posts/PapersWithCode/06-vae/VAE.html#latent-variable-models",
    "title": "06: Auto-Encoding Variational Bayes(VAE)",
    "section": "1.1 Latent Variable Models",
    "text": "1.1 Latent Variable Models\n在深度学习中，我们经常处理 高维、复杂、带噪声的真实世界数据：图像、语音、文本…… 这些数据的背后，其实常常有一些 未被直接观测到的隐藏结构 —— 这就是 “Latent Variables”（潜变量）。\n想象一下，你看一张人脸照片。照片是观测变量（observed variable）。 但是让这张脸“看起来像某个人”的，是一些不能直接看到的因素： • 光照（lighting） • 情绪（expression） • 姿态（pose） • 脸部特征（identity） • 背景（background）\n这些因素虽然没有在数据中显式标注，却真实存在，并决定了我们看到的图像。 潜变量模型的目标，就是用数学方式把这些“隐藏因素”建模出来。\n\nLatent Variable Model（LVM）是一类 假设观测数据是由一些隐藏变量生成的概率模型。 比如我们看到的数据\\(\\mathrm{x} \\in \\mathbb{R}^{d}\\), 那它有相对应的隐藏变量 \\(z \\in \\mathbb{R}^{k}\\) 其中 \\(k \\ll d\\) . \\(z\\) 是我们观察不到的，也就是所谓的 Latent Variable.\n在使用Latent Variable Model时，我们有两个关键的任务： 1. Inference: 根据 \\(\\mathrm{x}\\) 我们来推断出 \\(\\mathrm{z}\\) 是什么，比如，给定一张人脸的照片，这个模型需要找出 - 这张脸是男生还是女生 - 表情是开心还是悲伤 - 是什么样的姿势 在数学上，就是求后验分布 \\(p(\\mathrm{z} | \\mathrm{x})\\) 2. Generation: 通过 \\(\\mathrm{z}\\)， 我们来生成一个 \\(\\mathrm{x}\\)。 这个就是生成模型。\nLatent Variable Model 是现代生成式模型的基石，基本上所有的生成式模型，比如 GAN， DDPM， Flow Model 等，都是以Latent Variable Model为基础，在此条件下，通过不同求 Latent Variable 的方法，来解决这种问题。"
  },
  {
    "objectID": "posts/PapersWithCode/06-vae/VAE.html#autoencoder",
    "href": "posts/PapersWithCode/06-vae/VAE.html#autoencoder",
    "title": "06: Auto-Encoding Variational Bayes(VAE)",
    "section": "1.2 AutoEncoder",
    "text": "1.2 AutoEncoder\n\nAutoEncoder 是一种Self-Supervised Learning 的方法。 它可以让神经网络学会压缩，并且在还原数据，通过这种方法，我们可以学习到的低纬度的Latent Variable \\(\\mathrm{z}\\)。 AutoEncoder 也可以看作是Latent Variable Model的一种学习方法。 AutoEncoder 也通常用在Representation Learning 表征学习。"
  },
  {
    "objectID": "posts/PapersWithCode/06-vae/VAE.html#kl-divergence",
    "href": "posts/PapersWithCode/06-vae/VAE.html#kl-divergence",
    "title": "06: Auto-Encoding Variational Bayes(VAE)",
    "section": "1.3 KL-Divergence",
    "text": "1.3 KL-Divergence\nKL 散度（Kullback–Leibler Divergence）是用来衡量两个概率分布之间差异的一种度量方法。定义如下: \\[\nD_{KL}(Q \\| P) = \\mathbb{E}_{x \\sim Q}\\left[ \\log \\frac{Q(x)}{P(x)} \\right]\n\\]\n直观的解释KL-Divergence就是: - 如果数据 \\(x\\) 是从 \\(Q\\) 分布中来的，但如果我们用 \\(P\\) 分布 来解释这些数据，会损失多少信息量\n对于高斯分布（Gaussian Distribution），我们KL- Divergence有以下的形式：\n$$\nD_{KL}(Q | P) = $$\n如果是Diagonal Gaussian， 那么KL-Divergence 可以简化为： $$ D_{KL}(q ,|, p)\n_{i=1}^{d} $$\n\\[\nD_{KL}(q(\\mathbf{z}) \\ \\| \\  \\mathcal{N}(0, I)) =\n\\frac{1}{2}\n\\sum_{i=1}^{d}\n\\left(\n\\mu_{q,i}^2 + \\sigma_{q,i}^2 - \\log \\sigma_{q,i}^2 - 1\n\\right)\n\\]\n需要注意的一个点是，KL-Divergence是不对称的， \\[\nD_{KL}(Q \\| P) \\neq D_{KL}(P \\| Q)\n\\]\n为什么我们要强调这一点，是因为： 对于不同位置的Q，P我们所求的是不一样的，简单来说，就是用 \\(\\|\\) 后面的分布，来approximate \\(\\|\\) 前面的分布，具体如下图。"
  },
  {
    "objectID": "posts/PapersWithCode/06-vae/VAE.html#variational-inference",
    "href": "posts/PapersWithCode/06-vae/VAE.html#variational-inference",
    "title": "06: Auto-Encoding Variational Bayes(VAE)",
    "section": "1.4 Variational Inference",
    "text": "1.4 Variational Inference\nVariational Inference（VI，变分推断）是一种用可优化、易计算的分布来近似一个难以求解的后验分布，通过最小化两者之间的 KL 距离，从而实现高效概率推断的方法。 在 Latent Variable 模型中，我们想求： \\[\np (\\mathrm{z} | \\mathrm{x}) = \\frac{p(\\mathrm{x}, \\mathrm{z})}{p(\\mathrm{x})}\n\\]\n通常 \\(p(\\mathrm{x})\\) 是不可求的，因为: \\[\np(\\mathrm{x}) = \\int p(\\mathrm{x}, \\mathrm{z}) d\\mathrm{z}\n\\] 通常是不可能直接求的。因此我们采取一种 “曲线救国” 的方式： 我们不去计算真正的后验，而是找一个 可计算并且可优化的分布来近似它： \\[\nq(z | x) \\approx p(z | x)\n\\] 实现起来也是很简单的 1. 选一个可计算的分布族 （Variational Family） 2. 让它尽可能的接近真实的后验： \\[\n\\underset{\\phi}{\\min}  D_{KL}(q_{\\phi}(z | x ) \\| p(z | x))\n\\]\n直观的来说，Variational Inference 就是：不过不断拉升，旋转，一个椭圆型 \\(q\\), 来使它尽可能的可以和云朵形状的 \\(p\\) 来重叠"
  },
  {
    "objectID": "posts/PapersWithCode/06-vae/VAE.html#re-parameterization-trick",
    "href": "posts/PapersWithCode/06-vae/VAE.html#re-parameterization-trick",
    "title": "06: Auto-Encoding Variational Bayes(VAE)",
    "section": "2.1 Re-parameterization Trick",
    "text": "2.1 Re-parameterization Trick\n 如上图可见，\\(z\\) 是由 \\(x, \\phi\\) 决定的。通过这种方式，那么我们改取怎么样的 \\(g_{\\phi}(x, \\epsilon)\\) 呢。文章中给出了3个基本的方法： 1. 方法1 2. 2 3. c\n\n\n\n\n\n\nREINFORCE\n\n\n\n对于 Re-Parametrization Trick是针对 \\(z\\) 是 Continuous的情况，如果 \\(z\\) 是离散（Discrete) 的 那么我们则可以使用 REINFOCE 的方法。"
  },
  {
    "objectID": "posts/PapersWithCode/06-vae/VAE.html#amortized-inference",
    "href": "posts/PapersWithCode/06-vae/VAE.html#amortized-inference",
    "title": "06: Auto-Encoding Variational Bayes(VAE)",
    "section": "2.2 Amortized Inference",
    "text": "2.2 Amortized Inference\n还有一个比较容易被忽略的一点就是，VAE 还运用了Amortized Inference。什么意思呢？\n传统的变分推断中，我们需要为每一个观测样本 x 单独优化一个变分分布 q(z|x) 的参数，这意味着每来一个新样本都要重新做一遍变分优化，成本非常高。而在 VAE 中，我们不再为每个样本单独优化后验，而是训练一个 共享的推断网络（Encoder） 来预测 q_(z|x) 的参数。也就是说，模型通过学习一个函数 f_(x) 来一次性“摊销”所有样本的推断成本，使得对任意新样本 x，只需一次前向传播就能得到近似后验，不再需要昂贵的 per-sample 优化。这种方式极大地提升了推断效率，也让变分推断能够在深度学习规模上落地。\n也就是用神经网络来一次性学习后验\n 将上面的几个结合起来，我们就得到的了 Auto-Encoding VB Algorithm。\n## 和 AutoEncoder的关系"
  },
  {
    "objectID": "posts/PapersWithCode/06-vae/VAE.html#experiements",
    "href": "posts/PapersWithCode/06-vae/VAE.html#experiements",
    "title": "06: Auto-Encoding Variational Bayes(VAE)",
    "section": "2.3 Experiements",
    "text": "2.3 Experiements\nVAE Loss can be defined as this one:\nclass VAELoss(nn.Module):\n    def __init__(self, rec_loss=\"bce\", kl_beta=1.0):\n        super().__init__()\n\n        self.rec_loss = rec_loss.lower()\n        self.kl_beta = kl_beta\n\n        self.eval()\n\n    def forward(self, x, x_recon, mu, logvar):\n        B = x.shape[0]\n        if self.rec_loss == \"bce\":\n            rec = F.binary_cross_entropy(x_recon, x, reduction=\"sum\")\n        elif self.rec_loss == \"mse\":\n            rec = F.mse_loss(x_recon, x, reduction=\"sum\")\n\n        # KL divergence: D_KL(q(z|x) || p(z))\n        kl = 0.5 * torch.sum(logvar.exp() + mu.pow(2) - logvar - 1)\n\n        total = (rec + self.kl_beta * kl) / B\n\n        return total, {\n            \"recon\": rec.detach().cpu() / B,\n            \"kl\": kl.detach().cpu() / B,\n        }\nThe Loss curve and illustration examples:"
  },
  {
    "objectID": "posts/Projects/projects_index.html",
    "href": "posts/Projects/projects_index.html",
    "title": "Projects",
    "section": "",
    "text": "Qwen3-VL Inference\n\n4 min\n\n\nLarge Language Model\n\nMulti-Modality\n\nFine-Tuning\n\n\n\nBuilt a Qwen3-VL model from scratch in PyTorch, loaded the 14B (224x224) model, fine-tuned it with LoRA for specific tasks, and developed a Gradio app to showcase its…\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaliGemma Inference and Fine Tuning\n\n1 min\n\n\nLarge Language Model\n\nMulti-Modality\n\nFine-Tuning\n\n\n\nBuilt a PaliGemma model from scratch in PyTorch, loaded the 3B (224x224) model, fine-tuned it with LoRA for specific tasks, and developed a Gradio app to showcase its…\n\n\n\nYuyang Zhang\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "posts/Blogs/Position-Embedding/post.html",
    "href": "posts/Blogs/Position-Embedding/post.html",
    "title": "The Evolution of Position Encoding in the Transformer",
    "section": "",
    "text": "1 Preliminary\n  \n  1.1 \\(\\sin\\) and \\(\\cos\\) function\n  1.2 Rotation Matrix and Complex Exponential\n  1.3 Attention in Transformer\n  \n  2 Absolute Positional Encoding\n  \n  2.1 Learned Position Encoding\n  2.2 2D Positional Encoding\n  \n  3 Relative Positional Encoding\n  4 RoPE\n  \n  4.1 2D-RoPE\n  4.2 M-RoPE\n  \n  5 Summary\nAfter Transformer(Vaswani et al. 2023) was introduced, it become the default components in the Deep Learning. It has several components, one of the most important (and most confused for me) was Position Encoding. However the original position encoding methods has several limit, several different methods was proposed later to improve the perform of position encoding in the transformer. In the article, we will explore different methods of position encoding, and extend to the image and video position encoding. But before that, let’s prepare some mathematic background to help us better understand the concepts."
  },
  {
    "objectID": "posts/Blogs/Position-Embedding/post.html#sin-and-cos-function",
    "href": "posts/Blogs/Position-Embedding/post.html#sin-and-cos-function",
    "title": "The Evolution of Position Encoding in the Transformer",
    "section": "1.1 \\(\\sin\\) and \\(\\cos\\) function",
    "text": "1.1 \\(\\sin\\) and \\(\\cos\\) function\n\\(\\sin\\) and \\(\\cos\\) function are two most basic periodic functions that we learned in high school. Both \\(\\sin\\) and \\(\\cos\\) function are periodic with period \\(2\\pi\\): \\[\n\\sin(\\theta + 2\\pi) = \\sin \\theta , \\quad   \\cos(\\theta + 2\\pi) = \\cos \\theta\n\\] The phase-shift(addition) formula for the \\(\\sin\\) and \\(\\cos\\) function is: \\[\n\\begin{split}\n& \\sin (\\theta + \\Delta ) = \\sin \\theta \\cos \\Delta + \\cos \\theta \\sin \\Delta  \\\\\n& \\cos(\\theta + \\Delta) = \\cos\\theta\\cos\\Delta - \\sin\\theta\\sin\\Delta\n\\end{split}\n\\]\nWe can view those as functions of a real variable \\(t\\), than, \\(\\sin t\\) and \\(\\cos t\\) are smooth oscillating waves. A general 1D wave can be written as: \\[\nA \\cos (\\omega t + \\phi)\n\\] where: - \\(A\\) is amplitude: max / min value - \\(\\omega\\) is frequency: how many cycles per unit time. - \\(\\phi\\) is phase shift: horizontal shift\nHere is the plot with different value of \\(\\omega\\):\n\n\n\n\n\n\nFigure 1\n\n\n\nAs we can see in the Figure 1, with larger values of \\(\\omega\\), the \\(\\cos\\) function oscillates more rapidly, while smaller values of \\(\\omega\\) produce slower, less frequent oscillations.\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Prepare data\nt = np.linspace(0, 10, 2000)\nomegas = [0.1, 1, 2]\n\ndata = []\nfor w in omegas:\n    data.append(pd.DataFrame({\"t\": t, \"value\": np.cos(w * t), \"omega\": str(w)}))\n\ndf = pd.concat(data)\n\n# Plot\nsns.set_theme(style=\"whitegrid\")\n\nplt.figure(figsize=(10, 5))\nsns.lineplot(data=df, x=\"t\", y=\"value\", hue=\"omega\")\n\nplt.title(\"cos(ω t) for different ω\")\nplt.xlabel(\"t\")\nplt.ylabel(\"cos(ω t)\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/Blogs/Position-Embedding/post.html#rotation-matrix-and-complex-exponential",
    "href": "posts/Blogs/Position-Embedding/post.html#rotation-matrix-and-complex-exponential",
    "title": "The Evolution of Position Encoding in the Transformer",
    "section": "1.2 Rotation Matrix and Complex Exponential",
    "text": "1.2 Rotation Matrix and Complex Exponential\nThe other usage of the \\(\\sin\\) and \\(\\cos\\) function is the in the rotation matrix. To rotate any 2D vector: \\(\\mathbf{v} = \\begin{pmatrix}x  \\\\y \\end{pmatrix}\\) by and angle \\(\\theta\\) (counterclockwise), we can multiply it be the rotation matrix: \\[\nR(\\theta) =\n\\begin{pmatrix}\n\\cos\\theta & -\\sin\\theta \\\\  \n\\sin\\theta & \\cos\\theta  \\\\\n\\end{pmatrix}\n\\]\nSo, the new vector \\(\\mathbf{v}'\\) be come: \\[\n\\mathbf{v}' = R(\\theta) \\mathbf{v} =\n\\begin{pmatrix}  \nx\\cos\\theta - y\\sin\\theta \\\\\nx\\sin\\theta + y\\cos\\theta  \n\\end{pmatrix}\n\\] One of the good property of rotation matrix is that it is the orthonormal matrix(\\(R^{\\top}R = I\\)), which means the length of original vector will not change after rotate: \\[\n\\|Rv\\|^2 = (Rv)^\\top (Rv) = v^\\top (R^\\top R) v = v^\\top I v = v^\\top v = \\|v\\|^2\n\\]\n\nOn the other hand, a rotation in 2D can also be represented by multiplication be a complex number on the unit circle. The Euler’s Formula is define as: \\[\ne^{i\\theta} = \\cos \\theta + i \\sin \\theta\n\\] This complex number has: - magnitude 1 - angle \\(\\theta\\) By multiplying a complex number by \\(e^{i \\theta}\\) produces a rotation. For a point \\((x, y)\\), it can be represented as a complex number \\(z = x + iy\\). Rotate it by \\(\\theta\\) radians by multiplying \\(z = e^{i\\theta} z\\). Expand using Euler’s formula, we get: \\[\nz = (\\cos \\theta + i \\sin \\theta)(x + i y) = (x \\cos \\theta - y \\sin \\theta) + i(x \\sin \\theta + y \\cos \\theta)\n\\] which is exact the same as the matrix mutiplication.\nv = torch.tensor([1.5, 0.5])  # original vector v = (x, y)\ntheta = torch.tensor(np.pi / 2)  # rotation angle in radians (e.g\n\nz = torch.view_as_complex(v.view(-1, 2))\nrot = torch.exp(1j * theta)  # e^{iθ} j = \\sqrt{-1}.\n\nz_rot = rot * z  # rotated complex number\nv_rot = torch.view_as_real(z_rot).view(-1)  # rotated vector v' = R(θ) v\nWe mentioned that a phase shift of \\(\\Delta\\) in the previous. One of important observation is that it is equivalent to applying a 2D rotation on the vector \\(\\begin{pmatrix} \\cos \\theta \\\\ \\sin  \\theta\\end{pmatrix}\\): \\[\n\\begin{split}\n\\begin{pmatrix} \\cos(\\theta+\\Delta) \\\\ \\sin(\\theta+\\Delta) \\end{pmatrix}\n& = \\begin{pmatrix}\n\\cos\\theta\\cos\\Delta - \\sin\\theta\\sin\\Delta   \\\\\n\\sin \\theta \\cos \\Delta + \\cos \\theta \\sin \\Delta\n\\end{pmatrix} \\\\\n& = \\begin{pmatrix} \\cos\\Delta & -\\sin\\Delta \\\\ \\sin\\Delta & \\cos\\Delta \\end{pmatrix} \\begin{pmatrix} \\cos\\theta \\\\ \\sin\\theta \\end{pmatrix}  \\\\\n& = R(\\Delta) \\begin{pmatrix} \\cos \\theta \\\\ \\sin  \\theta\\end{pmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "posts/Blogs/Position-Embedding/post.html#attention-in-transformer",
    "href": "posts/Blogs/Position-Embedding/post.html#attention-in-transformer",
    "title": "The Evolution of Position Encoding in the Transformer",
    "section": "1.3 Attention in Transformer",
    "text": "1.3 Attention in Transformer\nAttention mechanism is the most important component in the transformer (Vaswani et al. 2023). It process sequences by employ three learnable weight matrices \\(W_{Q}, W_{K}, W_{V}  \\in \\mathbb{R}^{d \\times d_{\\text{model}}}\\). where \\(d_{\\text{model}}\\) represents the dimensionality of the projected subspaces. The matrices transform the input \\(\\mathrm{x}\\) into queries, keys, and values respectively: \\[\nQ = XW_{Q}, \\quad  K = XW_{K}, \\quad  V = XW_{V}\n\\] And the attention matrix is computed using the following formula: \\[\n\\begin{split}\n\\text{Attenion}(Q, K) &= \\text{softmax} \\left( \\frac{QK^{\\top}}{\\sqrt{ d_{\\text{model}} }} \\right) \\\\\nZ &= \\text{Attenion}(Q, K) V\n\\end{split}\n\\]\nAs we can see, on\n\nWe have review some concepts, now we are going to learned different types of the position encodings. The position encoding can be seperated into three different types:\n\nAbsolute Positional Encoding\n\nLearned Positional Encoding\n\nRelative Positional Encoding\nRoPE\n\nLet’s dig into one by one."
  },
  {
    "objectID": "posts/Blogs/Position-Embedding/post.html#learned-position-encoding",
    "href": "posts/Blogs/Position-Embedding/post.html#learned-position-encoding",
    "title": "The Evolution of Position Encoding in the Transformer",
    "section": "2.1 Learned Position Encoding",
    "text": "2.1 Learned Position Encoding\nInstead give each position a pre-defined position embedding vector, we can let neural network to learned the information. After Transformer was proposed, several work such as BERT (Devlin et al. 2019) and GPT. (Radford et al., n.d.) One of the most outstanding is the position encoding in the Vision Transformer (Dosovitskiy et al. 2021).\nEach patch in the image has a position id according to the raser order. Than the index is feed into MLP to project it into the position embedding space:\n\\[\nPE(p) = MLP(p)\n\\]\nCompare to the sine,, this is math simpler to implement:\nclass LearnedPositionEncoding(nn.Module):\n    def __init__(self, max_len, d_model):\n        super().__init__()\n        self.position_embeddings = nn.Embedding(max_len, d_model)\n\n    def forward(self, positions):\n        return self.position_embeddings(positions)\nJust 2 lines of the code.\nHowever, for the image, we known that it has 2D position, the x-axis and y-axis. So, with out flatten the patches, we can use the coordinate to represented the position encoding"
  },
  {
    "objectID": "posts/Blogs/Position-Embedding/post.html#d-positional-encoding",
    "href": "posts/Blogs/Position-Embedding/post.html#d-positional-encoding",
    "title": "The Evolution of Position Encoding in the Transformer",
    "section": "2.2 2D Positional Encoding",
    "text": "2.2 2D Positional Encoding\nSome model such as Diffusion Transformer (Peebles and Xie 2023), Masked Autoencoder (He et al. 2021) are use 2D sincos encoding as the position embedding.\nThe code implementation is as following:\nimport torch\nimport torch.nn as nn\n\n\nclass PosED(nn.Module):\n    def __init__(self, embed_dim, gird_zie):\n        super().__init__()\n\n        self.embed_dim = embed_dim\n        self.grid_size = grid_size\n\n        self.pos_embed = self.build_2d_sincos_pos_embed(\n            self.embed_dim, self.grid_size, cls_token=False, extra_tokens=0\n        )\n\n    def build_2d_sincos_pos_embed(self, embed_dim, grid_size, cls_token, extra_tokens):\n        # --- 1. build 2D grid ---\n        h = torch.arange(grid_size, dtype=torch.float32)\n        w = torch.arange(grid_size, dtype=torch.float32)\n        w_grid, h_grid = torch.meshgrid(w, h, indexing=\"ij\")  # match original numpy ordering: w first\n        grid = torch.stack([w_grid, h_grid], dim=0)  # (2, H, W)\n\n        # reshape to (2, H*W)\n        grid = grid.reshape(2, -1)\n\n        # --- 2. build 2D sin-cos embeddings ---\n        emb_h = self.build_1d_sincos(embed_dim // 2, grid[0])\n        emb_w = self.build_1d_sincos(embed_dim // 2, grid[1])\n        pos_embed = torch.cat([emb_h, emb_w], dim=1)  # (H*W, D)\n\n        # prepend extra tokens (e.g., cls token)\n        if cls_token and extra_tokens &gt; 0:\n            extra = torch.zeros(extra_tokens, embed_dim)\n            pos_embed = torch.cat([extra, pos_embed], dim=0)\n\n        return pos_embed\n\n    def build_1d_sincos(self, embed_dim, pos):\n        \"\"\"\n        embed_dim: D/2\n        pos: (M,)\n        returns: (M, D/2)\n        \"\"\"\n        assert embed_dim % 2 == 0\n\n        omega = torch.arange(embed_dim // 2, dtype=torch.float32)\n        omega = omega / (embed_dim / 2.0)\n        omega = 1.0 / (10000**omega)\n\n        # outer product\n        out = pos[:, None] * omega[None, :]  # (M, D/2)\n\n        emb = torch.cat([torch.sin(out), torch.cos(out)], dim=1)\n        return emb\n\n    def forward(self):\n        return self.pos_embed\nFor the image 2D position encoding is more natural than the 1D-sequence position encoding. For example, a 256×256 image split into 16×16 patches → 16×16 = 256 patches.\nThe actual spatial positions of these patches look like this:\n(0,0) (0,1) (0,2) ... (0,15)\n(1,0) (1,1) (1,2) ... (1,15)\n...\n(15,0)...          (15,15)\nIf you use 1-D sinusoidal positional encoding, the sequence becomes:\ntoken0, token1, token2, ...\nThis causes the model to incorrectly assume: • token1 is closer to token2 • token1 is farther from token17\nBut in the real 2-D image space: • token1 and token2 (left–right neighbors) are close • token1 and token17 (the patch directly below) are also close"
  },
  {
    "objectID": "posts/Blogs/Position-Embedding/post.html#d-rope",
    "href": "posts/Blogs/Position-Embedding/post.html#d-rope",
    "title": "The Evolution of Position Encoding in the Transformer",
    "section": "4.1 2D-RoPE",
    "text": "4.1 2D-RoPE"
  },
  {
    "objectID": "posts/Blogs/Position-Embedding/post.html#m-rope",
    "href": "posts/Blogs/Position-Embedding/post.html#m-rope",
    "title": "The Evolution of Position Encoding in the Transformer",
    "section": "4.2 M-RoPE",
    "text": "4.2 M-RoPE"
  },
  {
    "objectID": "posts/Blogs/Learnings-in-DeepLearning/post.html",
    "href": "posts/Blogs/Learnings-in-DeepLearning/post.html",
    "title": "All kinds of “Learnings” in Deep Learning",
    "section": "",
    "text": "1 Supervised Learning\n  \n  1.1 Regression Problems\n  1.2 Classification Problems\n  \n  2 Unsupervised Learning\n  \n  2.1 Clustering\n  2.2 Generative Models\n  \n  2.2.1 Variational Autoencoders (VAE)\n  2.2.2 Generative Adversarial Networks (GANs)\n  2.2.3 Normalizing Flows\n  2.2.4 Energy-Based Models\n  2.2.5 Diffusion Models\n  \n  \n  3 Self-Supervised Learning\n  \n  3.1 Contrastive Learning\n  3.2 Masked Language Modeling (MLM)\n  3.3 AutoRegressive Modeling\n  \n  4 Semi-Supervised Learning\n  5 Deep Reinforcement Learning\n  6 Representation Learning\n  7 Transfer Learning\n  8 Multi-Task Learning\n  9 Meta Learning\n  10 Online Learning\n  11 Federated Learning\n  12 Curriculum Learning\n  13 Active Learning\n  14 Zero-Shot and Few-Shot Learning\n  15 Continual Learning / Lifelong Learning\n  16 Manifold Learning\n  \n  16.1 JEPA\n  \n  17 Nested Learning\n  18 Summary\nIn the AI field, Deep Learning, which is a subset of machine learning, has revolutionized the way we approach complex problems. One of the key aspects of Deep Learning is the various “learnings” or learning paradigms that enable models to acquire knowledge from data. In this blog, I will cover all kinds of learnings in Deep Learning, including supervised learning, unsupervised learning, self-supervised learning, reinforcement learning, and more. By the end, you will have a solid understanding of different learning paradigms in Deep Learning."
  },
  {
    "objectID": "posts/Blogs/Learnings-in-DeepLearning/post.html#regression-problems",
    "href": "posts/Blogs/Learnings-in-DeepLearning/post.html#regression-problems",
    "title": "All kinds of “Learnings” in Deep Learning",
    "section": "1.1 Regression Problems",
    "text": "1.1 Regression Problems\nWhen the output variable is continuous, the task is referred to as a regression problem. Common examples of regression problems include predicting house prices, stock prices, or temperature values. Popular loss functions used in regression tasks include Mean Squared Error (MSE) and Mean Absolute Error (MAE).\nMean Squared Error (MSE): \\[\n\\mathcal{L}_\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - f(x_i))^2\n\\tag{1}\\]\nMean Absolute Error (MAE): \\[\n\\mathcal{L}_\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - f(x_i)|\n\\tag{2}\\]"
  },
  {
    "objectID": "posts/Blogs/Learnings-in-DeepLearning/post.html#classification-problems",
    "href": "posts/Blogs/Learnings-in-DeepLearning/post.html#classification-problems",
    "title": "All kinds of “Learnings” in Deep Learning",
    "section": "1.2 Classification Problems",
    "text": "1.2 Classification Problems\nWhen the output variable is categorical, the task is referred to as a classification problem. Common examples of classification problems include image classification, spam detection, and sentiment analysis. Popular loss functions used in classification tasks include Cross-Entropy Loss and Hinge Loss.\nCross-Entropy Loss: \\[\n\\mathcal{L}_\\text{CE} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(f_c(x_i))\n\\tag{3}\\] Hinge Loss: \\[\n\\mathcal{L}_\\text{Hinge} = \\frac{1}{N} \\sum_{i=1}^{N} \\max(0, 1 - y_i f(x_i))\n\\tag{4}\\]"
  },
  {
    "objectID": "posts/Blogs/Learnings-in-DeepLearning/post.html#clustering",
    "href": "posts/Blogs/Learnings-in-DeepLearning/post.html#clustering",
    "title": "All kinds of “Learnings” in Deep Learning",
    "section": "2.1 Clustering",
    "text": "2.1 Clustering\nClustering is the task of grouping similar data points together based on their features or characteristics. Common clustering algorithms include K-Means, Hierarchical Clustering, and DBSCAN."
  },
  {
    "objectID": "posts/Blogs/Learnings-in-DeepLearning/post.html#generative-models",
    "href": "posts/Blogs/Learnings-in-DeepLearning/post.html#generative-models",
    "title": "All kinds of “Learnings” in Deep Learning",
    "section": "2.2 Generative Models",
    "text": "2.2 Generative Models\nGenerative models aim to learn the underlying distribution of the data and generate new samples that resemble the original data. Some popular generative models include:\n\nVariational Autoencoders (VAE)\nGenerative Adversarial Networks (GANs)\nNormalizing Flows\nEnergy-Based Models\nDiffusion Models\n\n\n2.2.1 Variational Autoencoders (VAE)\nThe Variational Autoencoder (VAE) is a generative model that combines principles from variational inference and autoencoders. It consists of an encoder network that maps input data to a latent space and a decoder network that reconstructs the data from the latent representation. The VAE is trained to maximize the evidence lower bound (ELBO) on the data likelihood, which encourages the model to learn a meaningful latent representation while also generating realistic samples. The loss function for VAE can be expressed as:\n\\[\n\\mathcal{L}_\\text{VAE} = \\mathbb{E}_{\nq_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) || p(z))\n\\tag{5}\\]\n\n\n2.2.2 Generative Adversarial Networks (GANs)\nGenerative Adversarial Networks (GANs) are a class of generative models that consist of two neural networks: a generator and a discriminator. The generator aims to produce realistic samples that resemble the training data, while the discriminator tries to distinguish between real and generated samples. The two networks are trained in a minimax game, where the generator tries to fool the discriminator, and the discriminator tries to correctly classify real and fake samples. The objective function for GANs can be expressed as:\n\\[\n\\min_G \\max_D \\mathbb{E}_{x \\sim p_\\text{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]\n\\tag{6}\\]\n\n\n2.2.3 Normalizing Flows\nNormalizing Flows are a class of generative models that transform a simple probability distribution (e.g., Gaussian) into a more complex distribution by applying a series of invertible and differentiable transformations. The key idea is to use the change of variables formula to compute the likelihood of the data under the transformed distribution. Normalizing flows are trained by maximizing the log-likelihood of the data, which can be expressed as:\n\\[\n\\log p_X(x) = \\log p_Z(f^{-1}(x)) + \\log \\left| \\det \\frac{\\partial f^{-1}(x)}{\\partial x} \\right|\n\\tag{7}\\]\n\n\n2.2.4 Energy-Based Models\nEnergy-Based Models (EBMs) are a class of generative models that define a probability distribution over the data using an energy function. The energy function assigns low energy values to data points that are likely to occur and high energy values to unlikely data points. The probability distribution is defined as:\n\\[\np(x) = \\frac{e^{-E(x)}}{Z}\n\\tag{8}\\] where \\(E(x)\\) is the energy function and \\(Z\\) is the partition function that normalizes the distribution. EBMs are trained by minimizing the energy of the training data while maximizing the energy of negative samples, which can be achieved using techniques such as Contrastive Divergence or Score Matching.\n\n\n2.2.5 Diffusion Models\nDiffusion Models are a class of generative models that learn to generate data by reversing a diffusion process. The diffusion process gradually adds noise to the data, transforming it into a simple distribution (e.g., Gaussian). The model is trained to learn the reverse process, which denoises the data step by step, ultimately generating samples from the original data distribution. The training objective for diffusion models can be expressed as:\n\\[\n\\mathcal{L}_\\text{diffusion} = \\mathbb{E}_{x_0, \\epsilon, t} \\left[ \\| x_0 - f_\\theta(x_t, t) \\|^2 \\right]\n\\tag{9}\\] where \\(x_0\\) is the original data, \\(x_t\\) is the noised data at time step \\(t\\), \\(\\epsilon\\) is the noise added, and \\(f_\\theta\\) is the denoising function parameterized by \\(\\theta\\)."
  },
  {
    "objectID": "posts/Blogs/Learnings-in-DeepLearning/post.html#contrastive-learning",
    "href": "posts/Blogs/Learnings-in-DeepLearning/post.html#contrastive-learning",
    "title": "All kinds of “Learnings” in Deep Learning",
    "section": "3.1 Contrastive Learning",
    "text": "3.1 Contrastive Learning\n\n\n\n\n\n\nFigure 1: Illustration of Contrastive Learning\n\n\n\nContrastive Learning is a self-supervised learning technique that aims to learn representations by contrasting positive and negative pairs of data points. The model is trained to bring similar data points (positive pairs) closer together in the representation space while pushing dissimilar data points (negative pairs) apart. A popular loss function used in contrastive learning is the InfoNCE loss, which can be expressed as\n\\[\n\\mathcal{L}_\\text{InfoNCE} = - \\log \\frac{\\exp(\\text{sim}(h_i, h_j) / \\tau)}{\\sum_{k=1}^{N} \\exp(\\text{sim}(h_i, h_k) / \\tau)}\n\\tag{10}\\] where \\(h_i\\) and \\(h_j\\) are the representations of the positive pair, \\(h_k\\) are the representations of negative samples, \\(\\text{sim}(\\cdot, \\cdot)\\) is a similarity function (e.g., cosine similarity), and \\(\\tau\\) is a temperature parameter.\nOne of the most popular frameworks for contrastive learning is SimCLR(Chen et al. 2020), which uses data augmentations to create positive pairs and employs a deep neural network to learn representations.\nThe other one is DINO (Caron et al. 2021), which leverages a teacher-student architecture to learn representations without the need for negative samples. \nThe other contrastive learning methods is CLIP(Radford et al. 2021), which learns joint representations of images and text by contrasting image-text pairs. CLIP has demonstrated impressive zero-shot learning capabilities across various vision tasks.\n\n\n\n\n\n\nFigure 2: The illustration of CLIP model"
  },
  {
    "objectID": "posts/Blogs/Learnings-in-DeepLearning/post.html#masked-language-modeling-mlm",
    "href": "posts/Blogs/Learnings-in-DeepLearning/post.html#masked-language-modeling-mlm",
    "title": "All kinds of “Learnings” in Deep Learning",
    "section": "3.2 Masked Language Modeling (MLM)",
    "text": "3.2 Masked Language Modeling (MLM)\nMasked Language Modeling (MLM) is a self-supervised learning technique commonly used in natural language processing (NLP). In MLM, a portion of the input tokens in a text sequence are randomly masked, and the model is trained to predict the original tokens based on the context provided by the unmasked tokens. This approach allows the model to learn contextual representations of words and phrases. The loss function for MLM can be expressed as:\n\\[\n\\mathcal{L}_\\text{MLM} = - \\sum_{i \\in \\mathcal{M}} \\log p_\\theta(x_i | x_{\\setminus \\mathcal{M}})\n\\tag{11}\\] where \\(\\mathcal{M}\\) is the set of masked token positions, \\(x_i\\) is the original token at position \\(i\\), and \\(x_{\\setminus \\mathcal{M}}\\) represents the input sequence with masked tokens."
  },
  {
    "objectID": "posts/Blogs/Learnings-in-DeepLearning/post.html#autoregressive-modeling",
    "href": "posts/Blogs/Learnings-in-DeepLearning/post.html#autoregressive-modeling",
    "title": "All kinds of “Learnings” in Deep Learning",
    "section": "3.3 AutoRegressive Modeling",
    "text": "3.3 AutoRegressive Modeling\nAutoRegressive Modeling is another self-supervised learning technique widely used in natural language processing . In autoregressive modeling, the model is trained to predict the next token in a sequence given the previous tokens. This approach allows the model to learn sequential dependencies and generate coherent text. The loss function for autoregressive modeling can be expressed as: \\[\n\\mathcal{L}_\\text{AR} = - \\sum_{i=1}^{N} \\log p_\\theta(x_i | x_{&lt;i})\n\\tag{12}\\] where \\(x_i\\) is the token at position \\(i\\), and \\(x_{&lt;i}\\) represents the sequence of tokens preceding position \\(i\\)."
  },
  {
    "objectID": "posts/Blogs/Learnings-in-DeepLearning/post.html#jepa",
    "href": "posts/Blogs/Learnings-in-DeepLearning/post.html#jepa",
    "title": "All kinds of “Learnings” in Deep Learning",
    "section": "16.1 JEPA",
    "text": "16.1 JEPA\nJoint Embedding Predictive Architecture (JEPA) is a novel learning paradigm that focuses on learning joint embeddings of data from multiple modalities or views. The main idea behind JEPA is to learn a shared representation space where data points from different modalities can be compared and related to each other. This approach allows the model to capture the underlying structure and relationships between different types of data, enabling it to perform tasks such as cross-modal retrieval, multi-modal classification, and representation learning."
  },
  {
    "objectID": "posts/Blogs/LLM-Inference/post.html",
    "href": "posts/Blogs/LLM-Inference/post.html",
    "title": "LLM Part2: Inference",
    "section": "",
    "text": "In the part 1 of the LLM series, we covered the architecture of LLMs, including key components such as position encoding, attention mechanisms, and more. We also explored various normalization techniques and the training process for these models. In the part 2, we will explore different inference techniques, which is necessary for effectively utilizing LLMs in real-world applications. And we will also explore several practical examples and use cases to illustrate these techniques in action, such as:\n\nvLLM\n\nhttps://lilianweng.github.io/posts/2023-01-10-inference-optimization/ # Resource Different Architecture: - Mixture of Recursion: https://arxiv.org/abs/2507.10524 - Diffusion Text:\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Blogs/Speed-up-training/post.html",
    "href": "posts/Blogs/Speed-up-training/post.html",
    "title": "Speed Up Training for Neural Networks",
    "section": "",
    "text": "1 Preliminary\n  \n  1.1 Data Representation\n  1.2 Calculate Memory Usage of Model\n  1.3 Collective operations\n  \n  2 Profiling\n  \n  2.1 Simple Benchmarking\n  2.2 PyTorch Profiler\n  2.3 NVIDIA Nsight Systems\n  \n  3 Single GPU Optimization\n  \n  3.1 Fusion\n  3.2 Tiling\n  3.3 Memory Coalescing\n  3.4 Mixed Precision Training\n  3.5 Gradient Accumulation\n  3.6 Case Study: Flash Attention\n  \n  4 Multi-GPU Optimization(Parallelism)\n  \n  4.1 Data Parallelism\n  4.2 Model Parallelism\n  4.3 Pipeline Parallelism\n  4.4 Tensor Parallelism\n  4.5 Context Parallelism\n  4.6 Case Study: DeepSpeed\n  4.7 Case Study: Megatron-LM\nTraining large neural networks(such as large language models) can be time-consuming and resource-intensive. This blog post explores various techniques to speed up the training process, including optimizing calculations on a single GPU through different techniques such as fusion, tiling, memory coalescing, and parallelizing the training across multiple GPUs such as model parallelism and data parallelism. But before that, we need to understand the basic concepts of GPU, and data types to better understand why and when we need to use these techniques."
  },
  {
    "objectID": "posts/Blogs/Speed-up-training/post.html#preliminary",
    "href": "posts/Blogs/Speed-up-training/post.html#preliminary",
    "title": "Speed Up Training for Neural Networks",
    "section": "1 Preliminary",
    "text": "1 Preliminary\n\n1.1 Data Representation\nIn deep learning, we often use different data types to represent our data and model parameters. The most common data types are:\n\nFloat32: also known as single-precision floating-point format. This is the default floating-point representation  used in most deep learning frameworks. It provides a good balance between precision and performance. It use 32 bits (4 bytes) to represent a number.\nFloat16: This is a half-precision floating-point representation that uses 16 bits instead of 32 bits. It can significantly speed up training and reduce memory usage, but it may lead to numerical instability in some cases.\nBFloat16: This is a truncated version of Float32 that retains the exponent bits but reduces the mantissa bits. It is designed to provide a good trade-off between precision and performance, especially for training large models.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The representation of Float32\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) The representation of Float16\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) The representation of BFloat16\n\n\n\n\n\n\n\nFigure 1: The representation of float32, float16, and bfloat16 data types. The figure shows how the bits are allocated for the sign, exponent, and mantissa in each data type.\n\n\n\n\n\n\n\n\n\nHow those bits represent the number?\n\n\n\n\\[\n\\text{value} = (-1)^s \\times (1.f) \\times 2^{e - 127}\n\\]\nwhere:\n\n\\(s\\) is the sign bit (0 for positive, 1 for negative)\n\\(f\\) is the mantissa (the fractional part): \\(1.f = 1 + \\sum_{i=1}^{23} b_i \\cdot 2^{-i}\\), where \\(b_i\\) are the bits of the mantissa either 0 or 1.\n\\(e\\) is the exponent (an 8-bit unsigned int) with a bias of 127:\n\nFor Float32, \\(e\\) is 8 bits, which range from [1, 254]\nFor Float16, \\(e\\) is 5 bits, which range from [1, 30]\nFor BFloat16, \\(e\\) is 8 bits, which range from [1, 254]\n\n\n\n\nTo check the data type of a tensor and its properties in PyTorch, you can use the .dtype attribute. For example:\nx = torch.zeros(4, 8)\nx.dtype # check the data type of x\nx.numel() # check the number of elements in x\nx.element_size() # check the size of each element in bytes\nx.numel() * x.element_size() # check the total size in bytes\n\n\n1.2 Calculate Memory Usage of Model\nAssume we have a model with \\(N\\) parameters, and each parameter is represented by float32 (4 bytes). \\(A\\) is the number of activation elements stored during forward (depends on input and model depth). How can we calculate the memory need for training this model? Notice that the memory usage of a model is not only determined by the parameters, but also by:\n\nactivations\ngradients\noptimizer states\n\nFor a single parameter, the memory usage for one forward pass and backward pass is:\n\nparameter: 4 bytes (float32)\nactivation: 4 bytes (float32)\ngradient: 4 bytes (float32)\noptimizer state, which can vary depending on the optimizer used. For example, Adam optimizer requires 2 additional states (momentum and variance), each of which is 4 bytes (float32).\n\nSo, the total memory usage for one parameter is: \\[\n\\text{Memory per parameter} = 4 + 4  + 2 \\times 4 = 16 \\text{ bytes}\n\\]\nThus, the total memory usage for the model is: \\[\n\\text{Total Memory} = N \\times 16 \\text{ bytes} + A \\times 4 \\text{ bytes}\n\\]\n\n\n\n\n\n\nWhy need activation for backward pass?\n\n\n\nSay a layer denotes the input to the layer as \\(\\mathbf{x}\\), the weights as \\(\\theta\\), and the output as \\(\\mathbf{y}\\). The loss function is denoted as \\(L\\), which is a function of the output \\(\\mathbf{y}\\) and the target \\(t\\): \\[\n\\mathbf{y} = f(\\mathbf{x}; \\theta)\n\\] To compute the gradient of the loss with respect to the weights, we need to use the chain rule: \\[\n\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\cdot \\frac{\\partial \\mathbf{y}}{\\partial \\theta}\n\\]\nwhere \\(\\frac{\\partial \\mathbf{y}}{\\partial \\theta}\\) is the gradient of the output with respect to the weights \\(\\theta\\), which usually are the function of the input \\(\\mathbf{x}\\) and the weights \\(\\theta\\). To compute this gradient, we need to know the input \\(\\mathbf{x}\\). For example, the linear layer computes: \\[\n\\mathbf{y} = \\mathbf{x} \\cdot \\theta + b\n\\] to compute the gradient, we need to know the input \\(\\mathbf{x}\\), \\[\n\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\cdot \\mathbf{x}\n\\] where \\(\\mathbf{x}\\) is the input to the layer, which is the activation of the previous layer. Thus, we need to store the activation for the backward pass.\n\n\n\n\n1.3 Collective operations\nCollective operations are operations that involve multiple processes or devices, such as GPUs, to perform a computation. They are essential for parallelizing the training process across multiple GPUs. Some common collective operations include:\n\nBroadcast(Figure 2 (a)): This operation sends data from one process to all other processes. It is commonly used to share model parameters or hyperparameters across multiple GPUs.\nScatter(Figure 2 (b)): This operation distributes data from one process to multiple processes. It is often used to distribute input data across multiple GPUs.\nGather(Figure 2 (c)): This operation collects data from multiple processes and combines it into a single process. It is useful for aggregating results from multiple GPUs.\nReduce(Figure 2 (d)): This operation combines data from multiple processes into a single process. It is commonly used to compute the sum or maximum of gradients across multiple GPUs.\nAll-gather(Figure 2 (e)): This operation collects data from all processes and distributes it back to all processes. It is often used to gather gradients or model parameters from multiple GPUs without losing any information.\nAll-reduce(Figure 2 (g)): This operation combines data from all processes and distributes the result back to all processes. It is often used to average gradients across multiple GPUs during training.\nReduce-scatter(Figure 2 (f)): This operation combines data from multiple processes and distributes the result to each process. It is often used to reduce the amount of data that needs to be communicated between processes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Broadcast\n\n\n\n\n\n\n\n\n\n\n\n(b) Scatter\n\n\n\n\n\n\n\n\n\n\n\n(c) Gather\n\n\n\n\n\n\n\n\n\n\n\n(d) Reduce\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) All-gather\n\n\n\n\n\n\n\n\n\n\n\n(f) Reduce-scatter\n\n\n\n\n\n\n\n\n\n\n\n(g) All-reduce\n\n\n\n\n\n\n\nFigure 2: The illustration of different collective operations. The figure shows how data is communicated between processes in each operation.(Image take from: Stanford CS336)\n\n\n\n\nOne should take note is Reduce-Scatter combines two operations:\n\nReduce: Each process (or GPU) contributes its data, and a reduction operation (usually sum, mean, max, etc.) is applied across processes.\nScatter: The reduced result is partitioned and each process gets only a portion (its shard) of the reduced result.\n\n\n\n\n\n\n\nTip\n\n\n\nWay to remember the terminology:\n\nGather: collects data from multiple sources into one destination(not do any operation)\nReduce: performs some associative/commutative operation (sum, min, max)\nBroadcast/Scatter: is inverse of Gather\nAll: means destination is all devices"
  },
  {
    "objectID": "posts/Blogs/Speed-up-training/post.html#profiling",
    "href": "posts/Blogs/Speed-up-training/post.html#profiling",
    "title": "Speed Up Training for Neural Networks",
    "section": "2 Profiling",
    "text": "2 Profiling\nTo optimize the training process, we need to first profile our model to identify the bottlenecks. In this section, we will introduce several tools to profile the model and understand where the time is spent during training. We will discuss several tools that can help us profile our model and identify the bottlenecks in the training process:\n\nSimple Benchmarking (Section 2.1): The simplest way to measure the time taken for each operation in your model. You can use the time module in Python to measure the time taken for each operation. For example, you can wrap your forward pass in a timer to measure the time taken for each layer.\nPyTorch Profiler (Section 2.2): PyTorch provides a built-in profiler that can help you analyze the performance of your model. You can use the torch.profiler module to profile your model and visualize the results. The profiler provides detailed information about the time spent on each operation, memory usage, and more.\nNVIDIA Nsight Systems (Section 2.3): This is a powerful profiling tool that can help you analyze the performance of your model on NVIDIA GPUs. It provides detailed information about the GPU utilization, memory usage, and more. You can use it to identify bottlenecks in your model and optimize the performance.\n\n\n2.1 Simple Benchmarking\n\n\n2.2 PyTorch Profiler\n\n\n2.3 NVIDIA Nsight Systems"
  },
  {
    "objectID": "posts/Blogs/Speed-up-training/post.html#single-gpu-optimization",
    "href": "posts/Blogs/Speed-up-training/post.html#single-gpu-optimization",
    "title": "Speed Up Training for Neural Networks",
    "section": "3 Single GPU Optimization",
    "text": "3 Single GPU Optimization\nIn this section, we will discuss various techniques to optimize the training process on a single GPU. These techniques include: - Fusion(Section 3.1): This technique combines multiple operations into a single operation to reduce the number of kernel launches and improve performance. For example, you can fuse the forward and backward passes of a layer into a single operation. - Tiling(Section 3.2): This technique divides the input data into smaller tiles and processes them in parallel to improve memory access patterns and reduce memory usage. For example, you can tile the input data into smaller chunks and process them in parallel. - Memory Coalescing(Section 3.3): This technique optimizes memory access patterns to improve memory bandwidth utilization. For example, you can coalesce memory accesses to reduce the number of memory transactions and improve performance. - Mixed Precision Training(Section 3.4): This technique uses lower precision data types (such as float16 or bfloat16) to reduce memory usage and improve performance. It can significantly speed up training while maintaining model accuracy. PyTorch provides built-in support for mixed precision training through the torch.cuda.amp module, which allows you to automatically cast your model and inputs to lower precision during training. - Gradient Accumulation(Section 3.5): This technique accumulates gradients over multiple mini-batches before performing a weight update. It can help reduce the number of weight updates and improve training stability, especially when using large batch sizes. You can implement gradient accumulation by accumulating gradients in a buffer and updating the model parameters only after a certain number of mini-batches.\n\n3.1 Fusion\n\n\n3.2 Tiling\n\n\n3.3 Memory Coalescing\n\n\n3.4 Mixed Precision Training\n\n\n3.5 Gradient Accumulation\n\n\n3.6 Case Study: Flash Attention"
  },
  {
    "objectID": "posts/Blogs/Speed-up-training/post.html#multi-gpu-optimizationparallelism",
    "href": "posts/Blogs/Speed-up-training/post.html#multi-gpu-optimizationparallelism",
    "title": "Speed Up Training for Neural Networks",
    "section": "4 Multi-GPU Optimization(Parallelism)",
    "text": "4 Multi-GPU Optimization(Parallelism)\nIn this section, we will discuss various techniques to optimize the training process across multiple GPUs. These techniques include: - Data Parallelism(Section 4.1): This technique splits the input data across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different subset of the input data, and the gradients are averaged across GPUs before updating the model parameters. - Model Parallelism(Section 4.2): This technique splits the model across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different part of the model, and the gradients are averaged across GPUs before updating the model parameters. This is particularly useful for large models that do not fit into a single GPU’s memory. - Pipeline Parallelism(Section 4.3): This technique splits the model into multiple stages and processes each stage in parallel across multiple GPUs. Each GPU processes a different stage of the model, and the output of one stage is passed to the next stage. This can help improve throughput and reduce memory usage, especially for large models. - Tensor Parallelism(Section 4.4): This technique splits the tensors across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different part of the tensor, and the gradients are averaged across GPUs before updating the model parameters. This is particularly useful for large tensors that do not fit into a single GPU’s memory. - Context Parallelism(Section 4.5): This technique splits the context across multiple GPUs and performs the forward and backward passes in parallel. Each GPU processes a different part of the context, and the gradients are averaged across GPUs before updating the model parameters. This is particularly useful for large contexts that do not fit into a single GPU’s memory.\n\n4.1 Data Parallelism\n\n\n4.2 Model Parallelism\n\n\n4.3 Pipeline Parallelism\n\n\n4.4 Tensor Parallelism\n\n\n4.5 Context Parallelism\n\n\n4.6 Case Study: DeepSpeed\n\n\n4.7 Case Study: Megatron-LM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nSelf CPU %\nSelf CPU\nCPU total %\nCPU total\nCPU time avg\nSelf CUDA\nSelf CUDA %\nCUDA total\nCUDA time avg\n# of Calls\n\n\n\n\naten::gelu\n6.76%\n669.785us\n13.48%\n1.336ms\n1.336ms\n8.642ms\n100.00%\n8.642ms\n8.642ms\n1\n\n\nvoid at::native::vectorized_elementwise_kernel&lt;…&gt;\n0.00%\n0.000us\n0.00%\n0.000us\n0.000us\n8.642ms\n100.00%\n8.642ms\n8.642ms\n1\n\n\ncudaLaunchKernel\n6.72%\n665.807us\n6.72%\n665.807us\n665.807us\n0.000us\n0.00%\n0.000us\n0.000us\n1\n\n\ncudaDeviceSynchronize\n86.52%\n8.574ms\n86.52%\n8.574ms\n4.287ms\n0.000us\n0.00%\n0.000us\n0.000us\n2\n\n\n\nSelf CPU time total: 9.909 ms\nSelf CUDA time total: 8.642 ms"
  },
  {
    "objectID": "posts/Blogs/KL-Divergence/post.html",
    "href": "posts/Blogs/KL-Divergence/post.html",
    "title": "From Entropy to KL Divergence: A Comprehensive Guide",
    "section": "",
    "text": "1 Entropy\n  2 Cross-Entropy\n  3 KL Divergence\n  \n  3.1 Monte Carlo Estimation of KL Divergence\n  3.2 Control Variates\n  \n  4 Applications of KL Divergence\n  \n  4.1 KL Divergence in Generative Models\n  \n  4.1.1 Variational Autoencoders (VAEs)\n  \n  4.2 Model Distillation\n  4.3 Reinforcement Learning\n  \n  5 Conclusion\nKL Divergence, one of the most important concepts in information theory and statistics, measures the difference between two probability distributions. It quantifies how much information is lost when one distribution is used to approximate another. It is widely used in various fields, including machine learning, data science, and artificial intelligence. In this blog post, we will explore the concept of KL Divergence, its mathematical formulation, and its applications.\nFirst, let’s start with the concept of entropy, which is the foundation of KL Divergence."
  },
  {
    "objectID": "posts/Blogs/KL-Divergence/post.html#monte-carlo-estimation-of-kl-divergence",
    "href": "posts/Blogs/KL-Divergence/post.html#monte-carlo-estimation-of-kl-divergence",
    "title": "From Entropy to KL Divergence: A Comprehensive Guide",
    "section": "3.1 Monte Carlo Estimation of KL Divergence",
    "text": "3.1 Monte Carlo Estimation of KL Divergence\nIn practice, we sometimes don’t known the true distribution P, but we have samples from it. In this case, we can estimate the KL Divergence using Monte Carlo sampling:\nThis is un-biased estimator of KL Divergence using samples from distribution \\(P\\). However, it may have high variance depending on the number of samples and the distributions involved.\nTo reduce variance, we can use importance sampling:\nThis method uses samples from distribution \\(Q\\) and weights them according to the ratio of probabilities under \\(P\\) and \\(Q\\), leading to a lower variance estimate of KL Divergence.\nAnother method is Control Variates"
  },
  {
    "objectID": "posts/Blogs/KL-Divergence/post.html#control-variates",
    "href": "posts/Blogs/KL-Divergence/post.html#control-variates",
    "title": "From Entropy to KL Divergence: A Comprehensive Guide",
    "section": "3.2 Control Variates",
    "text": "3.2 Control Variates\nControl variates is a variance reduction technique that involves using a correlated variable with known expected value to reduce the variance of an estimator. In the context of KL Divergence estimation, we can use a control variate to improve the estimate:\n\\[\nD_{KL}(P || Q) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\log \\frac{P(x_i)}{Q(x_i)} - c (g(x_i) - E[g(X)]) \\right)\n\\]"
  },
  {
    "objectID": "posts/Blogs/KL-Divergence/post.html#kl-divergence-in-generative-models",
    "href": "posts/Blogs/KL-Divergence/post.html#kl-divergence-in-generative-models",
    "title": "From Entropy to KL Divergence: A Comprehensive Guide",
    "section": "4.1 KL Divergence in Generative Models",
    "text": "4.1 KL Divergence in Generative Models\n\n4.1.1 Variational Autoencoders (VAEs)\nIn VAEs, KL Divergence is used to regularize the latent space by minimizing the divergence between the approximate posterior distribution and the prior distribution.\n\\[\n\\mathcal{L} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - D_{KL}(q(z|x) || p(z))\n\\]"
  },
  {
    "objectID": "posts/Blogs/KL-Divergence/post.html#model-distillation",
    "href": "posts/Blogs/KL-Divergence/post.html#model-distillation",
    "title": "From Entropy to KL Divergence: A Comprehensive Guide",
    "section": "4.2 Model Distillation",
    "text": "4.2 Model Distillation\nIn model distillation, KL Divergence is used to align the output distributions of the teacher and student models.\n\\[\n\\mathcal{L} = D_{KL}(P_{teacher} || P_{student})\n\\]"
  },
  {
    "objectID": "posts/Blogs/KL-Divergence/post.html#reinforcement-learning",
    "href": "posts/Blogs/KL-Divergence/post.html#reinforcement-learning",
    "title": "From Entropy to KL Divergence: A Comprehensive Guide",
    "section": "4.3 Reinforcement Learning",
    "text": "4.3 Reinforcement Learning\nIn reinforcement learning, KL Divergence is used to constrain policy updates to ensure stability. For example, the RL in the LLM (RLHF) uses KL Divergence to keep the updated policy close to the original policy.\n\\[\n\\mathcal{L} = \\mathbb{E}_{s \\sim \\pi_{ref}} \\left[ \\frac{\\pi_{new}(a|s)}{\\pi_{ref}(a|s)} A(s, a) \\right] - \\beta D_{KL}(\\pi_{new} || \\pi_{ref})\n\\]"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html",
    "title": "All About Diffusion & Flow Models",
    "section": "",
    "text": "1 Preliminary\n  \n  1.1 Multivariate Gaussian Distribution\n  \n  1.1.1 Linear Gaussian\n  \n  1.2 KL-Divergence & Fisher Divergence\n  1.3 ELBO\n  1.4 Score function & Langevin Dynamics\n  \n  2 DDPM\n  \n  2.1 Forward and Backward Diffusion Process\n  2.2 Loss Function\n  2.3 Sampling from DDPM\n  2.4 Time Embedding\n  2.5 Sampling\n  \n  3 Score Matching\n  4 Conditioned Generation\n  \n  4.1 Classifier Generation\n  4.2 Classifier-Free Generation\n  \n  5 Speed Up Diffusion Models\n  \n  5.1 DDIM\n  5.2 Progressive Distillation\n  5.3 Consistency Models\n  5.4 Latent Diffusion Model\n  5.5 Score Matching\n  \n  6 From ODE and SDE view point\n  \n  6.1 ODE vs. SDE\n  \n  6.1.1 Vector Field\n  \n  6.2 Conditional Vector Field & Marginal Vector Field\n  6.3 Mean Flow\n  \n  7 Model Architecture\n  \n  7.1 U-Net\n  7.2 Control Net\n  7.3 Diffusion Transformer (DiT)\n  \n  8 Applications\n  \n  8.1 Text-Image Generation\n  \n  8.1.1 Imagen\n  8.1.2 DALL·E\n  8.1.3 Stable Diffusion\n  \n  8.2 Text-Video Generation\n  \n  8.2.1 Meta Movie Gen Video\n  8.2.2 Veo\n  \n  8.3 Language Modeling\n  8.4 Diffusion Policy\n  \n  9 Learning Resource\nThis article offers a comprehensive overview of diffusion models from multiple perspectives. We begin with the foundations—DDPM, DDIM, and Score Matching—and explore their relationships. From there, we introduce the ODE/SDE framework, showing how DDPM can be derived from stochastic differential equations and how this connects to Flow Matching.\nWe then highlight key model variants such as Stable Diffusion and Movie Gen, discussing their architectures and applications. Finally, we broaden the scope to examine how diffusion models are being adapted beyond image generation, including diffusion policies in reinforcement learning and their emerging role in large language models (LLMs)."
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#multivariate-gaussian-distribution",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#multivariate-gaussian-distribution",
    "title": "All About Diffusion & Flow Models",
    "section": "1.1 Multivariate Gaussian Distribution",
    "text": "1.1 Multivariate Gaussian Distribution\nThe probability density function of a random vector \\(x \\in \\mathbb{R}^d\\) that follows a multivariate Gaussian distribution with mean vector \\(\\mu \\in \\mathbb{R}^d\\) and covariance matrix \\(\\Sigma \\in \\mathbb{R}^{d \\times d}\\) is given by:\n\\[\np(x) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}}\n\\exp\\left( -\\tfrac{1}{2}(x - \\mu)^{\\top}\\Sigma^{-1}(x - \\mu) \\right)\n\\tag{1}\\]\nA special case arises when the covariance matrix is the identity, \\(\\Sigma = \\mathbf{I}_{d} \\in \\mathbb{R}^{d \\times d}\\). This is known as the isotropic Gaussian. In deep learning practice, it is common to only predict the mean of the Gaussian, denoted \\(\\mu_{\\theta}\\), while assuming an isotropic covariance:\n\\[\np(x) = \\frac{1}{(2\\pi)^{d/2}}\n\\exp\\left( -\\tfrac{1}{2}(x - \\mu_{\\theta})^{\\top}(x - \\mu_{\\theta}) \\right)\n\\tag{2}\\]\nA fundamental property of Gaussian distributions is that the sum of independent Gaussians is itself Gaussian:\n\\[\nx + y \\sim \\mathcal{N}(\\mu_1 + \\mu_2,\\ \\Sigma_1 + \\Sigma_2)\n\\tag{3}\\]\nAs a simple example, consider two independent random Gaussian variables \\(\\varepsilon_1, \\varepsilon_2 \\sim \\mathcal{N}(0, \\mathbf{I}_d)\\). Define: \\[\n\\mathrm{x}_1 = \\sigma_1 \\varepsilon_1, \\quad \\mathrm{x}_2 = \\sigma_2 \\varepsilon_2\n\\] Then, since \\(\\mathrm{x}_1\\) and \\(\\mathrm{x}_2\\) are independent, their sum satisfies:\n\\[\n\\begin{split}\n\\mathrm{x}_1 + \\mathrm{x}_2 &\\sim \\mathcal{N}(0, (\\sigma_1^2 + \\sigma_2^2)\\mathbf{I}_d) \\\\\n\\mathrm{x}_1 + \\mathrm{x}_2 &= \\sqrt{\\sigma_1^2 + \\sigma_2^2},\\varepsilon,\n\\quad \\varepsilon \\sim \\mathcal{N}(0, \\mathbf{I}_d)\n\\end{split}\n\\tag{4}\\]\n\n1.1.1 Linear Gaussian\nA linear Gaussian model specifies the conditional distribution of \\(\\mathbf{y}\\) given \\(\\mathbf{x}\\) as: \\[\nq(\\mathbf{y}\\mid \\mathbf{x}) = \\mathcal{N}\\big(\\mathbf{A}\\mathbf{x} + \\mathbf{b}, \\ \\boldsymbol{\\Sigma}\\big)\n\\tag{5}\\]\nwhere the mean of the \\(\\mathbf{y}\\) is depend on the \\(\\mathbf{x}\\). One simple case is: \\[\nq(\\mathbf{y}\\mid \\mathbf{x})\n=\\mathcal{N} \\big(\\alpha \\mathbf{x},\\beta\\mathbf{I}_{d}\\big)\n\\]\nAn important point to note is that when \\(\\beta\\) is large, the posterior distribution \\(q(\\mathbf{x}\\mid \\mathbf{y})\\) deviates significantly from being Gaussian. However, in the regime where \\(\\beta \\ll 1\\), the posterior can be well approximated by a Gaussian.This is the one important property we need to understand when are implementing the DDPM, where inference relies on approximating posterior distributions during the reverse diffusion process.\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\nCode Generated above graph: GitHub"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#kl-divergence-fisher-divergence",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#kl-divergence-fisher-divergence",
    "title": "All About Diffusion & Flow Models",
    "section": "1.2 KL-Divergence & Fisher Divergence",
    "text": "1.2 KL-Divergence & Fisher Divergence\nThe Kullback–Leibler (KL) divergence is a measure of how one probability distribution \\(Q\\) diverges from a reference distribution \\(P\\). It is defined as: \\[\nD_{\\text{KL}}(Q \\| P) = \\int Q(z) \\log \\frac{Q(z)}{P(z)}  dz = \\mathbb{E}_{Q}\\left[ \\log \\frac{Q}{P} \\right]\n\\tag{6}\\]\nKey properties:\n\n\\(D_{\\text{KL}} \\geq 0,\\) with equality if and only if \\(Q = P\\) almost everywhere.\nIt is asymmetric: \\(D_{\\text{KL}}(Q \\| P) \\neq D_{\\text{KL}}(P \\| Q)\\).\n\nThe Fisher divergence provides another way to measure discrepancy between two distributions \\(Q\\) and m\\(P\\), focusing on their score functions (the gradients of log densities, we will introduce score function later.). It is defined as: \\[\nD_{F}(Q \\| P) = \\frac{1}{2}  \\mathbb{E}_{z \\sim Q} \\Big[ \\big| \\nabla_z \\log Q(z) - \\nabla_z \\log P(z) \\big|^2 \\Big]\n\\tag{7}\\]\nFor example, for the gaussian distribution Equation 1, the KL-Divergence is:\n\\[\nD_{\\text{KL}}(Q \\| P) = \\frac{1}{2} \\Big(\n\\mathrm{tr}(\\Sigma_p^{-1}\\Sigma_q)(\\mu_p - \\mu_q)^{\\top}\\Sigma_p^{-1}(\\mu_p - \\mu_q)d + \\ln \\frac{\\det \\Sigma_p}{\\det \\Sigma_q}\n\\Big)\n\\tag{8}\\]"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#elbo",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#elbo",
    "title": "All About Diffusion & Flow Models",
    "section": "1.3 ELBO",
    "text": "1.3 ELBO\nIn probabilistic modeling and variational inference, we often want to compute the marginal likelihood of observed data \\(x\\):\n\\[\np(x) = \\int p(x, z),dz = \\int p(x \\mid z),p(z),dz\n\\tag{9}\\]\nwhere:\n\n\\(z\\): is the latent variable.\n\\(p(z)\\): is the prior distribution of the latent variable (we often assume Gaussian for the continuous variable).\n\\(p(x | z)\\): is the likelihood of the data point \\(x\\).\n\nHowever, directly computing \\(p(x)\\) is usually intractable.\n\n\n\n\n\n\nWhy \\(p(x)\\) is intractable?\n\n\n\nDirect computation of \\(p(x)\\) is generally intractable, since the integral is both high-dimensional \\(z \\in \\mathbb{R}^{d}\\) and involves nonlinear functions (e.g., neural networks in generative models).\n\n\nTo address this, we will introduce an tractable approximate distribution(also known as variational distribution) \\(Q_{\\phi}(z |x)\\) to approximate the true posterior \\(P(z |x)\\). Now, let’s re-write the log-likelihood, and insert \\(Q_{\\phi}(z | x)\\) in the equation:\n\\[\n\\begin{split}\n\\log_{\\theta}P(\\mathrm{x})   \n&= \\log \\int P_{\\theta}(\\mathrm{x} | \\mathrm{z}) \\, d\\mathrm{z} \\\\\n&=  \\log \\int P_{\\theta}(\\mathrm{x, z}) \\frac{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\, dx   \\\\\n&=  \\log \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}} \\left[ \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\right] \\\\\n&\\geq   \\boxed{\\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}} \\left[ \\log  \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\right] } \\\\\n&=  \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}} \\left[  \\log\\frac{P_{\\theta}(\\mathrm{x} | \\mathrm{z}) P(\\mathrm{z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}  \\right]    \\\\\n& = \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}}[\\log P_{\\theta}(\\mathrm{x} | \\mathrm{z})] - D_{KL}[Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\| P(\\mathrm{z})]\n\\end{split}\n\\tag{10}\\]\nThe inequality follows from Jensen’s inequality (\\(\\log \\mathbb{E}[f] \\geq \\mathbb{E}[\\log f]\\), since \\(\\log\\) is concave).\nThe boxed expectation is the Evidence Lower Bound (ELBO):\n\\[\n\\text{ELBO} = \\underbrace{ \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}}[\\log P_{\\theta}(\\mathrm{x} | \\mathrm{z})]  }_{ \\text{Reconstruction term} }- \\underbrace{ D_{KL}[Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\| P(\\mathrm{z})] }_{ \\text{Regularization term} }\n\\tag{11}\\]\n\nThe first term encourages the model to reconstruct the data well.\nThe second term regularizes the approximate posterior q_{}(z x) to stay close to the prior p(z).\n\nMaximizing the ELBO therefore makes \\(Q_{\\phi}(z \\mid x)\\) approximate the true posterior, while also maximizing the likelihood of the observed data.\nNow, let’s derive the ELBO from the another perspective, let’s measure how different \\(Q_{\\phi}(\\mathrm{z} | \\mathrm{x})\\) and \\(P(\\mathrm{z}|\\mathrm{x})\\) through KL-divergence:\n\\[\n\\begin{align}\nD_{KL}[Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\| P(\\mathrm{z} | \\mathrm{x})]   \n& = \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\left[ \\log \\frac{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}{P(\\mathrm{z} | \\mathrm{x})} \\right] \\\\\n& = \\int Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\log \\frac{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}{P(\\mathrm{z} | \\mathrm{x})} \\, d\\mathrm{z}  \\\\\n& = - \\int Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\log \\frac{P(\\mathrm{z} | \\mathrm{x})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\, d\\mathrm{z}  \\\\\n& = - \\int Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\log \\frac{P(\\mathrm{z} | \\mathrm{x}) P_{\\theta}(\\mathrm{x})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) P_{\\theta}(\\mathrm{x})}  \\, d\\mathrm{z}  \\\\\n& = - \\int Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\log \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) P_{\\theta}(\\mathrm{x})}  \\, d\\mathrm{z}  \\\\\n& = - \\int Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\log \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}  \\, d\\mathrm{z}   + \\int Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\log P_{\\theta}(\\mathrm{x})  \\, d\\mathrm{z}  \\\\\n& = - \\boxed{\\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}\\left[\\log \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\right]}  + \\log P_{\\theta}(\\mathrm{x})\n\\end{align}\n\\tag{12}\\]\nThat lead to: \\[\n\\log P_{\\theta}(\\mathrm{x}) = \\underbrace{ \\boxed{\\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}\\left[\\log \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\right] } }_{ ELBO }+ D_{KL}[Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\| P(\\mathrm{z} | \\mathrm{x})]    \n\\tag{13}\\]\nThe KN-Divergence is greater than 0, so, the log-likelihood is greater or equal ELBO. When the variational distribution \\(Q_{\\phi}(\\mathrm{z} | \\mathrm{x})\\) is same as the true distribution \\(P(\\mathrm{z} | \\mathrm{x})\\), the ELBO is equal to the log-likelihood.\nSo, in summary, the ELBO is, which is defined as: \\[\nEBLO =  \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}\\left[\\log \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\right]  =  \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}}[\\log P_{\\theta}(\\mathrm{x} | \\mathrm{z})] - D_{KL}[Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\| P(\\mathrm{z})]\n\\tag{14}\\]\nOne of the most well-known applications of the ELBO in deep learning is the Variational AutoEncoder (Kingma and Welling 2022). The VAE is a generative model that combines probabilistic latent variable modeling with neural networks. It introdce an encoder network to parameterize the variational distribution \\(q_{\\phi}(z \\mid x)\\) and a decoder network to model the likelihood \\(p_{\\theta}(x \\mid z)\\). Training the VAE corresponds to maximizing the ELBO, which balances two objectives: (1) accurately reconstructing the input data from latent codes, and (2) regularizing the latent distribution to remain close to a simple prior (typically Gaussian). This makes VAEs powerful tools for both representation learning and generative modeling. For those who are interested in the implementation of the VAE, and deep dive in to VAE, please to check:\n\nVAE Code\nVAE Blog"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#score-function-langevin-dynamics",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#score-function-langevin-dynamics",
    "title": "All About Diffusion & Flow Models",
    "section": "1.4 Score function & Langevin Dynamics",
    "text": "1.4 Score function & Langevin Dynamics\nThe score function of a probability distribution \\(p(x)\\) is defined as the gradient of its log-density with respect to the variable \\(x\\): \\[\ns(x) = \\nabla_x \\log p(x)\n\\tag{15}\\]\nthe score function points toward regions of higher probability mass. In high-dimensional spaces, where the explicit density \\(p(x)\\) may be intractable to compute, the score function provides a powerful alternative representation: instead of knowing the density itself, we only need to know the direction in which probability increases.\nLangevin dynamics originates from statistical physics and describes the motion of particles subject to both deterministic forces and random noise. In the context of sampling from a distribution p(x), Langevin dynamics provides a stochastic iterative update rule: \\[\nx_{t+1} = x_t + \\frac{\\eta}{2} \\nabla_x \\log p(x_t) + \\sqrt{\\eta}\\varepsilon_t\n\\quad \\varepsilon_t \\sim \\mathcal{N}(0, I)\n\\tag{16}\\]\nHere:\n\n\\(\\eta &gt; 0\\) is the step size\nthe gradient term drives samples toward high-probability regions,\nthe noise term ensures proper exploration of the space.\n\nThis stochastic process converges to the target distribution \\(p(x)\\) under suitable conditions, making it a foundational method for Markov Chain Monte Carlo (MCMC) sampling.\nFor example, the score of the Gaussian Distribution is: \\[\ns(x) = \\nabla_x \\log p(x) = -\\Sigma^{-1}(x - \\mu)\n\\tag{17}\\]\nSo, we can run the langevin dynamics as following: \\[\nx_{t+1} = x_t + \\frac{\\eta_t}{2}\\left(-\\frac{x_t-\\mu}{\\sigma^2}\\right) + \\sqrt{\\eta_t}\\,\\varepsilon_t\n\\tag{18}\\]\ndef langevin_dynamics_update(x, step_size, score):\n    noise = np.random.randn()\n    x = x + (step_size / 2.0) * score + np.sqrt(step_size) * noise\n    return x\nBelow are two plot showing the Langevin Dynamics on 1-d Gaussian Distribution, \n\n\n\n\n\n\nFigure 3: langevin_step_vary\n\n\n\n\n\n\n\n\n\nLangevin Dynamics vs. Gradient Descient\n\n\n\nThe Langevin Dynamics is very similary as the algorithm we used to update the parameters in the neural network, which if defined as: \\[\nx_{t+1} = x_t - \\eta \\,\\nabla f(x_t)\n\\] where \\(\\eta\\) is the learning rate. However, there are several different: - The Graident Descient is Determinsitc while Langevin Dynamics is storchasic becuase of \\(\\sqrt{ \\eta } \\varepsilon_t\\) - Gradient Descent minimizes an explicit function f(x) while Langevin Dynamics simulates a Markov chain whose stationary distribution is p(x) with constant \\(\\eta\\), it generates samples from that distribution."
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#forward-and-backward-diffusion-process",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#forward-and-backward-diffusion-process",
    "title": "All About Diffusion & Flow Models",
    "section": "2.1 Forward and Backward Diffusion Process",
    "text": "2.1 Forward and Backward Diffusion Process\nFor the diffusion process, we gradually add standard normal distribution, until it become the pure gaussian, mathematically, it can be express as:\n\\[\np(\\mathrm{x}_{t} | \\mathrm{x}_{t- 1}) = \\mathcal{N}(\\mathrm{x}_{t}; \\sqrt{ 1 - \\beta_{t} }\\mathrm{x}_{t -1}, \\beta_{t} \\mathbf{I}_{d})\n\\]\nwhere \\(\\{ \\beta_{t} \\in (0, 1) \\}_{t = 1}^{T}\\) and \\(\\beta_{1} \\leq \\beta_{2} \\leq \\dots \\leq \\beta_{T}\\). The \\(\\mathrm{x}_{t}\\) can be expressed as: \\[\n\\mathrm{x}_{t} =  \\sqrt{ 1 - \\beta_{t} }\\mathrm{x}_{t -1} +\\sqrt{ \\beta_{t} } \\epsilon_{t}\n\\]\nThere are many different choice of \\(\\beta\\):\n\nLearned\nConstant\nLinearly or quadratically increased\nFollows a cosine function\n\nOne thing to notice is that \\(\\beta_{t} \\ll 1\\) to make sure that \\(p_{\\theta}(\\mathrm{x}_{t-1} | \\mathrm{x}_{t})\\) can be approximated to the Gaussian Distribution. Look at the expression of the \\(\\mathrm{x}_{t}\\), we can see that it depends on the \\(\\mathrm{x}_{t-1}\\), while \\(\\mathrm{x}_{t -1 }\\) depends on the \\(\\mathrm{x}_{t - 2}\\), and so on, so, the \\(\\mathrm{x}_{t}\\) can also be expressed as:\n\\[\n\\mathrm{x}_{t} = \\sqrt{ \\alpha_{t} }\\mathrm{x}_{0}+ \\sqrt{ 1-\\alpha_{t} } \\epsilon_{t} \\quad \\text{where}\\ \\alpha_{t} = \\prod_{\\tau=1}^{t}(1 - \\beta_{\\tau})\n\\]\nThis is called the forward process,\nAnd the whole forward process format a Markov Chain:\n\\[\np(\\mathrm{x}_{0}, \\mathrm{x}_{1: T}) = p(\\mathrm{x}_{0}) \\prod_{t = 1}^{T}p(\\mathrm{x}_{t} | \\mathrm{x}_{t  -1 })\n\\]\nLet’s quick summary the forward process, and get familiar with following equations:\n\\[\n\\begin{split}\n\\mathrm{x}_{t} & =  \\sqrt{ 1 - \\beta_{t} }\\mathrm{x}_{t -1} +\\sqrt{ \\beta_{t} } \\epsilon_{t} \\\\\n\\mathrm{x}_{t} & = \\sqrt{ \\alpha_{t} }\\mathrm{x}_{0}+ \\sqrt{ 1-\\alpha_{t} } \\epsilon_{t}  \\\\\n\\mathrm{x}_{0} & = \\frac{\\mathrm{x}_{t} - \\sqrt{ 1-\\alpha_{t} } \\epsilon_{t}}{\\sqrt{ \\alpha_{t} }} \\\\\n\\epsilon_t & = \\frac{\\mathrm{x}_t - \\sqrt{\\alpha_t} \\mathrm{x}_0 }{\\sqrt{1 - \\alpha_t}}\n\\end{split}\n\\]\nFrom the last three equation, we can conclude that, as long as we know two of three \\(\\mathrm{x}_{0}, \\mathrm{x}_{t}, \\mathrm{\\epsilon}_{t}\\), we can get other three, this is very useful when we are training the DDPM. \nBackward Process, backward process is from \\(\\mathrm{x}_{t}\\) to \\(\\mathrm{x}_{t -1}\\), it can be expression as: \\[\np(\\mathrm{x}_{t - 1} |  \\mathrm{ x}_{t}) = \\int p(\\mathrm{x}_{t - 1} | \\mathrm{x}_{t}, \\mathrm{x}_{0})p(\\mathrm{x}_{0} | \\mathrm{x}_{t}) d\\mathrm{x}_{0}\n\\]\nThis is intractable if we marginal over \\(\\mathrm{x}_{0}\\). However, if we also conditioned on the \\(\\mathrm{x}_{0}\\), we will get: \\[\np(\\mathrm{x}_{t - 1} | \\mathrm{x}_{t}, \\mathrm{x}_{0}) = \\frac{q(\\mathrm{x}_{t} | \\mathrm{x}_{t - 1}, \\mathrm{x}_{0}) q(\\mathrm{x}_{t - 1} | \\mathrm{x}_{0})}{q(\\mathrm{x}_{t} | \\mathrm{x}_{0})}\n\\]\nWe now that the Markov property of the forward process, we have: \\[\nq(\\mathrm{x}_{t} | \\mathrm{x}_{t - 1}, \\mathrm{x}_{0})  = q(\\mathrm{x}_{t} | \\mathrm{x}_{t - 1})\n\\] which we know, the exact equation is, and we also know what \\(q(\\mathrm{x}_{t} | \\mathrm{x}_{0})\\), so, we know extact what the \\(p(\\mathrm{x}_{t - 1} | \\mathrm{x}_{t}, \\mathrm{x}_{0})\\) is: \\[\n\\begin{split}\np(\\mathrm{x}_{t - 1} | \\mathrm{x}_{t}, \\mathrm{x}_{0})  & = \\mathcal{N}(\\mathrm{x}_{t -1} | \\mu_{t}(\\mathrm{x}_{0}, \\mathrm{x}_{t}), \\sigma_{t}^{2}\\mathbf{I}_{d}) \\\\ \\\\\n\\quad \\text{where}\\   \\mu_{t}(\\mathrm{x}_{0}, \\mathrm{x}_{t}) & = \\frac{(1 - \\alpha_{t - 1})\\sqrt{ 1- \\beta _{t}}\\mathrm{x}_{t} + \\sqrt{ \\alpha_{t - 1} }\\beta_{t}\\mathrm{x}_{0}}{1 -\\alpha_{t}} \\\\\n\\sigma_{t}^{2} & = \\frac{\\beta_{t}( 1- \\alpha_{t - 1})}{1 -\\alpha_{t}}\n\\end{split}\n\\]\nHowever, when we are generating the example, we don’t know what \\(\\mathrm{x}_{0}\\) is. That why we need train a neural network to approximate it.\nNext, derive the loss function we needed to train the neural network."
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#loss-function",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#loss-function",
    "title": "All About Diffusion & Flow Models",
    "section": "2.2 Loss Function",
    "text": "2.2 Loss Function\nLet’s first derive the loss function of the DDPM. DDPM can be view as the hierarchical VAE. So, we can derive the loss function using the ELBO Equation 14. Recall, the log-likelihood with ELBO is defined as following:\n\\[\n\\begin{align}\n\\log P_{\\theta}(\\mathrm{x}) & =  ELBO + D_{KL}[Q_{\\phi}(\\mathrm{z} | \\mathrm{x}) \\| P(\\mathrm{z} | \\mathrm{x})]     \\\\\n& \\geq   \\mathbb{E}_{\\mathrm{z} \\sim Q_{\\phi}(\\mathrm{z} | \\mathrm{x})}\\left[\\log \\frac{P_{\\theta}(\\mathrm{x, z})}{Q_{\\phi}(\\mathrm{z} | \\mathrm{x})} \\right]    \\\\\n& = \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q_{\\phi}(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})}\\left[\\log \\frac{P_{\\theta}(\\mathrm{x}_{0}, \\mathrm{x}_{1:T})}{Q_{\\phi}(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\right]  \n\\end{align}\n\\] where \\(\\mathrm{x}_{1:T}\\) is the latent variable.\nOne thing good about DDPM is that, we know what is the posteriror distribution \\(Q_{\\phi}(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})\\) exactly: \\[\nQ_{\\phi}(\\mathrm{x}_{1:T} | \\mathrm{x}_{0}) = \\prod_{t=1}^{T}P(\\mathrm{x}_{t} | \\mathrm{x}_{t - 1})\n\\]\nSo, the ELBO become: \\[\n\\begin{align}\nEBLO   \n& = \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q_{\\phi}(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})}\\left[\\log \\frac{P_{\\theta}(\\mathrm{x}_{0}, \\mathrm{x}_{1:T})}{Q_{\\phi}(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\right]   \\\\\n& =  \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\frac{P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}}) \\prod_{t = 2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})P(\\mathrm{x}_{T})}\n{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})\\prod_{t=2}^{T}Q(\\mathrm{x}_{t} | \\mathrm{x}_{t - 1})}  \\right] \\\\\n& = \\mathbb{E}_{ Q(\\mathrm{x}_{T} | \\mathrm{x}_{0})} [\\log  P(\\mathrm{x}_{T})] +  \\mathbb{E}_{ Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\frac{\\prod_{t=2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}{\\prod_{t=2}^{T}Q(\\mathrm{x}_{t} | \\mathrm{x}_{t - 1})} \\right] + \\mathbb{E}_{ Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})} \\left[ \\log \\frac{P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x}_{1})}{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})}  \\right] \\\\\n& = \\mathbb{E}_{ Q(\\mathrm{x}_{T} | \\mathrm{x}_{0})} [\\log  P(\\mathrm{x}_{T})]  \n+ \\log  \\sum_{t=2}^{T}\\mathbb{E}_{ Q(\\mathrm{x}_{t-1}, \\mathrm{x_{t}} | \\mathrm{x}_{0})}\\left[ \\log \\frac{P_{\\theta}(\\mathrm{x}_{t-1} | \\mathrm{x}_{t})}{Q(\\mathrm{x}_{t} | \\mathrm{x}_{t-1})}  \\right]\n+ \\mathbb{E}_{ Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})} \\left[ \\log \\frac{P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x}_{1})}{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})}  \\right]\n\\end{align}\n\\]\nAs we can see, to calculate the second term, we need to sample from two random distribution, to get \\(\\mathrm{x_{t}}, \\mathrm{x_{t-1}}\\). This will create very noisy estimate with high variance. So, we need to re-write the ELBO, to make it better low variance, using the Bayesian Rule, we get:\n\\[\n\\begin{align}\nELBO &=   \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\left( P(\\mathrm{x}_{T})\\frac{\\prod_{t = 2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}\n{\\prod_{t=2}^{T}Q(\\mathrm{x}_{t} | \\mathrm{x}_{t - 1})} \\frac{P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}})}{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})} \\right)  \\right]    \\\\\n&=   \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\left( P(\\mathrm{x}_{T})\\frac{\\prod_{t = 2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}\n{\\prod_{t=2}^{T}Q(\\textcolor{green}{\\mathrm{x}_{t} | \\mathrm{x}_{t - 1}, \\mathrm{x}_{0}})} \\frac{P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}})}{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})} \\right)  \\right]  \\\\\n&=   \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\left( P(\\mathrm{x}_{T})\\frac{\\prod_{t = 2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}\n{\\prod_{t=2}^{T}Q(\\textcolor{green}{\\mathrm{x}_{t-1} | \\mathrm{x}_{t}, \\mathrm{x}_{0}})} \\frac{Q(\\mathrm{x}_{t-1} | \\mathrm{x}_{0})}{Q(\\mathrm{x}_{t}|\\mathrm{x}_{0})}\\frac{P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}})}{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})} \\right)  \\right]  \\\\\n&=   \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\left( P(\\mathrm{x}_{T})\\frac{\\prod_{t = 2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}\n{\\prod_{t=2}^{T}Q(\\textcolor{green}{\\mathrm{x}_{t-1} | \\mathrm{x}_{t}, \\mathrm{x}_{0}})} \\frac{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})}{Q(\\mathrm{x}_{T}|\\mathrm{x}_{0})}\\frac{P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}})}{Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})} \\right)  \\right]  \\\\\n&=  \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\left( \\frac{P(\\mathrm{x}_{T})}{Q(\\mathrm{x}_{T}|\\mathrm{x}_{0})} \\frac{\\prod_{t = 2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}\n{\\prod_{t=2}^{T}Q(\\textcolor{green}{\\mathrm{x}_{t-1} | \\mathrm{x}_{t}, \\mathrm{x}_{0}})}P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}}) \\right)  \\right]   \\\\\n&=  \\mathbb{E}_{\\mathrm{x}_{1:T} \\sim Q(\\mathrm{x}_{1:T} | \\mathrm{x}_{0})} \\left[ \\log \\left( \\frac{P(\\mathrm{x}_{T})}{Q(\\mathrm{x}_{T}|\\mathrm{x}_{0})} \\frac{\\prod_{t = 2}^{T}P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}\n{\\prod_{t=2}^{T}Q(\\textcolor{green}{\\mathrm{x}_{t-1} | \\mathrm{x}_{t}, \\mathrm{x}_{0}})}P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}}) \\right)  \\right]   \\\\\n& = \\mathbb{E}_{\\mathrm{x}_{T} \\sim Q(\\mathrm{x}_{T} | \\mathrm{x}_{0})} \\left[\\log \\frac{P(\\mathrm{x}_{T})}{Q(\\mathrm{x}_{T}|\\mathrm{x}_{0})} \\right] + \\sum_{t=2}^{T} \\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} |\\mathrm{x}_{0} )} \\left[\\log \\frac{P_{\\theta}(\\mathrm{x_{t - 1} | \\mathrm{x}_{t}})}\n{Q(\\textcolor{green}{\\mathrm{x}_{t-1} | \\mathrm{x}_{t}, \\mathrm{x}_{0}})} \\right]\n+ \\mathbb{E}_{\\mathrm{x}_{1} \\sim Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})}[P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}}) ]  \\\\\n& = -D_{KL}[Q(\\mathrm{x}_{T} | \\mathrm{x}_{0}) \\| P(\\mathrm{x}_{T})]  \\\\ &\n\\quad - \\sum_{t=2}^{T}\\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} |\\mathrm{x}_{0} )} [D_{KL}[Q(\\mathrm{x}_{t-1} |\\mathrm{x}_{t} \\mathrm{x}_{0}) \\| P_{\\theta}(\\mathrm{x}_{t-1} | \\mathrm{x}_{t})]  ]\\\\&\n\\quad +  \\mathbb{E}_{\\mathrm{x}_{1} \\sim Q(\\mathrm{x}_{1} | \\mathrm{x}_{0})}[P_{\\theta}(\\mathrm{x}_{0} | \\mathrm{x_{1}}) ]\n\\end{align}\n\\]\nThe first term is the prior matching term, which is the constant, no need to optimize. The third term is the reconstruction term, which is the negilibale, because the variance schedule make it almost constant and its learning signal is weak compared to the denoising terms. Now, let’s check the most complex and horriable term, the second term is the consistent term, which the KL Divergence Equation 6 between two gaussian distribution, which has close form: \\[\n\\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} |\\mathrm{x}_{0} )} [D_{KL}[Q(\\mathrm{x}_{t-1} |\\mathrm{x}_{t} \\mathrm{x}_{0}) \\| P_{\\theta}(\\mathrm{x}_{t-1} | \\mathrm{x}_{t})]  ] = \\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} | \\mathrm{x}_{0})} \\left [\\frac{1}{2\\tilde{\\sigma}_{t}^{2}}\\| \\mu_{\\theta}(\\mathrm{x}_{t}, t)  - \\tilde{\\mu}(\\mathrm{x}_{t} | \\mathrm{x}_{0})\\|^{2} \\right]\n\\]\nSince we know that the \\(\\tilde{\\mu}(\\mathrm{x}_{t} | \\mathrm{x}_{0})\\) exact it, we can optimize this by:\n\nSample \\(\\mathrm{x}_{0}\\), \\(t\\)\nSample \\(\\mathrm{x}_{t}\\) from \\(\\mathcal{N}(\\tilde{\\mu}(\\mathrm{x}_{t} | \\mathrm{x}_{0}), \\beta_{t} \\varepsilon_{t})\\)\nPass the \\(\\mathrm{x}_{t}\\) and \\(t\\) to the neural network \\(\\mu_{\\theta}\\)\nCalculate the mean square loss and update the parameters \\(\\theta\\)\nRepeat\n\nLet’s see how can we get \\(\\mathrm{x}_{0}\\) deriectly from the neural network, let rewrite the ELBO:\n\\[\n\\begin{align}\n\\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} |\\mathrm{x}_{0} )} [D_{KL}[Q(\\mathrm{x}_{t-1} |\\mathrm{x}_{t} \\mathrm{x}_{0}) \\| P_{\\theta}(\\mathrm{x}_{t-1} | \\mathrm{x}_{t})]  ] &= \\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} | \\mathrm{x}_{0})} \\left [\\frac{1}{2\\tilde{\\sigma}_{t}^{2}}\\| \\mu_{\\theta}(\\mathrm{x}_{t}, t)  - \\tilde{\\mu}(\\mathrm{x}_{t}, \\mathrm{x}_{0})\\|^{2} \\right] \\\\\n& = \\frac{1}{2 \\tilde{\\sigma}_t^2} \\cdot \n\\frac{\\bar{\\alpha}_{t-1} \\beta_t^2}{(1-\\bar{\\alpha}_t)^2} \n\\mathbb{E}_{\\mathrm{x}_{t}\\sim Q(\\mathrm{x}_t|\\mathrm{x}_0)} \n\\left[ \\| \\hat{x}_\\theta(x_t, t) - x_0 \\|^2 \\right] \\\\\n& =\\omega_t\n\\mathbb{E}_{\\mathrm{x}_{t}\\sim Q(\\mathrm{x}_t|\\mathrm{x}_0)} \n\\left[ \\| \\hat{\\mathrm{x}}_\\theta(\\mathrm{x}_t, t) - \\mathrm{x}_0 \\|^2 \\right]\n\\end{align}\n\\]\nAs we can see, the \\(\\mathrm{x}_{\\theta}\\) can be write as the \\(\\mu_{\\theta}\\) to some constant.\nFinally, let’s get our noise predictor \\(\\varepsilon_t\\)-predictor: \\[\n\\begin{align}\n\\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} |\\mathrm{x}_{0} )} [D_{KL}[Q(\\mathrm{x}_{t-1} |\\mathrm{x}_{t} \\mathrm{x}_{0}) \\| P_{\\theta}(\\mathrm{x}_{t-1} | \\mathrm{x}_{t})]  ] &= \\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_{t} | \\mathrm{x}_{0})} \\left [\\frac{1}{2\\tilde{\\sigma}_{t}^{2}}\\| \\mu_{\\theta}(\\mathrm{x}_{t}, t)  - \\tilde{\\mu}(\\mathrm{x}_{t}, \\mathrm{x}_{0})\\|^{2} \\right] \\\\\n& = \\frac{1}{2 \\tilde{\\sigma}_t^2}\n\\cdot\n\\frac{(1-\\bar{\\alpha}_t)^2}{\\bar{\\alpha}_t (1-\\bar{\\alpha}_t)}\n\\mathbb{E}_{\\mathrm{x}_{t} \\sim Q(\\mathrm{x}_t|\\mathrm{x}_0)}\n\\left[ \\| \\hat{\\varepsilon}_\\theta(x_t, t) - \\varepsilon_t \\|^2 \\right]\n\\\\\n& =\\omega_{t}'\n\\mathbb{E}_{\\mathrm{x}_{t}\\sim Q(\\mathrm{x}_t|\\mathrm{x}_0)} \n\\left[ \\| \\hat{\\varepsilon}_\\theta(x_t, t) - \\varepsilon_t \\|^2 \\right]\n\\end{align}\n\\]\nSummary, from the DDPM, we have derive 3 different predictor:\n\nMean Predictor\n\\(x_{0}\\) Predictor\nNoise Predictor \n\n\nIn practice, we can simply drop the weight term in training: and use noise predictor\nIn this blog, we will first introduce what is the diffusion models, than we will introduce how to implement the DDPM from scratch using PyTorch. After that, we will explore the flow matching and score matching model through the ODE/SDE. By the end of the blog, I believe you will gain a comprehensive understanding of the diffusion model, and SOTA generative models."
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#sampling-from-ddpm",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#sampling-from-ddpm",
    "title": "All About Diffusion & Flow Models",
    "section": "2.3 Sampling from DDPM",
    "text": "2.3 Sampling from DDPM\nDiffusion Model\n\n\nForward Diffusion Process: \\[\nq(\\mathrm{x}_{t} | \\mathrm{x}_{t - 1}) =\\mathcal{N}(\n\\mathrm{x}_{t};\n\\sqrt{ 1 - \\beta_{t} }\\mathrm{x}_{t}, \\beta_{t}\\mathbf{I}\n)\n\\]\n\\[\n\\mathrm{x}_{t} =  \\sqrt{ 1 - \\beta_{t} }\\mathrm{x}_{t} + \\beta_{t}\\epsilon_{t}, \\quad \\text{where} \\ \\epsilon_{t} \\sim \\mathcal{N}(0, \\mathbf{I}_{})\n\\]\n\\[\n\\mathrm{x}_{t} = \\sqrt{ \\bar{\\alpha}_{t} }\\mathrm{x_{0}} +  \\sqrt{ 1 - \\bar{\\alpha}_{t} }\\epsilon\n\\]\n Langevin dynamics: $$ t = {t-1} + {} p({t-1}) + ,_t, _t (0, )\n$$\nBackward Diffusion Process: $$ \\[\\begin{align}\n& p_{\\theta}(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T) \\prod_{t=1}^T p_{\\theta}(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)  \\\\\n\n&  p_{\\theta}(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) = \\mathcal{N}\\!\\left(\\mathbf{x}_{t-1}; \\mu_{\\theta}(\\mathbf{x}_t, t), \\Sigma_{\\theta}(\\mathbf{x}_t, t)\\right)\n\n\\end{align}\\] $$\nThe above content is intractable, one thing to notice that is is tractable when we conditioned on the \\(\\mathrm{x}_{0}\\) \\[\nq(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\n= \\mathcal{N}\\!\\left(\\mathbf{x}_{t-1};\n\\textcolor{blue}{\\tilde{\\mu}(\\mathbf{x}_t, \\mathbf{x}_0)}, \\,\n\\textcolor{red}{\\tilde{\\beta}_t \\mathbf{I}}\\right)\n\\] where : \\[\n\\begin{align}\n\\tilde{\\mu}_t\n& = \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t}\\mathbf{x}_t\n+ \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t}\n   \\frac{1}{\\sqrt{\\alpha_t}} \\left(\\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_t}\\,\\epsilon_t\\right) \\\\\n& = \\textcolor{cyan}{\\frac{1}{\\sqrt{\\alpha_t}}\n   \\left(\\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\,\\epsilon_t\\right)}\n\\end{align}\n\\]\nSo, the loss function become: $$\n\\[\\begin{align}\n\\mathcal{L}_t^{\\text{simple}}\n& = \\mathbb{E}_{t \\sim [1,T], \\mathbf{x}_0, \\epsilon_t}\n  \\left[ \\left\\| \\epsilon_t - \\epsilon_\\theta(\\mathbf{x}_t, t) \\right\\|^2 \\right] \\\\\n& = \\mathbb{E}_{t \\sim [1,T], \\mathbf{x}_0, \\epsilon_t}\n  \\left[ \\left\\| \\epsilon_t - \\epsilon_\\theta\\!\\left(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0\n  + \\sqrt{1 - \\bar{\\alpha}_t}\\,\\epsilon_t,\\, t \\right) \\right\\|^2 \\right]\n\\end{align}\\] $$\nand the loss is: \\[\n\\mathcal{L} = \\mathcal{L}_{t} + C\n\\] where \\(C\\) is some constant not depend on \\(\\theta\\)"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#time-embedding",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#time-embedding",
    "title": "All About Diffusion & Flow Models",
    "section": "2.4 Time Embedding",
    "text": "2.4 Time Embedding\ndef get_timestep_embedding(timesteps, embedding_dim):\n    \"\"\"\n    This matches the implementation in Denoising Diffusion Probabilistic Models:\n    From Fairseq.\n    Build sinusoidal embeddings.\n    This matches the implementation in tensor2tensor, but differs slightly\n    from the description in Section 3.5 of \"Attention Is All You Need\".\n    \"\"\"\n    assert len(timesteps.shape) == 1\n\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#sampling",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#sampling",
    "title": "All About Diffusion & Flow Models",
    "section": "2.5 Sampling",
    "text": "2.5 Sampling\nAfter training a noise denoiser, we can sample from the \\(p_{\\text{init}}\\), and convert it to the \\(p_{\\text{data}}\\)\nThis is relatively simple,"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#classifier-generation",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#classifier-generation",
    "title": "All About Diffusion & Flow Models",
    "section": "4.1 Classifier Generation",
    "text": "4.1 Classifier Generation"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#classifier-free-generation",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#classifier-free-generation",
    "title": "All About Diffusion & Flow Models",
    "section": "4.2 Classifier-Free Generation",
    "text": "4.2 Classifier-Free Generation"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#ddim",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#ddim",
    "title": "All About Diffusion & Flow Models",
    "section": "5.1 DDIM",
    "text": "5.1 DDIM\nDDIM is determinstic"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#progressive-distillation",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#progressive-distillation",
    "title": "All About Diffusion & Flow Models",
    "section": "5.2 Progressive Distillation",
    "text": "5.2 Progressive Distillation\nAs proposed in (ProgressiveDistillationFast2022salimans?)"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#consistency-models",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#consistency-models",
    "title": "All About Diffusion & Flow Models",
    "section": "5.3 Consistency Models",
    "text": "5.3 Consistency Models\nAs proposed in the (Song et al. 2023)"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#latent-diffusion-model",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#latent-diffusion-model",
    "title": "All About Diffusion & Flow Models",
    "section": "5.4 Latent Diffusion Model",
    "text": "5.4 Latent Diffusion Model\nVariance Autoencoder"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#score-matching-1",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#score-matching-1",
    "title": "All About Diffusion & Flow Models",
    "section": "5.5 Score Matching",
    "text": "5.5 Score Matching\n\\[\n\\nabla_{x_t} \\log q(x_t|x_0)\n= \\nabla_x \\left( - \\frac{\\| x_t - \\sqrt{\\bar{\\alpha}_t} x_0 \\|^2}{2(1-\\bar{\\alpha}_t)} \\right)\n= - \\frac{x_t - \\sqrt{\\bar{\\alpha}_t} x_0}{1-\\bar{\\alpha}_t}\n\\]\n$$ _{x_t} q(x_t|x_0) = - = - \n$$\nSo, can be interpreted as predicting the score \\(\\nabla_{x_t} \\log q(x_t|x_0)\\) up to a scaling factor \\(- \\frac{1}{\\sqrt{1-\\bar{\\alpha}_t}}\\)\nAccording to the Tweedie’s formula, we have: \\[\n\\nabla_{x_t} \\log q(x_t)\n= - \\frac{x_t - \\sqrt{\\bar{\\alpha}_t}\\,\\mathbb{E}[x_0 \\mid x_t]}{1-\\bar{\\alpha}_t}\n\\]\n \n\nSo, this is the Noise-Conditional Score-Based Models \n\nSo, the solution is the Annealed Langevin Dynamics  At the beginning (when \\(\\sigma_{t}\\) is large), As time progresses (and \\(\\sigma_{t}\\) decreases),"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#ode-vs.-sde",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#ode-vs.-sde",
    "title": "All About Diffusion & Flow Models",
    "section": "6.1 ODE vs. SDE",
    "text": "6.1 ODE vs. SDE\nBefore talk about the ODE and SDE, let’s first understand some concepts to solid our understanding. DDPM can be viewed as the discreted version of the SDE, and SDE can be viewed as continuous version of DDPM.\n\n6.1.1 Vector Field\nVector Field is a function that assign a vector to every point in space. For example: imagine a weather map, at each location, an arrow shows the wind’s direction and strength. That arrow map is a vector field.\n\n\\[\nF: \\mathbb{R}^{n} \\to \\mathbb{R}^{n}\n\\]\nAnd every ODE \\(u\\) is defined by a vector field, and take in two variable \\(\\mathrm{x}\\) and \\(t\\) \\[\nu: \\mathbb{R}^{d} \\times [0, 1] \\to \\mathbb{R}^{d}, \\quad (x, t) \\to u_{t}(x)\n\\] that for every time \\(t\\) and location \\(\\mathrm{x}\\), we get a vector \\(u_{t}(\\mathrm{x})  \\in \\mathbb{R}^{d}\\) that point to some direction. Image a point in the weather map, \\(x\\) is a point in the map, and \\(u(x)\\) tell \\(x\\), which direction should go next.\n\n\n\n\n\n\n\n\n\n\n\n\nWhy we need \\(t\\) in the ODE?\n\n\n\nBecause for every location \\(\\mathrm{x}\\), we might arrive same location at different time, due to the random start point \\(\\mathrm{x}_{0}\\)\n\n\n\n\n\nlearned_marginals_norm\n\n\n\n\n\nlearned_marginals_norm\n\n\n\n\n\nlearned_marginals_circle\n\n\n\n \\[\n\\begin{align}\n\\frac{d}{dt}\\mathrm{x}_{t } &= u_{t}(\\mathrm{x}_{t}) \\\\\n\\mathrm{x_{0}}&=x_{0}\n\\end{align}\n\\]\nSo, another question we want to ask it: when we start at \\(x_{0}\\), where are we at \\(t\\). This can be solved by flow, which is a solution to the ODE:\n$$\n\\[\\begin{align}\n\\psi : \\mathbb{R}^d \\times [0,1] \\mapsto \\mathbb{R}^d &,\n\\quad (x_0, t) \\mapsto \\psi_t(x_0) \\\\\n\n\\frac{d}{dt} \\psi_t(x_0) & = u_t(\\psi_t(x_0)) \\\\\n\n\\psi_0(x_0)& = x_0\\\\\n\\end{align}\\] $$\n\\[\n\\mathrm{x}_{1} \\sim  p_{\\text{data}}  \n\\] However, we can not solve the problem. But we can use the numerical analysis. One of the simplest and intuitive methods is Euler method:\n\\[\n\\mathrm{x}_{t + h} = \\mathrm{x}_{t} +  h u_{t}(\\mathrm{x}_{t}) \\quad (t = 0, h, 2h, 3h, \\dots,  1- h)\n\\]\nStochastic Differential Equations extend the ODEs with stochastic(random) trajectories, which is also known as stochastic process. The stochastic is add through a Brownian motion. A Brownain motion \\(W = (W_{t})_{0\\leq t \\leq 1}\\) is a stochastic process such that: \\(W_{0} = 0\\): - Normal Increments: \\(W_{t} - W_{s} \\sim \\mathcal{N}(0, (t - s)\\mathbf{I}_{d})\\) for all \\(0 \\leq s \\leq t\\) - Independent Increments\nBrownian Motion is also known as Wiener Process: \\[\nW_{t + h} = W_{t} + \\sqrt{ h }\\epsilon_{t}, \\quad \\text{where} \\ \\epsilon_{t} \\sim \\mathcal{N}(0, \\mathbf{I}_{d})\n\\]\nOrnstein-Unlenbeck(OU) process\nEuler-Maruyama Method is a numerical method.\n\n\n\nforward_panel_with_side_strips_full\n\n\n\n\n\nreverse_panel_with_side_strips_full"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#conditional-vector-field-marginal-vector-field",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#conditional-vector-field-marginal-vector-field",
    "title": "All About Diffusion & Flow Models",
    "section": "6.2 Conditional Vector Field & Marginal Vector Field",
    "text": "6.2 Conditional Vector Field & Marginal Vector Field\nGiven data point \\(\\mathrm{z}\\), we can construct conditional vector field that: \\[\n\\frac{d}{dt}\\mathrm{x}_{t} = u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z})\n\\] where, by following the ODE \\(u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z})\\), the \\(\\mathrm{x}_{t}\\) will end in the data point \\(\\mathrm{z}\\). However, what we actually want is the marginal vector field: \\[\n\\begin{split}\nu_{t}^{\\text{target}}(\\mathrm{x}_{t} )\n&= \\int u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z}) p(\\mathrm{z} | \\mathrm{x}) \\, d\\mathrm{z} \\\\\n&=  \\int u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z}) \\frac{p_{t}(\\mathrm{x}|\\mathrm{z})p_{\\text{data}}(\\mathrm{z})}{p_{t}(\\mathrm{x})} \\, d\\mathrm{z}  \\\\\n\\end{split}\n\\]\nThis is statisfy the property we want. We can derive is through continuity equation.\n\\[\n\\begin{split}\n\\mathcal{L}_{FM}(\\theta) = \\mathbb{E}_{t \\sim [0,1], \\mathrm{z} \\sim p_{data}, \\mathrm{x_{t}} \\sim p_{t}(\\mathrm{x}_{t} | \\mathrm{z})} \\left[\\| u_{t}^{\\theta}(\\mathrm{x}_{t}) - u_{t}^{\\text{target}}(\\mathrm{x}_{t}) \\|^{2} \\right]\n\\end{split}\n\\]\nOne problem of this is that \\(u_{t}^{\\text{target}}(\\mathrm{x}_{t} )\\) is intractable due to the marginal over high-dimensional.\nLet’s rewrite the \\(\\mathcal{L}_{FM}\\), by using the factor \\(\\|a- b \\|^{2} = \\|a\\|^{2} - 2a^{T}b + \\|b\\|^{2}\\):\n\\[\n\\begin{split}\n\\mathcal{L}_{FM}(\\theta)\n&= \\mathbb{E}_{t\\sim[0,1],\\, z\\sim p_{\\rm data},\\, x_t\\sim p_t(x_t|z)}\n\\left[\\|u^\\theta_t(x_t)-u^{\\rm target}_t(x_t)\\|^2\\right] \\\\\n&= \\mathbb{E}_{t \\sim \\text{Unif},\\, z \\sim p_{\\text{data}},\\, x \\sim p_t(\\cdot|z)}\\left[\\|u^\\theta_t(x_t)\\|^2\\right]\n-2\\,\\mathbb{E}_{t \\sim \\text{Unif},\\,  x \\sim p_t(\\cdot|z)}\\left[u^\\theta_t(x_t)^{T} u^{\\rm target}_t(x_t)\\right]\n+\\underbrace{ \\mathbb{E}_{t \\sim \\text{Unif},\\,  x \\sim p_t(\\cdot|z)}\\left[\\|u^{\\rm target}_t(x_t)\\|^2\\right] }_{ C_{1} }\n\\end{split}\n\\]\nAs we can see, the third term is the constant w.r.t to the \\(\\theta\\), let check the second term: \\[\n\\begin{split}\n\\mathbb{E}_{t \\sim \\text{Unif},\\, x \\sim p_t}\n\\!\\left[u_t^\\theta(x)^{T} u_t^{\\text{target}}(x)\\right]\n&\\overset{(i)}{=}\n\\int_0^1 \\!\\!\\int p_t(x)\\, u_t^\\theta(x)^{T} u_t^{\\text{target}}(x)\\, dx\\, dt \\\\\n&\\overset{(ii)}{=}\n\\int_0^1 \\!\\!\\int p_t(x)\\, u_t^\\theta(x)^{T}\n\\left[\\int u_t^{\\text{target}}(x|z)\\,\n\\frac{p_t(x|z)\\,p_{\\text{data}}(z)}{p_t(x)}\\, dz \\right] dx\\, dt \\\\\n&\\overset{(iii)}{=}\n\\int_0^1 \\!\\!\\int\\!\\!\\int u_t^\\theta(x)^{T} u_t^{\\text{target}}(x|z)\\,\np_t(x|z)\\, p_{\\text{data}}(z)\\, dz\\, dx\\, dt \\\\\n&\\overset{(iv)}{=}\n\\mathbb{E}_{t \\sim \\text{Unif},\\, z \\sim p_{\\text{data}},\\, x \\sim p_t(\\cdot|z)}\n\\!\\left[u_t^\\theta(x)^{T} u_t^{\\text{target}}(x|z)\\right]\n\\end{split}\n\\]\nSo, we can get that: \\[\n\\begin{split}\n\\mathcal{L}_{FM}\n& = \\mathbb{E}_{t \\sim \\text{Unif},\\, z \\sim p_{\\text{data}},\\, x \\sim p_t(\\cdot|z)}\\left[\\|u^\\theta_t(x_t)\\|^2\\right]\n-2\\,\\mathbb{E}_{t \\sim \\text{Unif},\\,  x \\sim p_t(\\cdot|z)}\\left[u^\\theta_t(x_t)^{T} u^{\\rm target}_t(x_t)\\right] + C_{1} \\\\\n&=  \\mathbb{E}_{t \\sim \\text{Unif},\\, z \\sim p_{\\text{data}},\\, x \\sim p_t(\\cdot|z)}\\left[\\|u^\\theta_t(x_t)\\|^2\\right]  - 2 \\mathbb{E}_{t \\sim \\text{Unif},\\, z \\sim p_{\\text{data}},\\, x \\sim p_t(\\cdot|z)}\n\\left[u_t^\\theta(x)^{T} u_t^{\\text{target}}(x|z)\\right] + C_{1}\\\\\n&=  \\mathbb{E}_{t \\sim \\text{Unif},\\, z \\sim p_{\\text{data}},\\, x \\sim p_t(\\cdot|z)}\\left[\\|u^\\theta_t(x_t)\\|^2 - 2u_t^\\theta(x)^{T} u_t^{\\text{target}}(x|z)\\right]  + C_{1} \\\\\n&=  \\mathbb{E}_{t \\sim \\text{Unif},\\, z \\sim p_{\\text{data}},\\, x \\sim p_t(\\cdot|z)}\\left[\\|u^\\theta_t(x_t)\\|^2 - 2u_t^\\theta(x)^{T} u_t^{\\text{target}}(x|z) + \\|u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z})\\|^{2}  - \\|u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z}) \\|^{2} \\right]  + C_{1} \\\\\n&=  \\mathbb{E}_{t \\sim \\text{Unif},\\, z \\sim p_{\\text{data}},\\, x \\sim p_t(\\cdot|z)}\\left[\\|u^\\theta_t(x_t)-u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z})\\|^2  - \\|u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z}) \\|^{2} \\right]  + C_{1} \\\\\n&=  \\mathbb{E}_{t \\sim \\text{Unif},\\, z \\sim p_{\\text{data}},\\, x \\sim p_t(\\cdot|z)}\\left[\\|u^\\theta_t(x_t)-u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z})\\|^2   \\right] \\underbrace{ -\\mathbb{E} \\left[\\|u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z}) \\|^{2} \\right] }_{ C_{2} } + C_{1} \\\\\n&=  \\mathcal{L}_{CFM}(\\theta) + C_{2} + C_{1} \\\\\n\\end{split}\n\\]\nAs we can see the \\(\\mathcal{L}_{CFM}\\) is the \\(\\mathcal{L}_{FM}\\) to some constant \\(C\\). So, we can just minimizing \\(\\mathcal{L}_{CFM}\\), we will get the minizer value of \\(\\mathcal{L}_{FM}\\) as well. \\[\n\\mathcal{L}_{CFM} = \\mathbb{E}_{t \\sim \\text{Unif},\\, z \\sim p_{\\text{data}},\\, x \\sim p_t(\\cdot|z)}\\left[\\|u^\\theta_t(x_t)-u_{t}^{\\text{target}}(\\mathrm{x}_{t} | \\mathrm{z})\\|^2   \\right]\n\\]\nFrom there, we get the Flow Matching. This is just a simple regression problem with respect to the vector field. Now, let’s see how to derive the score matching from the SDE ## Conditional Score Function & Marginal Score Function\n\n\n\n\nsamples_evolution\n\n\n\n\n\ndensity_evolution"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#mean-flow",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#mean-flow",
    "title": "All About Diffusion & Flow Models",
    "section": "6.3 Mean Flow",
    "text": "6.3 Mean Flow\nMean Flows for One-step Generative Modeling\nMMDiT"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#u-net",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#u-net",
    "title": "All About Diffusion & Flow Models",
    "section": "7.1 U-Net",
    "text": "7.1 U-Net\nU-Net: Convolutional Networks for Biomedical Image Segmentation\n\n\n\nSummarize the most important point of this section in 1–2 sentences.\n\nKeep it concise and action-oriented so readers walk away with clarity.\n\n\n\n\nPose a reflective or guiding question to the reader.\n\nExample: How would this method scale if we doubled the dataset size?"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#control-net",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#control-net",
    "title": "All About Diffusion & Flow Models",
    "section": "7.2 Control Net",
    "text": "7.2 Control Net\nAdding Conditional Control to Text-to-Image Diffusion Models"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#diffusion-transformer-dit",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#diffusion-transformer-dit",
    "title": "All About Diffusion & Flow Models",
    "section": "7.3 Diffusion Transformer (DiT)",
    "text": "7.3 Diffusion Transformer (DiT)"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#text-image-generation",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#text-image-generation",
    "title": "All About Diffusion & Flow Models",
    "section": "8.1 Text-Image Generation",
    "text": "8.1 Text-Image Generation\n\n8.1.1 Imagen\nPhotorealistic Text-to-Image Diffusion Models with Deep Language Understanding\n\n\n8.1.2 DALL·E\n\n\n8.1.3 Stable Diffusion"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#text-video-generation",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#text-video-generation",
    "title": "All About Diffusion & Flow Models",
    "section": "8.2 Text-Video Generation",
    "text": "8.2 Text-Video Generation\n\n8.2.1 Meta Movie Gen Video\nMovie Gen: A Cast of Media Foundation Models\n\n\n8.2.2 Veo"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#language-modeling",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#language-modeling",
    "title": "All About Diffusion & Flow Models",
    "section": "8.3 Language Modeling",
    "text": "8.3 Language Modeling"
  },
  {
    "objectID": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#diffusion-policy",
    "href": "posts/Blogs/Diffusion-Models/Diffusion-Model.html#diffusion-policy",
    "title": "All About Diffusion & Flow Models",
    "section": "8.4 Diffusion Policy",
    "text": "8.4 Diffusion Policy\nRectified Flow: Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow https://arxiv.org/pdf/2209.03003\nMean Flow Mean Flows for One-step Generative Modeling https://arxiv.org/pdf/2505.13447"
  },
  {
    "objectID": "posts/Blogs/LLM-Alignment/post.html",
    "href": "posts/Blogs/LLM-Alignment/post.html",
    "title": "LLM Part3: Alignment",
    "section": "",
    "text": "1 Supervised Fine-Tuning (SFT)\n  2 Review of Reinforcement Learning\n  3 Format LLM Alignment as a Reinforcement Learning Problem\n  4 RLHF Algorithms\n  \n  4.1 Proximal Policy Optimization (PPO)\n  4.2 Direct Preference Optimization (DPO)\n  4.3 Group Relative Policy Optimization (GRPO)\n  4.4 Group Sequence Policy Optimization(GSPO)\n  4.5 Soft Adaptive Policy Optimization(SAPO)\n  \n  5 Conclusion\nIn the part 1 and part 2 of the LLM series, we covered the architecture and inference techniques for LLMs. In this part 3, we will focus on alignment techniques, which are crucial for ensuring that LLMs behave in ways that are consistent with human values and intentions. We will explore various methods for aligning LLMs, including reinforcement learning from human feedback (RLHF), and discuss their implications for the development and deployment of these models. We will first explore the simple Supervised Fine-Tuning (SFT) approach, which involves fine-tuning LLMs on curated datasets that reflect human values and preferences. Than we will explore different RLHF techniques, which involve training LLMs using feedback from human evaluators to improve their alignment with human intentions. We will explore algorithms from PPO, DPO to GRPO and"
  },
  {
    "objectID": "posts/Blogs/LLM-Alignment/post.html#proximal-policy-optimization-ppo",
    "href": "posts/Blogs/LLM-Alignment/post.html#proximal-policy-optimization-ppo",
    "title": "LLM Part3: Alignment",
    "section": "4.1 Proximal Policy Optimization (PPO)",
    "text": "4.1 Proximal Policy Optimization (PPO)"
  },
  {
    "objectID": "posts/Blogs/LLM-Alignment/post.html#direct-preference-optimization-dpo",
    "href": "posts/Blogs/LLM-Alignment/post.html#direct-preference-optimization-dpo",
    "title": "LLM Part3: Alignment",
    "section": "4.2 Direct Preference Optimization (DPO)",
    "text": "4.2 Direct Preference Optimization (DPO)"
  },
  {
    "objectID": "posts/Blogs/LLM-Alignment/post.html#group-relative-policy-optimization-grpo",
    "href": "posts/Blogs/LLM-Alignment/post.html#group-relative-policy-optimization-grpo",
    "title": "LLM Part3: Alignment",
    "section": "4.3 Group Relative Policy Optimization (GRPO)",
    "text": "4.3 Group Relative Policy Optimization (GRPO)\nhttps://arxiv.org/abs/2402.03300"
  },
  {
    "objectID": "posts/Blogs/LLM-Alignment/post.html#group-sequence-policy-optimizationgspo",
    "href": "posts/Blogs/LLM-Alignment/post.html#group-sequence-policy-optimizationgspo",
    "title": "LLM Part3: Alignment",
    "section": "4.4 Group Sequence Policy Optimization(GSPO)",
    "text": "4.4 Group Sequence Policy Optimization(GSPO)\nhttps://arxiv.org/abs/2507.18071"
  },
  {
    "objectID": "posts/Blogs/LLM-Alignment/post.html#soft-adaptive-policy-optimizationsapo",
    "href": "posts/Blogs/LLM-Alignment/post.html#soft-adaptive-policy-optimizationsapo",
    "title": "LLM Part3: Alignment",
    "section": "4.5 Soft Adaptive Policy Optimization(SAPO)",
    "text": "4.5 Soft Adaptive Policy Optimization(SAPO)\nUsed in the Qwen3-VL (Bai et al. 2025) model."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html",
    "title": "LLM Part1: Architecture",
    "section": "",
    "text": "1 The overview of transformer model\n  2 Position Encoding\n  \n  2.1 Learned Position Encoding\n  2.2 Absolute Position Encoding\n  2.3 Relative Position Encoding\n  2.4 RoPE (Rotary Position Embedding)\n  2.5 ALIBI\n  2.6 Extend to longer context\n  \n  2.6.1 Linear Position Interpolation\n  2.6.2 NTK-Aware Position Interpolation\n  2.6.3 YaRN\n  \n  \n  3 Normalization\n  \n  3.1 Layer Normalization vs. RMS Normalization\n  3.2 Pre-Layer Normalization vs. Post-Layer Normalization\n  3.3 QK Norm\n  \n  4 Attention Mechanism\n  \n  4.1 Multi Headed Attention\n  \n  4.1.1 Time Complexity of Scaled Dot-Product Attention\n  \n  4.2 Grouped Query Attention / Multi Query Attention\n  4.3 Sparse Attention\n  \n  4.3.1 Sliding window attention\n  4.3.2 Dilated Attention\n  \n  4.4 Multi Latent Attention\n  4.5 Flash Attention\n  \n  4.5.1 Flash Attention V1 vs. V2 vs. V3\n  \n  4.6 Native Sparse Attention\n  4.7 Attention Sink\n  \n  5 Activations\n  \n  5.1 Swish\n  5.2 Gated Linear Unit (GLU)\n  \n  6 Feed Forward Network & Mixture of Experts\n  \n  6.1 Multi Layer Perceptron (MLP)\n  6.2 Gated Linear Unit (GLU)\n  6.3 Mixture of Experts (MoE)\n  \n  7 Model Initialization\n  \n  7.1 Weight Initialization\n  7.2 Layer Initialization\n  \n  8 Case Study\n  \n  8.1 LLaMA\n  8.2 Qwen\n  8.3 DeepSeek\n  8.4 GPT-Oss\n  \n  9 Other Architectures\n  \n  9.1 Diffusion Language Models\n  9.2 State Space Model (SSM)\n  \n  10 Conclusion\nThis is the part 1 of the LLM series, architecture. In this blog, we will going through different transformer architectures that power LLMs. Such as position encoding, attention mechanisms, and more. In the end, we will explore some SOTA LLM architectures, and study how to they combine those techniques to achieve better performance. We are going from bottom to up:\nBesides that, we will also explore different normalization techniques, such as Layer Normalization and RMS Normalization, and the different position of the normalization layers within the architecture. Then, we will going through the training process and how to effectively train these architectures."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#learned-position-encoding",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#learned-position-encoding",
    "title": "LLM Part1: Architecture",
    "section": "2.1 Learned Position Encoding",
    "text": "2.1 Learned Position Encoding\nIn the absolute position encoding, we put our position information into fixed sinusoidal functions. It is hand-crafted for specific tasks and does not adapt to the data. So, is it possible to learn position encodings from the data itself? It is possible, and this leads us to the concept of learned position encodings. The learned position encodings are typically implemented as additional trainable parameters in the model. Instead of using fixed sinusoidal functions, the model learns to generate position embeddings that are optimized for the specific task and dataset.\nThis method is used in such as Vision Transformers (Dosovitskiy et al. 2021)."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#absolute-position-encoding",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#absolute-position-encoding",
    "title": "LLM Part1: Architecture",
    "section": "2.2 Absolute Position Encoding",
    "text": "2.2 Absolute Position Encoding\nAs used in the (Vaswani et al. 2023), absolute position encoding assigns a unique position embedding to each position in the input sequence, regardless of the content of the tokens. One common approach is to use a fixed sinusoidal function to generate the position embeddings. For example: for each position \\(pos\\), the position embedding \\(PE(pos)\\) can be defined as:\n\\[\n\\begin{aligned}\n\\text{PE}(pos, 2i) &= \\sin\\!\\left(pos \\times \\frac{1}{10,000^{2i/d_{\\text{model}}}}\\right) \\\\\n\\text{PE}(pos, 2i+1) &= \\cos\\!\\left(pos \\times \\frac{1}{10,000^{2i/d_{\\text{model}}}}\\right)\n\\end{aligned}\n\\tag{1}\\]\nwhere\n\n\\(d_{\\text{model}}\\) is the dimensionality of the embeddings.\n\\(i\\) is the index of the embedding dimension. The sin function is applied to the even indices \\(2i\\), while the cos function is applied to the odd indices \\(2i+1\\).\n\nWe can illustrate the encoding as following:\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Display of the position with different sequence lengths\n\n\n\n\n\n\n\n\n\n\n\n(b) Display of the position with different d_model\n\n\n\n\n\n\n\nFigure 2: Illustration of Absolute Position Encoding\n\n\n\n\nThere are several properties can be read from the Figure 2 and Equation 1:\n\nPeriodicity: The sine and cosine functions used in the position encoding have a periodic nature, which allows the model to easily learn to attend to relative positions. This is evident in Figure 2 (a), where the position encodings exhibit similar patterns for different sequence lengths.\nDimensionality: The choice of \\(d_{\\text{model}}\\) affects the granularity of the position encodings. As shown in Figure 2 (b), increasing the dimensionality results in more fine-grained position encodings, which can help the model capture more subtle positional information.\nThe low-dimension part of the dmodel is change more frequently than the high-dimension part, allowing the model to adapt more easily to different input lengths.\n\n\nWe chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset \\(k\\), \\(PE_{pos+k}\\) can be represented as a linear function of \\(PE_{pos}\\).  Attention Is All You Need \n\nHow to understand this sentence. Let’s first redefine the position encoding. \\[\n\\begin{aligned}\n\\mathrm{PE}(pos,2i) &= \\sin\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big), \\\\\n\\mathrm{PE}(pos,2i+1) &= \\cos\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big).\n\\end{aligned}\n\\] where \\(\\alpha_i = 10,000^{2i/d_{\\text{model}}}\\). And we consider \\((2i, 2i+1)\\) as one pair. Now, consider the same pair at position \\(pos + k\\). We can write the position encoding as:\n\\[\n\\begin{align}\n\\mathrm{PE}(pos+k,2i)\n&= \\sin\\!\\Big(\\tfrac{pos+k}{\\alpha_i}\\Big)\n    = \\sin\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big)\\cos\\!\\Big(\\tfrac{k}{\\alpha_i}\\Big)\n    + \\cos\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big)\\sin\\!\\Big(\\tfrac{k}{\\alpha_i}\\Big) \\\\\n\\mathrm{PE}(pos+k,2i+1)\n&= \\cos\\!\\Big(\\tfrac{pos+k}{\\alpha_i}\\Big)\n    =\\cos\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big)\\cos\\!\\Big(\\tfrac{k}{\\alpha_i}\\Big)\n    - \\sin\\!\\Big(\\tfrac{pos}{\\alpha_i}\\Big)\\sin\\!\\Big(\\tfrac{k}{\\alpha_i}\\Big)\n\\end{align}\n\\tag{2}\\]\n\n\nAngle addition formulas: \\[\n\\begin{align*}\n&\\sin(a+b) = \\sin(a)\\cos(b) + \\cos(a)\\sin(b) \\\\\n&\\cos(a+b) = \\cos(a)\\cos(b) - \\sin(a)\\sin(b)\n\\end{align*}\n\\]\nWrite this as vector form:\n\\[\n\\mathbf{p}_{pos}^{(i)} =\n\\begin{bmatrix}\n\\sin(pos/\\alpha_i) \\\\\n\\cos(pos/\\alpha_i)\n\\end{bmatrix}\n\\]\nThen \\(\\mathbf{p}_{pos+k}^{(i)}\\) equal to: \\[\n\\mathbf{p}_{pos+k}^{(i)} =\n\\underbrace{\n\\begin{bmatrix}\n\\cos(\\tfrac{k}{\\alpha_i}) & \\ \\ \\sin(\\tfrac{k}{\\alpha_i}) \\\\\n-\\sin(\\tfrac{k}{\\alpha_i}) & \\ \\ \\cos(\\tfrac{k}{\\alpha_i})\n\\end{bmatrix}\n}_{\\displaystyle R_i(k)}\n\\ \\mathbf{p}_{pos}^{(i)}\n\\]\nNotice that \\(R_i(k)\\) is known as rotation matrix which only depends on the relative position \\(k\\) and not on the absolute position \\(pos\\). This is the key insight that allows the model to generalize to different positions.\nStacking all pairs, \\[\n\\mathrm{PE}(pos+k) =\n\\underbrace{\n    \\begin{pmatrix}\n\\cos\\!\\big(\\tfrac{k}{\\alpha_1}\\big) & \\sin\\!\\big(\\tfrac{k}{\\alpha_1}\\big) & 0 & 0 & \\cdots & 0 & 0 \\\\\n-\\sin\\!\\big(\\tfrac{k}{\\alpha_1}\\big) & \\ \\cos\\!\\big(\\tfrac{k}{\\alpha_1}\\big) & 0 & 0 & \\cdots & 0 & 0 \\\\\n0 & 0 & \\cos\\!\\big(\\tfrac{k}{\\alpha_2}\\big) &\\sin\\!\\big(\\tfrac{k}{\\alpha_2}\\big) & \\cdots & 0 & 0 \\\\\n0 & 0 &  -\\sin\\!\\big(\\tfrac{k}{\\alpha_2}\\big) & \\ \\cos\\!\\big(\\tfrac{k}{\\alpha_2}\\big) & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & 0 & \\cdots & \\cos\\!\\big(\\tfrac{k}{\\alpha_{d/2}}\\big) & \\sin\\!\\big(\\tfrac{k}{\\alpha_{d/2}}\\big) \\\\\n0 & 0 & 0 & 0 & \\cdots & -\\sin\\!\\big(\\tfrac{k}{\\alpha_{d/2}}\\big) & \\ \\cos\\!\\big(\\tfrac{k}{\\alpha_{d/2}}\\big)\n\\end{pmatrix}\n}_{R(k)}\n\\cdot\n\\underbrace{\n    \\begin{pmatrix}\n\\sin(\\tfrac{k}{\\alpha_1}) \\\\\n\\cos(\\tfrac{k}{\\alpha_1}) \\\\\n\\sin(\\tfrac{k}{\\alpha_2}) \\\\\n\\cos(\\tfrac{k}{\\alpha_2}) \\\\\n\\vdots \\\\\n\\sin(\\tfrac{k}{\\alpha_{d/2}}) \\\\\n\\cos(\\tfrac{k}{\\alpha_{d/2}})\n\\end{pmatrix}\n}_{\\mathrm{PE}(pos)}\n\\]\nwhere \\(R(k)\\) is block-diagonal with those \\(2\\times2\\) rotations, \\(R(k)\\) depends on \\(k\\) but not on \\(pos\\) → a linear map of \\(\\mathrm{PE}(pos)\\)."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#relative-position-encoding",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#relative-position-encoding",
    "title": "LLM Part1: Architecture",
    "section": "2.3 Relative Position Encoding",
    "text": "2.3 Relative Position Encoding\nRelative Position Encoding, first proposed in Transformer-XL (Dai et al. 2019), then adaptive in different language model.\n\\[\nA_{i,j} =\n\\underbrace{Q_i^\\top K_j}_{\\text{content-based addressing}}\n+\n\\underbrace{Q_i^\\top R_{i-j}}_{\\text{content-dependent positional bias}}\n+\n\\underbrace{u^\\top K_j}_{\\text{global content bias}}\n+\n\\underbrace{v^\\top R_{i-j}}_{\\text{global positional bias}}\n\\]\nwhere:\n\n\\(Q_i \\in \\mathbb{R}^d\\): query vector at position \\(i\\)\n\n\\(K_j \\in \\mathbb{R}^d\\): key vector at position \\(j\\)\n\n\\(R_{i-j} \\in \\mathbb{R}^d\\): embedding of the relative distance \\((i-j)\\)\n\n\\(u, v \\in \\mathbb{R}^d\\): learnable global bias vectors\n\n\\(A_{i,j}\\): unnormalized attention score between position \\(i\\) and \\(j\\)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass RelPositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        self.d_model = d_model\n\n        # relative positions: range [-max_len, max_len]\n        range_len = max_len * 2 + 1\n        self.rel_emb = nn.Embedding(range_len, d_model)\n\n        # trainable biases u, v (Transformer-XL)\n        self.u = nn.Parameter(torch.Tensor(d_model))\n        self.v = nn.Parameter(torch.Tensor(d_model))\n\n    def forward(self, q, k, seq_len):\n        B, H, L, Dh = q.size()\n\n        # (L, L): relative position indices\n        pos_idx = torch.arange(L, dtype=torch.long, device=q.device)\n        rel_idx = pos_idx[None, :] - pos_idx[:, None]  # i-j\n        rel_idx = rel_idx + seq_len  # shift to [0, 2*max_len]\n        rel_pos_emb = self.rel_emb(rel_idx)  # (L, L, d_model)\n\n        # compute QK^T (content-based)\n        content_score = torch.matmul(q, k.transpose(-2, -1))  # (B, H, L, L)\n\n        # project queries with R\n        rel_q = q + self.v.view(1, 1, 1, -1)  # add bias v\n        rel_score = torch.einsum('bhld,lrd-&gt;bhlr', rel_q, rel_pos_emb)\n\n        # add global content bias (u)\n        content_bias = torch.einsum('d,bhjd-&gt;bhj', self.u, k).unsqueeze(2)\n\n        # total score\n        logits = content_score + rel_score + content_bias\n        return logits / (Dh ** 0.5)  # scale as in attention"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#rope-rotary-position-embedding",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#rope-rotary-position-embedding",
    "title": "LLM Part1: Architecture",
    "section": "2.4 RoPE (Rotary Position Embedding)",
    "text": "2.4 RoPE (Rotary Position Embedding)\nSo far we have see the absolute position encoding and relative position encoding. However, there is an problem with absolute position encoding. For example, for two sentence:\n\nEvery Day I will go to gym\nI will go to gym every day\n\nThe absolute position encoding is totally different from two sentences, even though they have the same words and means. On the other hand, the problem of the relative position encoding is that it does not capture the absolute position information, which is crucial for understanding the meaning of the sentences for some task such as text summarization.\nRoPE (Su et al. 2023) is a method combine those two. The vector rotated certain degree according to the absolute position in the sentence. On the other hand, it relative position information is preserved. According to Equation 2, the relative position is not related to the position.\n\n\n\n\n\n\nFigure 3: Illustration of RoPE\n\n\n\n\\[\nR_{\\Theta,m}^{d} \\mathbf{x}\n=\n\\begin{pmatrix}\nx_1\\\\\nx_2\\\\\nx_3\\\\\nx_4\\\\\n\\vdots\\\\\nx_{d-1}\\\\\nx_d\n\\end{pmatrix}\n\\otimes\n\\begin{pmatrix}\n\\cos(m\\theta_{1})\\\\\n\\cos(m\\theta_{1})\\\\\n\\cos(m\\theta_{2})\\\\\n\\cos(m\\theta_{2})\\\\\n\\vdots\\\\\n\\cos\\!\\big(m\\theta_{d/2}\\big)\\\\\n\\cos\\!\\big(m\\theta_{d/2}\\big)\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n- x_2\\\\\nx_1\\\\\n- x_4\\\\\nx_3\\\\\n\\vdots\\\\\n- x_d\\\\\nx_{d-1}\n\\end{pmatrix}\n\\otimes\n\\begin{pmatrix}\n\\sin(m\\theta_{1})\\\\\n\\sin(m\\theta_{1})\\\\\n\\sin(m\\theta_{2})\\\\\n\\sin(m\\theta_{2})\\\\\n\\vdots\\\\\n\\sin\\!\\big(m\\theta_{d/2}\\big)\\\\\n\\sin\\!\\big(m\\theta_{d/2}\\big)\n\\end{pmatrix}\n\\tag{3}\\]\nwhere\n\\[\n\\theta_{i,d} = \\frac{1}{10,000^{2(i - 1) / d}} ,\\quad i \\in [1, 2, \\dots, d / 2 ]\n\\]\nAs we can see, to implement the RoPE in code, we can:\n\nConstruct cos and sin matrices for the given input dimensions and maximum position.\nApply the rotation to the input embeddings using the constructed matrices.\n\n\n\n\n\n\n\nNote\n\n\n\nOne thing always bother me about this implementation is that the rotate_half function actually swap pair of the last dimension as mentioned in the paper. For example:\n&gt;&gt;&gt; x = torch.arange(0, 24).reshape(3, 8) # (B, D)\n&gt;&gt;&gt; x\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [16, 17, 18, 19, 20, 21, 22, 23]])\n&gt;&gt;&gt; x1 = rotate_half(x)\n&gt;&gt;&gt; x1\ntensor([[ -4,  -5,  -6,  -7,   0,   1,   2,   3],\n        [-12, -13, -14, -15,   8,   9,  10,  11],\n        [-20, -21, -22, -23,  16,  17,  18,  19]])\nThe above function just change the x to [-x_{d//2}, ..., -x_{d}, x_0, ..., x_{d//2-1}]\nCheck this linkif you are interested.\nIf this really bother you, you can just implement like this:\ndef rotate_half_v2(x):\n    # Assume x is (B, D)\n    x = x.reshape(x.shape[0], -1, 2)\n    x1, x2 = x.unbind(dim = -1)\n    x = torch.stack((-x2, x1), dim = -1)\n    return x.reshape(x.shape[0], -1)\nwhich is same as they mentioned in the paper.\n&gt;&gt;&gt; x\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [16, 17, 18, 19, 20, 21, 22, 23]])\n&gt;&gt;&gt; x2 = rotate_half_v2(x)\n&gt;&gt;&gt; x2\ntensor([[ -1,   0,  -3,   2,  -5,   4,  -7,   6],\n        [ -9,   8, -11,  10, -13,  12, -15,  14],\n        [-17,  16, -19,  18, -21,  20, -23,  22]])\nOr\ndef rotate_half_v3(x: torch.Tensor) -&gt; torch.Tensor:\n    y = torch.empty_like(x)\n    y[..., ::2] = -x[..., 1::2]  # even positions get -odd\n    y[..., 1::2] =  x[..., ::2]  # odd positions get even\n    return y\n\n&gt;&gt;&gt; x3 = rotate_half_v3(x)\n&gt;&gt;&gt; x3\ntensor([[ -1,   0,  -3,   2,  -5,   4,  -7,   6],\n        [ -9,   8, -11,  10, -13,  12, -15,  14],\n        [-17,  16, -19,  18, -21,  20, -23,  22]])\n\n\nThe other implementation of the RoPE is reply on the complex number. We can treat 2D vector \\((x, y)\\) as a complex number \\(z = x + iy\\), and the rotation can be done by multiplying with a complex exponential:\n\\[\nz' = z \\cdot e^{i\\theta} = (x + iy) \\cdot (\\cos \\theta + i \\sin \\theta) = (x \\cos \\theta - y \\sin \\theta) + i (x \\sin \\theta + y \\cos \\theta)\n\\]\nCode adapted from LLaMA model"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#alibi",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#alibi",
    "title": "LLM Part1: Architecture",
    "section": "2.5 ALIBI",
    "text": "2.5 ALIBI\n(Press, Smith, and Lewis 2022)\n\n\n\n\n\n\nFigure 4\n\n\n\nAliBi: simple, monotonic bias → strong extrapolation, lower overhead.\nSo far, for the position embedding, we modify the Q, K to add the position information for attention to calculate. However, is it possible to directly modify the attention score? To notify them the position information? This is exactly what ALIBI does. The ALIBI (Attention with Linear Biases) method introduces a linear bias to the attention scores based on the distance between tokens. The bias is added directly to the attention score before applying the softmax function. Mathematically:\n\\[\n\\operatorname{softmax}\\!\\Big( q_i k_j^\\top \\;+\\; m \\cdot (-(i-j)) \\Big)\n\\]\nwhere:\n\n\\(q_i \\in \\mathbb{R}^d\\): query vector at position \\(i\\)\n\\(k_j \\in \\mathbb{R}^d\\): key vector at position \\(j\\)\n\\(m \\in \\mathbb{R}\\): slop (head-dependent constant)\n\\((i - j) \\geq 0\\): relative distance\n\nFor example, for query \\(i\\), the logits against all keys \\([0, 1, \\dots, i ]\\) become: \\[\n\\ell_i \\;=\\;\n\\Big[\\, q_i k_0^\\top - m(i-0),\\;\n       q_i k_1^\\top - m(i-1),\\;\n       \\ldots,\\;\n       q_i k_{i-1}^\\top - m(1),\\;\n       q_i k_i^\\top \\,\\Big]\n\\]"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#extend-to-longer-context",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#extend-to-longer-context",
    "title": "LLM Part1: Architecture",
    "section": "2.6 Extend to longer context",
    "text": "2.6 Extend to longer context\nSo far, for all the position encoding we discussed, it has one main drawback: it fixed maximum context length during training.\n\n\n\n\n\n\nFigure 5: (Image Source Video: Long-Context LLM Extension)\n\n\n\nWhen we are training on the fixed context length, the model learns to attend to the positions within that context. However, during inference, we may want to extend the context length beyond what the model was trained on. This is where the challenge lies. One way is to train a longer context length model from the beginning. However this requires more computational resources and may not be feasible for all applications. So, we need to find a way to adapt the model to longer contexts without retraining it from scratch.\nThere are several approaches to address this issue. Let’s discuss a few of them.\n\n2.6.1 Linear Position Interpolation\n\n\n\n\n\n\nFigure 6\n\n\n\ndecreases the frequencies of the basis functions so that more tokens fit within each period. The position interpolation (Chen et al. 2023) mentioned that we can just linearly interpolate the position embeddings for the extended context. This allows the model to generate position embeddings for longer sequences without requiring additional training. It just rescale the \\(m\\) base in the RoPE by: \\[\nf'(\\mathbf{x}, m) = f(\\mathbf{x}, m\\frac{L}{L'})\n\\] where \\(L\\) is the original context length and \\(L'\\) is the new context length.\n\n\n2.6.2 NTK-Aware Position Interpolation\nThe Linear Position Interpolation, if it was possible to pick the correct scale parameter dynamically based on the sequence length rather than having to settle for the fixed tradeoff of maximum sequence length vs. performance on shorter sequences. The NTK-Aware Position Interpolation method leverages the Neural Tangent Kernel (NTK) framework to adaptively adjust the position embeddings during inference. By analyzing the model’s behavior in the NTK regime, we can identify the optimal scaling factors for different sequence lengths, allowing for more effective extrapolation of position information.\n\\[\n\\alpha^{\\text{NTK-RoPE}}_{j} = \\kappa^{-\\frac{2j}{d_k}}\n\\]\n\n\n\n\n\n\nNeural Tangent Kernel (NTK)\n\n\n\n\n\n\n\n\n2.6.3 YaRN\nanother RoPE extension method, uses “NTK-by-parts” interpolation strategies across different dimensions of the embedding space and introduces a temperature factor to adjust the attention distribution for long inputs. But RoPE cannot extrapolate well to sequences longer than training (e.g., a model trained on 2K tokens struggles at 8K).\n\\[\n\\alpha^{\\mathrm{YaRN}}_{j}\n= \\frac{(1-\\gamma_j)\\,\\tfrac{1}{t} + \\gamma_j}{\\sqrt{T}} \\,.\n\\]\nYaRN modifies RoPE to support longer context windows while preserving model stability. (Peng et al. 2023)"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#layer-normalization-vs.-rms-normalization",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#layer-normalization-vs.-rms-normalization",
    "title": "LLM Part1: Architecture",
    "section": "3.1 Layer Normalization vs. RMS Normalization",
    "text": "3.1 Layer Normalization vs. RMS Normalization\nThe Layer Normalization (Ba, Kiros, and Hinton 2016) is a technique to normalize the inputs across the features for each training example. It is defined as:\n\\[\n\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma + \\epsilon} \\cdot \\gamma + \\beta\n\\tag{4}\\] where:\n\n\\(\\mu(x)\\): the mean of the input features.\n\\(\\sigma(x)\\): the standard deviation of the input features.\n\\(\\gamma\\): a learnable scale parameter.\n\\(\\beta\\): a learnable shift parameter.\n\nThere are two learnable parameters in Layer Normalization: \\(\\gamma\\) and \\(\\beta\\), which have the same shape as the input features \\(d_{\\text{model}}\\).\nHowever, in the Root Mean Square(RMS) Normalization, proposed in (Zhang and Sennrich 2019), that we remove the mean from the normalization process. The RMS Normalization is defined as:\n\\[\n\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\cdot \\gamma\n\\tag{5}\\] where:\n\n\\(\\epsilon\\): a small constant to prevent division by zero.\n\\(\\gamma\\): a learnable scale parameter, which has the same shape as the input features \\(d_{\\text{model}}\\).\n\nAs we can see, the main difference between Layer Normalization and RMS Normalization is the removal of the mean from the normalization process, and remove the learnable shift parameter \\(\\beta\\). There are several advantage of that:\n\nSimplicity: RMS Normalization is simpler and requires fewer parameters, making it easier to implement and faster to compute. For model with \\(d_{\\text{model}} = 512\\), 8 layers, each layer has 2 normalization. Than the reduction number of parameter is up to \\(512 \\times 8 \\times 2 = 8192\\) parameters.\nFewer operations: no mean subtraction, no bias addition.\nSaves memory bandwidth, which is often the bottleneck in GPUs (not FLOPs).\nWhile reduce the number of parameters, it also maintains similar performance."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#pre-layer-normalization-vs.-post-layer-normalization",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#pre-layer-normalization-vs.-post-layer-normalization",
    "title": "LLM Part1: Architecture",
    "section": "3.2 Pre-Layer Normalization vs. Post-Layer Normalization",
    "text": "3.2 Pre-Layer Normalization vs. Post-Layer Normalization\n\n\n\n\n\n\nFigure 7: The figure illustrates the three different position of the normalization layer in the transformer architecture.\n\n\n\nOne main good reason why Pre-Layer Normalization is perform bettern than the Post-Layer Normalization is that, according to (Xiong et al. 2020), the help the gradient flow back through the network without disrupting the residual connections. When using the Pre-Layer Normalization, it tends to converge faster and more stably. And the initlization methods is become less sensitive to the scale of the inputs.\nThere are third type of the normalization position by (Ding et al. 2021) called sandwiched normalization, which is a combination of pre-layer and post-layer normalization. Is is proposed to improve the training for the text-image pair.\n\n\n\n\n\n\nGradient Flow\n\n\n\nOne generalizable lesson is that we should keep residual connections “clean” identity paths."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#qk-norm",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#qk-norm",
    "title": "LLM Part1: Architecture",
    "section": "3.3 QK Norm",
    "text": "3.3 QK Norm\nThere is another normalization method called Query-Key Normalization (QK Norm), which is designed to improve the attention mechanism by normalizing the query and key vectors before computing the attention scores."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#multi-headed-attention",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#multi-headed-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.1 Multi Headed Attention",
    "text": "4.1 Multi Headed Attention\nThe standard multi headed attention is defined as:\n\\[\n\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O\n\\tag{6}\\]\nwhere each head is computed as:\n\\[\n\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n\\tag{7}\\] with \\(W_i^Q\\), \\(W_i^K\\), and \\(W_i^V\\) being learned projection matrices.\nAnd the attention function, usually is scaled dot-product attention, is defined as: \\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\\tag{8}\\]\nwhere \\(d_k\\) is the dimension of the keys. The reason for the scaling factor \\(\\sqrt{d_k}\\) is to counteract the effect of large dot-product values by the large dimension of \\(K\\), which can push the softmax function into regions with very small gradients.\n\n4.1.1 Time Complexity of Scaled Dot-Product Attention\nThe time complexity of the scaled dot-product attention mechanism can be analyzed as follows:\n\nQuery-Key Dot Product: The computation of the dot product between the query and key matrices has a time complexity of \\(O(n^2 d_k)\\), where \\(n\\) is the sequence length and \\(d_k\\) is the dimension of the keys.\nSoftmax Computation: The softmax function is applied to the dot product results, which has a time complexity of \\(O(n^2)\\).\nValue Weighting: The final step involves multiplying the softmax output with the value matrix, which has a time complexity of \\(O(n^2 d_v)\\), where \\(d_v\\) is the dimension of the values.\n\nOverall, the time complexity of the scaled dot-product attention is dominated by the query-key dot product and can be expressed as:\n\\[\nO(n^2 (d_k + d_v))\n\\tag{9}\\]\nAs we can see, the time complexity is quadratic in the sequence length, which can be a bottleneck for long sequences / contexts. Let’s see how we can improve it."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#grouped-query-attention-multi-query-attention",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#grouped-query-attention-multi-query-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.2 Grouped Query Attention / Multi Query Attention",
    "text": "4.2 Grouped Query Attention / Multi Query Attention\n\n\n\n\n\n\nFigure 8: Overview of Grouped Query Attention & Multi Query Attention (Image Source: (Ainslie et al. 2023))\n\n\n\nProposed by the (Ainslie et al. 2023), Grouped Query Attention (GQA) and Multi Query Attention (MQA) are designed to reduce the computational burden of the attention mechanism by grouping queries and sharing keys and values across multiple queries."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#sparse-attention",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#sparse-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.3 Sparse Attention",
    "text": "4.3 Sparse Attention\n\nFixed Pattern Attention: for example, sliding window attention\nStrided Attention: for example, dilated attention, every m-th token\nBlock Sparse Attention: for example, BigBird (BigBirdSparseAttention2020zaheer?), Longformer (Beltagy, Peters, and Cohan 2020)\nGlobal Attention:\nLearned / Dynamic Spare Attention\n\n\n4.3.1 Sliding window attention\n\n\n\n\n\n\nFigure 9\n\n\n\n(Beltagy, Peters, and Cohan 2020)\n\n\n4.3.2 Dilated Attention\n\n\n\n\n\n\nFigure 10: The illustration of Sparse Attention. (Image Source: Generating Long Sequences with Sparse Transformers)\n\n\n\n(Child et al. 2019)"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#multi-latent-attention",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#multi-latent-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.4 Multi Latent Attention",
    "text": "4.4 Multi Latent Attention\n\n\n\n\n\n\nFigure 11: Compare between MHA, Grouped Query Attention, Multi Query Attention and Multi Latent Attention.\n\n\n\nMulti Latent Attention (MLA), proposed in (DeepSeek-AI et al. 2024) is a proposed extension to the attention mechanism that aims to capture more complex relationships within the input data by introducing multiple latent spaces. Each latent space can learn different aspects of the data, allowing for a more nuanced understanding of the input.\n\n\n\n\n\n\nFigure 12: The detail of Multi Latent Attention (MLA)."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#flash-attention",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#flash-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.5 Flash Attention",
    "text": "4.5 Flash Attention\nSo far, we see different attention mechanisms that aim to improve the efficiency and effectiveness of the standard attention mechanism. However, all aforementioned methods improve the attention mechanism by approximating the attention calculation. On the other hand, Flash Attention, proposed in (Dao 2023), takes a different approach by optimizing the attention computation itself.\n\n\n\n\n\n\nFigure 13: The illustration of Flash Attention\n\n\n\n\n4.5.1 Flash Attention V1 vs. V2 vs. V3"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#native-sparse-attention",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#native-sparse-attention",
    "title": "LLM Part1: Architecture",
    "section": "4.6 Native Sparse Attention",
    "text": "4.6 Native Sparse Attention\n\n\n\n\n\n\nFigure 14: Illustration of Native Sparse Attention (Image Source: Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention)\n\n\n\nProposed in (Yuan et al. 2025), this is a novel approach to sparse attention that aligns with hardware capabilities and allows for efficient training of sparse attention mechanisms."
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#attention-sink",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#attention-sink",
    "title": "LLM Part1: Architecture",
    "section": "4.7 Attention Sink",
    "text": "4.7 Attention Sink"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#swish",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#swish",
    "title": "LLM Part1: Architecture",
    "section": "5.1 Swish",
    "text": "5.1 Swish"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#gated-linear-unit-glu",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#gated-linear-unit-glu",
    "title": "LLM Part1: Architecture",
    "section": "5.2 Gated Linear Unit (GLU)",
    "text": "5.2 Gated Linear Unit (GLU)"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#multi-layer-perceptron-mlp",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#multi-layer-perceptron-mlp",
    "title": "LLM Part1: Architecture",
    "section": "6.1 Multi Layer Perceptron (MLP)",
    "text": "6.1 Multi Layer Perceptron (MLP)"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#gated-linear-unit-glu-1",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#gated-linear-unit-glu-1",
    "title": "LLM Part1: Architecture",
    "section": "6.2 Gated Linear Unit (GLU)",
    "text": "6.2 Gated Linear Unit (GLU)"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#mixture-of-experts-moe",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#mixture-of-experts-moe",
    "title": "LLM Part1: Architecture",
    "section": "6.3 Mixture of Experts (MoE)",
    "text": "6.3 Mixture of Experts (MoE)"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#weight-initialization",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#weight-initialization",
    "title": "LLM Part1: Architecture",
    "section": "7.1 Weight Initialization",
    "text": "7.1 Weight Initialization"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#layer-initialization",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#layer-initialization",
    "title": "LLM Part1: Architecture",
    "section": "7.2 Layer Initialization",
    "text": "7.2 Layer Initialization"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#llama",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#llama",
    "title": "LLM Part1: Architecture",
    "section": "8.1 LLaMA",
    "text": "8.1 LLaMA"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#qwen",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#qwen",
    "title": "LLM Part1: Architecture",
    "section": "8.2 Qwen",
    "text": "8.2 Qwen"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#deepseek",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#deepseek",
    "title": "LLM Part1: Architecture",
    "section": "8.3 DeepSeek",
    "text": "8.3 DeepSeek"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#gpt-oss",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#gpt-oss",
    "title": "LLM Part1: Architecture",
    "section": "8.4 GPT-Oss",
    "text": "8.4 GPT-Oss"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#diffusion-language-models",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#diffusion-language-models",
    "title": "LLM Part1: Architecture",
    "section": "9.1 Diffusion Language Models",
    "text": "9.1 Diffusion Language Models\n\n\n\n\n\n\nFigure 15: Illustration of Diffusion Language Model. (Video Source: Inception Lab)\n\n\n\nLLaDA (Nie et al. 2025) is a diffusion-based language model that leverages the principles of diffusion models to generate text. By modeling the text generation process as a diffusion process, LLaDA aims to improve the quality and diversity of generated text.\n\n\n\n\n\n\nFigure 16: Example of LLaDA generation process. Prompt: “Explain what artificial intelligence is.” (Image Source: LLaDA demo)\n\n\n\n\n\n\n\n\n\nFigure 17: The training process and sampling process of LLaDA. (Image Source: LLaDA)"
  },
  {
    "objectID": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#state-space-model-ssm",
    "href": "posts/Blogs/LLM-Architecture/LLM-Architecture.html#state-space-model-ssm",
    "title": "LLM Part1: Architecture",
    "section": "9.2 State Space Model (SSM)",
    "text": "9.2 State Space Model (SSM)"
  },
  {
    "objectID": "posts/Blogs/blogs_index.html",
    "href": "posts/Blogs/blogs_index.html",
    "title": "👋🏻Welcome to Yuyang’s Blog",
    "section": "",
    "text": "From Entropy to KL Divergence: A Comprehensive Guide\n\n\n\nMathematics\n\n\n\nKL Divergence, also known as Kullback-Leibler Divergence, is a fundamental concept in information theory and statistics. In this blog post, we will explore the concept of KL Divergence, its mathematical formulation, and its applications in various fields such as machine learning, data science, and artificial intelligence. By the end of this post, you will have a solid understanding of KL Divergence and how to apply it in your own projects.\n\n\n\n\n\n2025-12-02\n\n\nYuyang Zhang\n\n5 min\n\n951 words\n\n\n\n\n\n\n\nAll kinds of “Learnings” in Deep Learning\n\n\n\nDeep Learning\n\nSummary\n\n\n\nIn the blog, I will cover all kind of learnings in Deep Learning, including supervised learning, unsupervised learning, self-supervised learning, reinforcement learning, and more. By the end, you will have a solid understanding of different learning paradigms in Deep Learning.\n\n\n\n\n\n2025-12-02\n\n\nYuyang Zhang\n\n14 min\n\n2,766 words\n\n\n\n\n\n\n\nThe Evolution of Position Encoding in the Transformer\n\n\n\nTransformer\n\nLarge Language Model\n\n\n\nIn the blog, I will go through different position encoding machnism used in the Transformer-based neural network architecture. We will explore why the position encoding is important and different types of position encoding. In the end, we will extend the position encoding to visual field, such as in the Image and Video to unify the position encoding.\n\n\n\n\n\n2025-11-25\n\n\nYuyang Zhang\n\n16 min\n\n3,174 words\n\n\n\n\n\n\n\nAll About Diffusion & Flow Models\n\n\n\nGenerative-Model\n\nDiffusion-Model\n\n\n\nThis article offers a comprehensive overview of diffusion models from multiple perspectives. We begin with the foundations—DDPM, DDIM, and Score Matching—and explore their relationships. From there, we introduce the ODE/SDE framework, showing how DDPM can be derived from stochastic differential equations and how this connects to Flow Matching. We then highlight key model variants such as Stable Diffusion and Movie Gen, discussing their architectures and applications. Finally, we broaden the scope to examine how diffusion models are being adapted beyond image generation, including diffusion policies in reinforcement learning and their emerging role in large language models (LLMs).\n\n\n\n\n\n2025-10-15\n\n\nYuyang Zhang\n\n26 min\n\n5,001 words\n\n\n\n\n\n\n\nLLM Part3: Alignment\n\n\n\nTransformer\n\nLarge Language Model\n\n\n\nIn this blog, we will going through different alignment techniques for LLMs, including how to effectively align them with human values and intentions. We will explore techniques such as reinforcement learning from human feedback (RLHF), and more. By the end, you will have a solid understanding of how to align LLMs for your own applications.\n\n\n\n\n\n2025-10-10\n\n\nYuyang Zhang\n\n2 min\n\n380 words\n\n\n\n\n\n\n\nLLM Part2: Inference\n\n\n\nLarge Language Model\n\nInference\n\n\n\nIn this blog, we will going through the inference process of LLMs, including how to effectively use them for various tasks. We will explore techniques such as prompt engineering, few-shot learning, and more. By the end, you will have a solid understanding of how to leverage LLMs for your own applications.\n\n\n\n\n\n2025-10-01\n\n\nYuyang Zhang\n\n1 min\n\n88 words\n\n\n\n\n\n\n\nLLM Part1: Architecture\n\n\n\nTransformer\n\nLarge Language Model\n\n\n\nIn this blog, we will going through different transformer architectures that power LLMs. Such as position encoding, attention mechanisms, and more. In the end, we will explore some SOTA LLM architectures, and study how to they combine those techniques to achieve better performance.\n\n\n\n\n\n2025-09-25\n\n\nYuyang Zhang\n\n28 min\n\n5,428 words\n\n\n\n\n\n\n\nSpeed Up Training for Neural Networks\n\n\n\nTraining Tricks\n\n\n\nTraining large neural networks can be time-consuming and resource-intensive. This blog post explores various techniques to speed up the training process, including optimizing calculating in the single GPUs, and parallelizing the training across multiple GPUs.\n\n\n\n\n\n2025-09-10\n\n\nYuyang Zhang\n\n14 min\n\n2,648 words\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "posts/Projects/Qwen3-VL/Qwen3-VL.html",
    "href": "posts/Projects/Qwen3-VL/Qwen3-VL.html",
    "title": "Qwen3-VL Inference",
    "section": "",
    "text": "1 Qwen3-VL Architecture\n  \n  1.1 Image Input Processing\n  1.2 Vision Encoder\n  \n  1.2.1 2D RoPE\n  1.2.2 Position Interpolation\n  1.2.3 Patch Merger\n  \n  1.3 Vision Language Fusion\n  \n  1.3.1 DeepStack\n  \n  \n  2 Related Resources"
  },
  {
    "objectID": "posts/Projects/Qwen3-VL/Qwen3-VL.html#image-input-processing",
    "href": "posts/Projects/Qwen3-VL/Qwen3-VL.html#image-input-processing",
    "title": "Qwen3-VL Inference",
    "section": "1.1 Image Input Processing",
    "text": "1.1 Image Input Processing"
  },
  {
    "objectID": "posts/Projects/Qwen3-VL/Qwen3-VL.html#vision-encoder",
    "href": "posts/Projects/Qwen3-VL/Qwen3-VL.html#vision-encoder",
    "title": "Qwen3-VL Inference",
    "section": "1.2 Vision Encoder",
    "text": "1.2 Vision Encoder\n\nWe utilize the SigLIP-2 architecture as our vision encoder and continue training it with dynamic input resolutions, initialized from official pretrained checkpoints. To accommodate dynamic resolutions effectively, we employ 2D-RoPE and interpolate absolute position embeddings based on input size, following the methodology of CoMP.  Qwen3-VL Technical Report (Bai et al. 2025), P3\n\nThe vision encoder is the SigLip-2 architecture (Tschannen et al. 2025). As mentioned in the Qwen3-VL technical report (Bai et al. 2025), the weight is loaded from the SigLip-2 pretrained checkpoint, but with the continuous training to fit the dynamic input resolutions. One of the key techniques is change the position encoding. It use:\n\n2D-RoPE (Rotary Position Embedding)(Su et al. 2023) to replace the original position encoding in ViT.\nInterpolate the absolute position embeddings based on input size.\n\n\n1.2.1 2D RoPE\nThe 2D RoPe is an extension of the original RoPE (Su et al. 2023) to 2D inputs. The original RoPE is designed for 1D sequences, such as text. It encodes the position information by rotating the query and key vectors in the self-attention mechanism. The 2D RoPE extends this idea to 2D inputs, such as images. It encodes the position information by rotating the query and key vectors in both height and width dimensions. The mathematical formulation of 2D RoPE is as follows:\n\\[\n\\text{RoPE}_{2D}(Q, K, pos_h, pos_w) = Q \\cdot R(pos_h) \\cdot R(pos_w), K \\cdot R(pos_h) \\cdot R(pos_w)\n\\tag{1}\\]\nwhere \\(R(pos_h)\\) and \\(R(pos_w)\\) are the rotation matrices for height and width positions, respectively. By applying 2D RoPE, the model can effectively capture the spatial relationships in images, which is crucial for vision tasks.\n\n\n1.2.2 Position Interpolation\nIn addition to 2D RoPE, the Qwen3-VL model also uses position interpolation to handle dynamic input resolutions. The original ViT model uses absolute position embeddings, which are fixed for a specific input size. To accommodate dynamic input sizes, the Qwen3-VL model interpolates the absolute position embeddings based on the input size. The interpolation is done using bilinear interpolation, which allows the model to adapt to different input sizes without losing the positional information. The mathematical formulation of position interpolation is as follows:\n\\[\nPE_{interp}(x, y) = \\text{BilinearInterpolate}(PE, x, y)\n\\tag{2}\\]\nwhere \\(PE\\) is the original position embedding matrix, and \\((x, y)\\) are the coordinates in the interpolated space. By using position interpolation, the model can effectively handle images of varying sizes while maintaining the positional information necessary for accurate vision tasks.\n\n\n\n\n\n\nFigure 2: The illustration of CoMP\n\n\n\n\\[\n\\mathbf{R}_{x,y} =\n\\left(\\begin{array}{cc:cc}\n    \\cos x\\theta & -\\sin x\\theta & 0 & 0 \\\\\n    \\sin x\\theta & \\cos x\\theta & 0 & 0 \\\\\n    \\hdashline\n    0 & 0 & \\cos y\\theta & -\\sin y\\theta \\\\\n    0 & 0 & \\sin y\\theta & \\cos y\\theta\n\\end{array} \\right)\n\\tag{3}\\]\nwhere \\(x\\) and \\(y\\) are the position indices in height and width dimensions, respectively, and \\(\\theta\\) is a predefined angle.\n\n\n1.2.3 Patch Merger\n\n\n\n\n\n\nFigure 3: The function of Patch Merger. patch merging process, a down-sampling technique used in transformer architectures like the Swin Transformer. This operation reduces the spatial dimensions of the feature map while increasing the channel dimension.\n\n\n\nHowever, unlike other patch merger such as the one in Swin Transformer (Liu et al. 2021), the Patch Merger in Qwen3-VL is implemented as a linear layer that projects the concatenated patch embeddings into a lower-dimensional space. The adjcant patch embeddings are processed during the pre-processing step, and the Patch Merger layer simply reduces the dimensionality of the concatenated embeddings. Wewill see more"
  },
  {
    "objectID": "posts/Projects/Qwen3-VL/Qwen3-VL.html#vision-language-fusion",
    "href": "posts/Projects/Qwen3-VL/Qwen3-VL.html#vision-language-fusion",
    "title": "Qwen3-VL Inference",
    "section": "1.3 Vision Language Fusion",
    "text": "1.3 Vision Language Fusion\n\n1.3.1 DeepStack\n\n\n\n\n\n\nFigure 4: The architecture of DeepStack\n\n\n\n(Meng et al. 2024)\nThe DeepStack module is designed to effectively merge visual and textual information. It consists of multiple layers of cross-attention and feed-forward networks. The cross-attention mechanism allows the model to attend to relevant visual features based on the textual input, enabling a more comprehensive understanding of the multi-modal data. The feed-forward networks further process the combined features to enhance their representation."
  },
  {
    "objectID": "posts/Projects/PaliGemma/pali-gemma.html",
    "href": "posts/Projects/PaliGemma/pali-gemma.html",
    "title": "PaliGemma Inference and Fine Tuning",
    "section": "",
    "text": "Here is the full process of the Pali-Gemma\n\n\n\n\n\n\n\nFigure 1: The Pali-Gemma model is a multi-modal large language model that integrates vision and language tasks. The full process includes data collection, model training, and fine-tuning for specific applications.\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/PapersWithCode/100_Papers_index.html",
    "href": "posts/PapersWithCode/100_Papers_index.html",
    "title": "100 Papers with Code",
    "section": "",
    "text": "Note\n\n\n\n Due to the large number of papers included in this series. I will mainly write in Chinese to speed up the writing process. If you have any questions or suggestions, please feel free to contact me. Or if you would like to contribute translations of any of the articles into English or other languages, please let me know! \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01: Attention is all you need (Transformer)\n\n\n\nTransformer\n\nAttention\n\nNLP\n\nArchitecture\n\n\n\nTransformer 是一种基于自注意力机制的深度学习架构，能够并行处理序列，在语言、视觉和多模态任务中表现出色，并且作为 GPT、BERT 等大型语言模型（LLM）的核心基础，推动了当今生成式人工智能的快速发展。\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n03: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(Swin Transformer)\n\n\n\nComputer-Vision\n\n\n\nSwin Transformer 是一种使用层次化结构和滑动窗口自注意力机制的ViT模型，既保留了局部建模的高效性，又通过窗口偏移实现跨区域信息交互，可作为通用视觉骨干网络，适用于图像分类、目标检测和语义分割等多种视觉任务。\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n04: Learning Transferable Visual Models From Natural Language Supervision(CLIP)\n\n\n\nMulti-Modality\n\nMustReadPaper\n\n\n\nCLIP（对比语言-图像预训练模型）是 一个视觉-语言模型，通过在数亿对图文数据上进行对比学习，学得联合表征，从而能够在零样本条件下完成多种视觉任务。\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n05: Emerging Properties in Self-Supervised Vision Transformers(DINO)\n\n\n\nSelf-Supervised-Learning\n\nRepresentation-Learning\n\n\n\nThis is the DINO article\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n06: Auto-Encoding Variational Bayes(VAE)\n\n\n\nSelf-Supervised-Learning\n\nGenerative-Model\n\nRepresentation-Learning\n\n\n\nVAE（变分自动编码器）是一类结合概率图模型与神经网络的生成模型，它通过引入可参数化的近似后验 \\(q_\\phi(z|x)\\) 来摊销推断成本，并用最大化 ELBO 的方式同时学习数据的潜在表示与生成过程：其中重建项确保模型能从潜变量还原数据，KL 项则将潜空间约束为接近先验的连续结构。借助重参数化技巧，VAE 能在端到端训练中高效地学习一个平滑、可采样的潜空间，从而实现表示学习、插值、生成等功能，是现代深度生成模型的重要基础。\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n07: Denoising Diffusion Probabilistic Models(DDPM)\n\n\nA comprehensive overview of Denoising Diffusion Probabilistic Models (DDPM) and their applications in generative modeling.\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n08: Scalable Diffusion Models with Transformers (DiT)\n\n\nA comprehensive overview of the DiT model architecture and its applications in scalable diffusion processes.\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nVision-Transformer\n\n\n\nComputer Vision\n\nTransformer\n\n\n\nVision Transformer (ViT) 通过将图像切分为 patch 并直接应用标准 Transformer，实现了在大规模数据下超越 CNN 的图像分类性能。\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nVQ-VAE\n\n\n\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "posts/PapersWithCode/07-ddpm/DDPM.html",
    "href": "posts/PapersWithCode/07-ddpm/DDPM.html",
    "title": "07: Denoising Diffusion Probabilistic Models(DDPM)",
    "section": "",
    "text": "On this page\n   \n  \n  1 Preliminary\n  2 DDPM\n  3 Summary\n  4 Key Concepts\n  5 Q & A\n  6 Related resource & Further Reading\n  \n\n\n1 Preliminary\n\n\n2 DDPM\n\n\n3 Summary\n\n\n4 Key Concepts\n\n\n5 Q & A\n\n\n6 Related resource & Further Reading\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/PapersWithCode/04-clip/CLIP.html",
    "href": "posts/PapersWithCode/04-clip/CLIP.html",
    "title": "04: Learning Transferable Visual Models From Natural Language Supervision(CLIP)",
    "section": "",
    "text": "On this page\n   \n  \n  1 CLIP\n  2 Key Concepts\n  3 Q & A\n  4 扩展\n  \n\n\n1 CLIP\n\nTraining procedure Preprocessing The exact details of preprocessing of images during training/validation can be found here.\n\nImages are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\n # Summary\n\n\n2 Key Concepts\n\n\n3 Q & A\n\n\n4 扩展\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/PapersWithCode/08-dit/DiT.html",
    "href": "posts/PapersWithCode/08-dit/DiT.html",
    "title": "08: Scalable Diffusion Models with Transformers (DiT)",
    "section": "",
    "text": "On this page\n   \n  \n  1 Preliminary\n  2 DiT\n  3 Summary\n  4 Key Concepts\n  5 Q & A\n  6 Related resource & Further Reading\n  \n\n\n1 Preliminary\n\n\n2 DiT\n\n\n3 Summary\n\n\n4 Key Concepts\n\n\n5 Q & A\n\n\n6 Related resource & Further Reading\n\nCode Reference: Meta DiT\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html",
    "title": "01: Attention is all you need (Transformer)",
    "section": "",
    "text": "1 Preliminary\n  \n  1.1 Softmax Function\n  1.2 Vector Similarity\n  1.3 Word Embedding Layer\n  1.4 Position Embedding Layer\n  1.5 Attention Layer\n  \n  1.5.1 Self-Attention Layer\n  1.5.2 Cross Attention Layer\n  \n  1.6 Normalization Layer\n  1.7 Feed Forward Layer\n  1.8 Residual Connection\n  1.9 Output Layer\n  1.10 Others\n  1.11 Experiment\n  1.12 Weight Initialization\n  \n  1.12.1 Optimizer\n  \n  \n  2 Summary\n  3 Key Concepts\n  4 Q & A\n  5 Related resource & Further Reading\n我们开始第一篇论文的学习： 《Attention is All You Need》 (Vaswani et al. 2023)，也就是传说中的Transformer模型。Transformer模型的提出，彻底改变了自然语言处理（NLP）以及更广泛的领域。该架构完全基于注意力机制(Attention)，不再依赖循环（RNN）或卷积（CNN），因此在训练时更易并行化、效率更高。Transformer 已成为众多前沿模型的基础，不仅在 NLP 中表现突出，也扩展到计算机视觉等领域。比如 ChatGPT、DeepSeek 等大语言模型（LLM）都以 Transformer 为核心架构。所以我们自然就把它当作我们第一篇文章的首选。"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#softmax-function",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#softmax-function",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.1 Softmax Function",
    "text": "1.1 Softmax Function"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#vector-similarity",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#vector-similarity",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.2 Vector Similarity",
    "text": "1.2 Vector Similarity\n简单回顾了一下这些数学知识，接下来，让我们来看看Transformer到底是个什么东西。 # Transformer Transformer 是 Google 在2017年提出的新的神经网络架构，它的提出主要是为了解决，\n\n序列建模(Sequence Modeling)的效率问题:\n\n在 Transformer 出现之前，主流方法是 RNN（循环神经网络）和 CNN（卷积神经网络）。\nRNN 需要按顺序逐步处理序列，无法并行化，训练和推理效率低下。\nCNN 虽然有一定的并行性，但捕捉长距离依赖需要堆叠很多层，计算开销大。\n\n长距离依赖建模问题:\n\nRNN 在捕捉长距离依赖时容易出现梯度消失(Gradient Vanish) 或梯度爆炸(Gradient Explosion)，导致模型难以学习远距离的信息。\n\n\n\nThis inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. … In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions  Attention is all you need, p. \n\n模型的基本架构如下图所示：\n\n用代码显示就是:\nclass Transformer(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.encoder = Encoder(config)\n        self.decoder = Decoder(config)\n        self.output_layer = nn.Linear(config.d_model, config.tgt_vocab_size)\n        ...\n    \n    def forward(self, original, target):\n        ...\n接下来让我们从下至上，来深度解刨Transformer的模型结构分别是: 1. Word Embedding Layer 2. Position Embedding Layer 3. Attention Layer: 1. Self-Attention Layer 2. Cross-Attention Layer 4. Normalization Layer 5. Feed Forward Layer 6. Output Layer"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#word-embedding-layer",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#word-embedding-layer",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.3 Word Embedding Layer",
    "text": "1.3 Word Embedding Layer\nWord Embedding 基本是所有语言模型的第一步，"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#position-embedding-layer",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#position-embedding-layer",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.4 Position Embedding Layer",
    "text": "1.4 Position Embedding Layer\n\\[\n\\begin{split}\nPE_{(pos, 2i)} & = \\sin (pos / 10,000^{2i / d_{model}}) \\\\\nPE_{(pos, 2i+1)} & = \\cos (pos / 10,000^{2i+1 / d_{model}})\n\\end{split}\n\\]"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#attention-layer",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#attention-layer",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.5 Attention Layer",
    "text": "1.5 Attention Layer\n\n1.5.1 Self-Attention Layer\n\n\n1.5.2 Cross Attention Layer\n\\[\n\\begin{array}{|l|l|}\n\\hline\n\\textbf{Step} & \\textbf{Time Complexity} \\\\\n\\hline\nQK^\\top & \\mathcal{O}(n^2 d) \\\\\n\\text{softmax}(QK^\\top) & \\mathcal{O}(n^2) \\\\\n\\text{attention} \\times V & \\mathcal{O}(n^2 d) \\\\\n\\hline\n\\textbf{Total} & \\mathcal{O}(n^2 d) \\\\\n\\hline\n\\end{array}\n\\]"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#normalization-layer",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#normalization-layer",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.6 Normalization Layer",
    "text": "1.6 Normalization Layer\nLayer Normalization"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#feed-forward-layer",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#feed-forward-layer",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.7 Feed Forward Layer",
    "text": "1.7 Feed Forward Layer\n\\[\n\\text{FFN}(\\mathrm{x}) = \\underset{}{\\max} (0, \\mathrm{x} W_{1} + b_{1}) W_{2} + b_{2}\n\\]"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#residual-connection",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#residual-connection",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.8 Residual Connection",
    "text": "1.8 Residual Connection\n\\[\n\\mathbf{y} = \\text{LayerNorm}(\\mathbf{x} + \\mathrm{Sublayer}(\\mathbf{x}))\n\\]\n\\[\n\\begin{split}\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}}\n&= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\cdot \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\cdot \\left( \\mathbf{I} + \\frac{\\partial \\mathrm{Sublayer}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right) \\\\\n&= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}}}_{\\text{straight path}} +\n\\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\cdot\n\\frac{\\partial\\,\\mathrm{Sublayer}(\\mathbf{x})}{\\partial \\mathbf{x}}}_{\\text{through the sub-layer}}\n\\end{split}\n\\]"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#output-layer",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#output-layer",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.9 Output Layer",
    "text": "1.9 Output Layer"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#others",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#others",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.10 Others",
    "text": "1.10 Others\n当然，除了以上的一个部分，Transformer中还有几个值得一提的部分，不过处于篇幅的关系，在这里就不过多的赘述了，其中包括Tokenization等。"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#experiment",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#experiment",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.11 Experiment",
    "text": "1.11 Experiment"
  },
  {
    "objectID": "posts/PapersWithCode/01-transformer/Transformer.html#weight-initialization",
    "href": "posts/PapersWithCode/01-transformer/Transformer.html#weight-initialization",
    "title": "01: Attention is all you need (Transformer)",
    "section": "1.12 Weight Initialization",
    "text": "1.12 Weight Initialization\n\n1.12.1 Optimizer\nTransformer的论文中，用的是Adam Optimizer,\n\n\n\n\n\n\nNote\n\n\n\n对于不了的Adam的同学，也不用太担心，之后我们会有一系列的文章，专门介绍这些优化器的，包括Adam，AdamW，以及最近比较火的Moun"
  },
  {
    "objectID": "posts/LearningNotes/DLFaC/Chapter01/Chapter01.html",
    "href": "posts/LearningNotes/DLFaC/Chapter01/Chapter01.html",
    "title": "DLFaC Chapter 01: Introduction to the Deep Learning",
    "section": "",
    "text": "Chapter01 介绍Deep Learning的基本概念\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/LearningNotes/LLM-Series/index.html",
    "href": "posts/LearningNotes/LLM-Series/index.html",
    "title": "LLM Model Series Learning Notes",
    "section": "",
    "text": "Qwen Model Series\n\n\n在这篇文章中，我们将探索一系列的Qwen模型，沿着Qwen模型的发展时，来看看不同时期的Qwen模型运用了怎么样的不同的技术\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "posts/LearningNotes/CS336/index.html",
    "href": "posts/LearningNotes/CS336/index.html",
    "title": "CS336: LLM from Scratch Lecture Notes and Assignments",
    "section": "",
    "text": "Note\n\n\n\n Due to the time constraint, these notes may contain errors or omissions. And to speed up the writing process, I only wrote the notes in CHINESES. If you are interested in collaborating to improve these notes,or transalting to different languages, please feel free to contact me. \n\n\n\n\n\nRelated Resources:\n\nLecture Website: CS336 LLM from Scratch\nLecture Recordings: YouTube Playlist\nMy Solution Repo: GitHub\n\n\n\nAbout this Course:\n\nThis course has 17 Lectures and 5 Assignments in total.\nIt might take around 200 hours to finish all the lectures and assignments.\n\n\n\n\n\n\nLecture Notes for CS336\n\n\n\n\n\n\n\n\n\n\nLecture 01: Introduction & Tokenization\n\n\nLecture01介绍了课程的大纲以及LLM的现状和基本概念。在课程的后半段介绍了Language Modeling的第一步，即Tokenization的基本概念和常用方法。着重介绍了Byte Pair Encoding (BPE)算法的原理和实现。在学习完Lecture 01后，我们可以开始尝试Assignment 01的第一部分，即BPE-Tokenization的实现\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 02: PyTorch Basics & Resource Accounts\n\n\nLecture02 介绍了PyTorch的基本概念和使用方法。可以将这节课当作一个review，复习一下之前学过的PyTorch知识点。同时，课程中介绍了一个 einops 的库，可以简化张量操作的代码编写。这节课也介绍了不同数据类型（如FP32, FP16, BF16等）在深度学习中的应用和优缺点。并且通过计算这些数据类型在内存中的占用，帮助我们理解为什么有些数据类型更适合在有限资源下进行训练, 并且在什么情况下需要应用混合精度训练（Mixed Precision Training），什么情况下需要用高精度的数据类型。在课程的最后，还介绍了不同的Optimizer（如SGD, Adam等）的基本原理和使用场景\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 03: LM Model Architecture & Hyperparameters\n\n\nLecture03 介绍了现代大语言模型的核心架构与超参数设计。课程对比了原始 Transformer 与主流 LLaMA-like 架构，总结了 pre-norm、RMSNorm、SwiGLU、RoPE 等关键设计的经验共识，并结合大量近期模型实践，讲解了 MLP 宽度比例、注意力头配置、模型深宽比与词表规模等超参数选择原则。同时还介绍了 z-loss、QK-Norm、MQA/GQA 等用于提升训练稳定性和推理效率的关键技巧。\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\nNo matching items\n\n\n\nAssignments\n\n\n\n\n\n\n\n\n\n\nAssignment 01: Tokenization & Language Modeling\n\n\nAssignment 01 要求我们实现一个简单的语言模型训练流程，涵盖数据预处理、模型定义、训练和评估等步骤。通过这个作业，我们将巩固对自然语言处理基础概念的理解，并掌握使用PyTorch进行深度学习模型开发的基本技能\n\n\n\n\n\nYuyang Zhang\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "",
    "text": "Lecture 02 主要介绍了 PyTorch 的基础知识和一些实用的工具库，比如 einops。课程内容涵盖了张量操作、数据类型、优化器等方面的内容。并且最重要的是，提出了一个 Resource Accounting， 即训练一个模型，我们需要多大的内存和计算资源。 通过 Resource Accounting，我们可以更好地理解模型训练的资源需求，从而优化模型设计和训练过程。\n课程视频如下所示："
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#memory-accounting",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#memory-accounting",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "1.1 Memory Accounting",
    "text": "1.1 Memory Accounting\n所有的数据（包括模型参数、激活值、梯度、优化器状态等）都是以 Tensor 的形式储存。我们有很多种方式创建一个 Tensor，比如：\nx = torch.tensor([[1., 2, 3], [4, 5, 6]])\nx= torch.randn(3, 4)\nx = torch.zeros(2, 5)\nx = torch.empty(10, 10)\n每个 Tensor 都有一个数据类型 (Data Type)，默认的数据类型是 float32 (也称为 FP32)。不同的数据类型会占用不同的内存空间。接下来我们来看看几种常见的数据类型及其内存占用\n\n1.1.1 Common Data Types\n在了解不同的Float Types之前，我们先来了解一下浮点数的表示方法。计算机中的浮点数通常采用 IEEE 754 标准进行表示。浮点数由三部分组成：符号位 (Sign Bit)、指数位 (Exponent Bits) 和尾数位 (Mantissa Bits), 也叫Fraction 。\n\n\n\n\n\n\nFigure 1: 浮点数的表示\n\n\n\n浮点数的值可以通过以下公式计算： \\[\nnumber = (-1)^{Sign} \\times Base^{(Exponent - Bias)} \\times 1.Mantissa\n\\tag{1}\\]\n其中，Base 通常为2，Bias 是一个用于调整指数的偏移量，具体取决于指数位的长度, 通常为2的指数位长度减1的值 为127。\n = b_1·2^{-1} + b_2·2^{-2} + b_3·2^{-3} + … + b_{23}·2^{-23}\n对于 Figure 1 中的浮点数表示：\n\n符号位 (Sign) 为 0，表示正数\n指数位 (Exponent) 为 01111100，转换为十进制为 124，减去 Bias 127 得到 -3\n尾数位 (Mantissa) 为 01000000000000000000000 ，转换为十进制为 0.25，因此 1.Mantissa = 1 + 0.25 = 1.25\n\n将这些值代入公式 Equation 1 中，可以计算出浮点数的值为： \\[\nnumber = (-1)^0 \\times 2^{-3} \\times 1.25 = 0.15625\n\\]\n比如 bias 10000000，转换为十进制为 128，减去 Bias 127 得到 1\n\n\n\n\n\n\nFigure 2: 通过这个在线计算器，验证浮点数的表示方法。\n\n\n\n明白了Float Number的计算方法之后，我们来看看几种常见的数据类型及其内存占用。\n\n1.1.1.1 Float32\n\n\n\n\n\n\nFigure 3: Float32 使用32位 (4字节) 来表示一个浮点数。它由1位符号位、8位指数位和23位尾数位组成，可以表示大约7位十进制有效数字。\n\n\n\nFloat32 也叫 single precision 是深度学习中最常用的数据类型，几乎所有的深度学习框架都默认使用 Float32 作为张量的数据类型。 Float32 可以表示的数值范围大约在 1.18e-38 到 3.4e+38 之间，足以满足大多数深度学习任务的需求。\nx = torch.tensor([1.0, 2.0, 3.0])\nprint(x.dtype)  # 输出: torch.float32\nprint(x.element_size())  # 输出: 4 (每个元素占用4字节)\n\n\n\n\n\n\nFigure 4: Float16 使用16位 (2字节) 来表示一个浮点数。它由1位符号位、5位指数位和10位尾数位组成，可以表示大约3位十进制有效数字。\n\n\n\nFloat16 也叫 half precision，主要用于减少内存占用和加速计算。相比于 Float32，Float16 可以显著降低内存使用量，从而允许我们训练更大的模型或者使用更大的批量大小 (Batch Size)。\nFloat16 可以表示的数值范围大约在 6.1e-5 到 6.5e+4 之间，相比于 Float32 有一定的限制，尤其是在表示非常小或者非常大的数值时可能会出现溢出（Overflow）或者下溢（Underflow）的问题。当这个问题出现时，我们可能会出现NaN的情况，由此导致训练失败。\n\n\n\n\n\n\nTip\n\n\n\n当我们训练神经网络时，损失函数出现NaN的情况，通常是因为数值溢出或者下溢导致的。\n\n\nIn [14]: x = torch.tensor([1e-8], dtype=torch.float16)\n\nIn [15]: x\nOut[15]: tensor([0.], dtype=torch.float16)\n\nIn [8]: x = torch.tensor([1e+8], dtype=torch.float16)\n\nIn [9]: x\nOut[9]: tensor([inf], dtype=torch.float16)\n\n\n1.1.1.2 BFloat16\n\n\n\n\n\n\nFigure 5: BFloat16 使用16位 (2字节) 来表示一个浮点数。它由1位符号位、8位指数位和7位尾数位组成，可以表示大约3位十进制有效数字。\n\n\n\nGoogle 在2018年提出了 BFloat16 (Brain Float Point 16) 数据类型，主要用于深度学习加速。相比于 Float16，BFloat16 保留了与 Float32 相同的指数位长度，因此可以表示更大的数值范围，从而减少了溢出和下溢的风险。相对于Float32， BFloat16 的精度较低，但在许多深度学习任务中，BFloat16 的精度已经足够使用。\nIn [11]: x\nOut[11]: tensor([1.0014e+08], dtype=torch.bfloat16)\n\nIn [12]: x = torch.tensor([1e-8], dtype=torch.bfloat16)\n\nIn [13]: x\nOut[13]: tensor([1.0012e-08], dtype=torch.bfloat16)\nIn [16]: torch.finfo(torch.float32)\nOut[16]: finfo(resolution=1e-06, min=-3.40282e+38, max=3.40282e+38, eps=1.19209e-07, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=float32)\n\nIn [17]: torch.finfo(torch.float16)\nOut[17]: finfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16)\n\nIn [18]: torch.finfo(torch.bfloat16)\nOut[18]: finfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)\n\n\n1.1.1.3 FP8\nFP8 是一种8位浮点数表示方法，通常用于极端内存受限的场景（比如将模型部署到边缘设备）。FP8 有两种主要格式：E4M3 和 E5M2。\n\n\n\n\n\n\nFigure 6: FP8 使用8位 (1字节) 来表示一个浮点数。E4M3 由1位符号位、4位指数位和3位尾数位组成；E5M2 由1位符号位、5位指数位和2位尾数位组成。\n\n\n\nE4M3 (range [-448, 448]) and E5M2 ([-57344, 57344]).\n\n\n1.1.1.4 Other Data Types\n除了上述几种常见的数据类型之外，还有一些其他的数据类型，比如 int8 和 int4。这些数据类型通常用于量化 (Quantization) 技术，通过将浮点数转换为整数来减少内存占用和加速计算。\n\n\n\nData Type\nDescription\nBits per Value\nBytes per Value\n\n\n\n\nfloat32\nSingle Precision\n32\n4\n\n\nfloat16\nHalf Precision\n16\n2\n\n\nbfloat16\nBrain Float\n16\n2\n\n\nint8\n8-bit Integer\n8\n1\n\n\nint4\n4-bit Integer\n4\n0.5\n\n\n\n我们可以看到，不同的数据类型有不同的内存占用。选择合适的数据类型可以帮助我们在内存受限的情况下训练更大的模型或者使用更大的批量大小：\n\n用 float32 进行训练，适用于大多数任务，但内存占用较高。\n用 float16 进行训练，可以显著减少内存占用，但需要注意数值溢出和下溢的问题。\n用 bfloat16 进行训练，可以在减少内存占用的同时，保持较大的数值范围，适用于大多数深度学习任务。\n用 int8 或 int4 进行量化训练，可以极大地减少内存占用，但需要进行额外的量化和反量化操作，适用于部署阶段。\n\n因此在训练的过程中，我们通常采用一种Mixed Precision Training的方法，来平衡内存占用和数值精度的问题。"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#compute-accounting",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#compute-accounting",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "1.2 Compute Accounting",
    "text": "1.2 Compute Accounting\n在了解了内存占用之后，我们来看看计算资源的需求。计算资源主要取决于模型的参数量和训练数据的规模。\n\n1.2.1 Tensor on GPUs\n当我们创建一个 Tensor 时，它默认是在 CPU 上创建的。如果我们想要在 GPU 上进行计算，需要将 Tensor 移动到 GPU 上：\nx = torch.tensor([1.0, 2.0, 3.0])\nx.device  # 输出: cpu\nx = x.cuda()  # 将 Tensor 移动到 GPU 上\nx.device  # 输出: cuda:0\n\n\n\n\n\n\nFigure 7: 我们将Tensor从CPU的RAM上，移动到DRAM上。\n\n\n\n我们可以通过以下方式创建一个直接在 GPU 上的 Tensor：\nx = torch.tensor([1.0, 2.0, 3.0], device='cuda')\n可以通过GPU的内存情况来查看当前 GPU 上的内存使用情况：\nimport torch\nprint(torch.cuda.memory_summary())\nmemory_allocated = torch.cuda.memory_allocated() \nx = torch.tensor([1.0, 2.0, 3.0], device='cuda')\nmemory_allocated_after = torch.cuda.memory_allocated()\nprint(f\"Memory allocated before: {memory_allocated}, after: {memory_allocated_after}\")"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#tensor-operations",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#tensor-operations",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "1.3 Tensor Operations",
    "text": "1.3 Tensor Operations\nPyTorch Tensor 是一个Pointer，指向一块连续的内存区域。我们可以通过strides来访问Tensor中的数据。\n\n\n\n\n\n\nFigure 8\n\n\n\nTensor 有很多操作，比如 reshape, permute, transpose 等等。这些操作通常不会改变数据的存储方式，而是通过修改 strides 来实现对数据的不同视图 (View)。这是十分高效的，因为我们不需要进行数据的复制 (Copy)，只需要修改 Tensor 的元数据 (Metadata)。不过需要小心的是，当我们修改 Tensor 的数据时，可能会影响到原始数据，因为它们共享同一块内存区域。\n有一些操作会导致数据变得不连续 (Non-Contiguous)，比如 transpose 和 permute。这些操作会改变数据的存储顺序，从而导致数据在内存中不再是连续存储的。这时，我们可以使用 contiguous() 方法来创建一个新的连续存储的 Tensor。\n\n\n\n\n\n\nNote\n\n\n\n.transpose().contiguous() 的操作我们在一 Attention 的计算中经常会用到。\n\n\nElement-wise 操作 (比如加法、乘法等) 通常要求输入的 Tensor 是连续存储的。如果输入的 Tensor 是不连续的，PyTorch 会自动调用 contiguous() 方法来创建一个新的连续存储的 Tensor，从而保证操作的正确性。，比如：\nx = torch.tensor([1, 4, 9])\nx.pow(2)\nx.sqrt()\nx.rsqrt()\nx + x \nx * 2\nx.triu()\n\n1.3.1 Matrix Multiplication\n最重要的也是最常用的操作之一是矩阵乘法 (Matrix Multiplication)，在深度学习中，矩阵乘法被广泛应用于神经网络的前向传播和反向传播过程中。\nA = torch.randn(3, 4)\nB = torch.randn(4, 5)\nC = torch.matmul(A, B)  # C 的形状为 (3,5)\nC = A @ B  # 另一种矩阵乘法的写法\nC = A.mm(B)  # 另一种矩阵乘法的写法"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#gradient-calculation",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#gradient-calculation",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "1.4 Gradient Calculation",
    "text": "1.4 Gradient Calculation\n了解了矩阵乘法之后，我们来看看如何计算梯度 (Gradient)。在深度学习中，梯度是用来更新模型参数的关键。PyTorch 提供了自动微分 (Autograd) 功能，可以自动计算张量的梯度。\n假设我们有个简单的神经网络层：\n\\[\nY = 0.5 * (X  W - 5)^2\n\\]\n在前置的传播过程中，我们计算输出 Y：\nx = torch.randn([1., 2, 3])  # 输入张量 X\nw = torch.randn([1., 2, 3], requires_grad=True)  # 权重张\ny = 0.5 * (x @ w - 5) ** 2  # 前向传播计算输出 Y\npred_y = x @ w\nloss = 0.5 * (pred_y - 5) ** 2\n在反向传播过程中，我们计算梯度：\nloss.backward()  # 反向传播计算梯度\nprint(w.grad)  # 输出权重 w 的梯度"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#summary",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#summary",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "1.5 Summary",
    "text": "1.5 Summary\nForward Pass: 2 * Number of data points * Number of parameters Backward Pass: 4 * Number of data points * Number of parameters\nTotal: 6 * Number of data points * Number of parameters\n\n1.5.1 Einops Library\n在处理高维张量时，张量的重排 (Rearrangement) 和变形 (Reshaping) 是非常常见的操作。传统的方法通常需要多行代码，并且容易出错。einops 是一个强大的库，可以简化这些操作，使代码更加简洁和易读。\n\n\n\n\n\n\n\nTip\n\n\n\n如果以上的内容比较难以理解的话，可以访问这个链接。 里面有非常详细的einops教程，包含了很多例子和可视化的图示。\n\n\n在这里就不具体展开了。"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#tensor-flops",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#tensor-flops",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "1.6 Tensor Flops",
    "text": "1.6 Tensor Flops\n了解了内存占用和计算资源之后，我们来看看如何计算 Tensor 的 FLOPs (Floating Point Operations)。FLOPs 是衡量计算复杂度的一个重要指标，表示每秒钟可以执行多少次浮点运算。其中两个常见的指标是：\n\nFLOPs: 总共需要执行的浮点运算次数\nFLOP/s 也写作FLOPS： 每秒钟可以执行的浮点运算次数\n\n对于两个 矩阵 A (形状为 m x n) 和 B (形状为 n x p) 的矩阵乘法 C = A @ B，我们可以计算出 FLOPs 如下： \\[\nFLOPs = 2 * m * n * p\n\\]\n其中，乘法操作需要 m * n * p 次， 加法操作也需要 m * n * p 次，因此总共需要 2 * m * n * p 次浮点运算。\n对于其他的张量操作，我们也可以类似地计算 FLOPs。了解 FLOPs 可以帮助我们评估模型的计算复杂度，从而优化模型设计和训练过程。比如：\n\nElement-wise 操作 (比如加法、乘法等) 的 FLOPs 通常与张量的元素数量成正比 \\(\\mathcal{O}(m \\times n)\\)\nAddition of two matrices of shape (m, n): FLOPs = m * n\n\n由此可见，矩阵乘法的计算复杂度远高于 Element-wise 操作，因此在设计模型时，我们通常会尽量减少矩阵乘法的次数，从而降低计算复杂度。"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#model-definition",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#model-definition",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "2.1 Model definition",
    "text": "2.1 Model definition\n首先，我们需要定义一个模型。模型通常由多个层 (Layer) 组成，每个层都有自己的参数 (Parameters)。现代的LLM模型通常是基于 Transformer (Vaswani et al. 2023) 架构构建的。 具体的内容，会在Lecture 03中详细介绍，在这里就先不展开了。\n\n\n\n\n\n\nNote\n\n\n\n对于Transformer比较陌生的同学，可以参考我这一篇笔记 100-Paper with Code: 01 Transformer"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#parameter-initialization",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#parameter-initialization",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "2.2 Parameter initialization",
    "text": "2.2 Parameter initialization\n定义好模型之后，我们需要初始化模型的参数。参数的初始化方式会影响模型的训练效果和收敛速度。常见的初始化方法有随机初始化 (Random Initialization)、Xavier 初始化 (Xavier Initialization) 和 He 初始化 (He Initialization) 等等。\n许多加速器（比如 GPU 和 TPU）都对矩阵乘法进行了高度优化，利用并行计算和专用硬件单元来加速矩阵乘法的计算过程。因此，在深度学习中，尽量将计算任务转化为矩阵乘法，可以显著提升计算效率。"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#optimizer-selection",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#optimizer-selection",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "2.3 Optimizer Selection",
    "text": "2.3 Optimizer Selection\n在训练模型时，我们需要选择一个优化器 (Optimizer) 来更新模型的参数。 常见的优化器有随机梯度下降 (SGD)、动量法 (Momentum)、Adam 和 AdamW 等等。不同的优化器有不同的更新规则和超参数 (Hyperparameters)，选择合适的优化器可以帮助我们更快地收敛到最优解。"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#loss-function",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#loss-function",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "2.4 Loss Function",
    "text": "2.4 Loss Function\n在训练模型时，我们需要定义一个损失函数 (Loss Function) 来衡量模型的预测结果与真实标签之间的差距。常见的损失函数有均方误差 (Mean Squared Error, MSE)、交叉熵损失 (Cross Entropy Loss) 等等。选择合适的损失函数可以帮助我们更好地优化"
  },
  {
    "objectID": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#training-loop",
    "href": "posts/LearningNotes/CS336/Lectures/Lecture02/lec02.html#training-loop",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "2.5 Training Loop",
    "text": "2.5 Training Loop\n当我们定义好模型、初始化参数、选择优化器和损失函数之后，我们就可以开始训练模型了。训练过程通常包括以下几个步骤：\n加载数据 (Data Loading): 从数据集中加载训练数据，通常使用批量 (Batch) 的方式进行加载。 前向传播 (Forward Pass): 将输入数据传递给模型，计算模型的输出。 计算损失 (Loss Calculation): 使用损失函数计算模型输出与真实标签之间的差距。 反向传播 (Backward Pass): 计算损失函数相对于模型参数的梯度。 参数更新 (Parameter Update): 使用优化器根据计算得到的梯度更新模型参数。\n重复以上步骤，直到模型收敛或者达到预定的训练轮数 (Epochs)。\n\n2.5.1 Randomness Control\n在训练模型时，随机性 (Randomness) 是不可避免的。比如，参数的初始化、数据的打乱 (Shuffling) 和批量的选择 (Batch Selection)等操作都涉及到随机性。为了保证实验的可重复性 (Reproducibility)，我们通常需要控制随机数生成器 (Random Number Generator, RNG) 的种子 (Seed)。\nimport random\nimport numpy as np\nimport torch\n\ndef seed_everything(seed):    \n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\nseed_everything(42)\n\n\n2.5.2 Data Loading\n\n\n2.5.3 Checkpointing\n在训练大型模型时，训练过程可能会非常耗时，并且容易受到各种意外情况的影响，比如断电、系统崩溃等。为了避免训练过程中的数据丢失，我们通常会使用检查点 (Checkpointing) 技术来保存模型的状态。\nModel Checkpointing 通常保存以下几个方面的信息：\n\n模型参数 (Model Parameters): 保存模型的权重和偏置等参数。\n优化器状态 (Optimizer State): 保存优化器的状态，比如动量 (Momentum ) 和学习率 (Learning Rate) 等信息。\n学习率调度器状态 (Learning Rate Scheduler State): 保存学习率调度器的状态。\n训练进度 (Training Progress): 保存当前的训练轮数 (Epochs) 和批量索引 (Batch Index) 等信息。\n\n在 GPU 上进行计算时，数据传输的速度通常是一个瓶颈。为了提高数据传输的效率，我们可以使用 Pinned Memory (也叫 Page-Locked Memory)。Pinned Memory 是一种特殊的内存区域，可以加速主机 (Host) 和设备 (Device) 之间的数据传输。\n\n\n\n\n\n\nFigure 9: 如图所示，Pinned memory 可以作为设备(Device)到主机(Host)拷贝的中转区，直接在 pinned memory 中分配主机数组，就能避免 pageable 内存与 pinned 内存之间的额外拷贝开销，从而提升数据传输效率。\n\n\n\n这篇文章中介绍了4种常见的加速数据传输的方法：\n\n利用 Numpy Memmap 处理大数据集: 通过内存映射技术，只将数据集的一部分加载到内存中，减少内存使用，提高数据加载速度。\n多利用 torch.from_numpy 函数: 直接将 NumPy 数组转换为 PyTorch 张量，避免不必要的数据复制，提高数据传输效率。\n将 num_workers 设置为大于0: 通过多线程数据加载，提高数据预处理和加载的并行度，减少数据加载时间。\n使用 Pinned Memory 加速主机与设备之间的数据传输: 通过将数据存储在固定内存中，减少数据传输的延迟，提高\n\nfrom torch.utils.data import DataLoader\n\n# some code\n\nloader = DataLoader(your_dataset, ..., pin_memory=True)\ndata_iter = iter(loader)\n\nnext_batch = data_iter.next() # start loading the first batch\nnext_batch = [ _.cuda(non_blocking=True) for _ in next_batch ]  # with pin_memory=True and non_blocking=True, this will copy data to GPU non blockingly\n\nfor i in range(len(loader)):\n    batch = next_batch \n    if i + 2 != len(loader): \n        # start copying data of next batch\n        next_batch = data_iter.next()\n        next_batch = [ _.cuda(async=True) for _ in next_batch]\n这几个方法可以显著提升数据传输和加载的效率，尤其在处理大规模数据集时效果尤为明显。在我们完成Assignment 01时，会用到这些技巧来优化数据加载过程。\n\n\n2.5.4 Mixed Precision Training\n在之前的内容中，我们介绍了不同的数据类型及其内存占用。在实际的模型训练过程中，我们通常会采用混合精度训练 (Mixed Precision Training) 的方法，来平衡内存占用和数值精度的问题。\n那那些需要混合精度训练呢？通常在以下几种情况下，我们会考虑使用混合精度训练：\n\nbfloat16 或者 fp8 作为前向的计算数据类型（activations）\nfloat32 作为梯度计算的数据类型, 并且是用 float32 来更新参数\n优化器状态 (Optimizer States) 使用 float32 来存储\n\nMicikevicius et al. (2018) 提出了一种混合精度训练的方法，称为 Loss Scaling。Loss Scaling 的基本思想是通过放大损失函数的值，来避免在使用低精度数据类型时出现数值下溢 (Underflow) 的问题。\nLoss Scaling 的具体步骤如下：\n\n在前向传播过程中，计算损失函数的值，并将其乘以一个放大因子 (Scaling Factor)。\n在反向传播过程中，计算梯度，并将其除以放大因子。\n使用优化器更新模型参数。 不过，当我们用 bfloat16 进行前向计算时，我们可以不需要使用 Loss Scaling，因为 bfloat16 已经有足够的数值范围来避免下溢的问题。"
  }
]