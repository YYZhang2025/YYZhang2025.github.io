<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yuyang Zhang">
<meta name="dcterms.date" content="2025-11-25">
<meta name="description" content="In the blog, I will go through different position encoding machnism used in the Transformer-based neural network architecture. We will explore why the position encoding is important and different types of position encoding. In the end, we will extend the position encoding to visual field, such as in the Image and Video to unify the position encoding.">

<title>The Evolution of Position Encoding in the Transformer</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../.././style/icon.avif" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-0348920b7671f696dc9078d39bff215e.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-859f99caab0bec132077bcc433b53446.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-970c7fc97ae78f6c1e7458e7c69915e7.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-859f99caab0bec132077bcc433b53446.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/line-highlight-1.0.0/line-highlight.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="https://fonts.cdnfonts.com/css/cmu-sans-serif" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../style/styles.css">
<link rel="stylesheet" href="../../../style/callout.css">
<meta property="og:title" content="The Evolution of Position Encoding in the Transformer">
<meta property="og:description" content="In the blog, I will go through different position encoding machnism used in the Transformer-based neural network architecture. We will explore why the position encoding is important and different types of position encoding. In the end, we will extend the position encoding to visual field, such as in the Image and Video to unify the position encoding.">
<meta property="og:image" content="assets/Position-Embedding-sin-cos-omega.png">
</head>

<body class="nav-sidebar docked nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">About Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/YYZhang2025"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/zhang-yuyang/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../posts/Blogs/blogs_index.html"> 
<span class="menu-text">Blogs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../posts/Projects/projects_index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../posts/PapersWithCode/100_Papers_index.html"> 
<span class="menu-text">100 Papers with Code</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-learning-notes" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Learning Notes</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-learning-notes">    
        <li>
    <a class="dropdown-item" href="../../../posts/LearningNotes/CS336/index.html">
 <span class="dropdown-text">Stanford CS336: LLM from Scratch</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/LearningNotes/DLFaC/index.html">
 <span class="dropdown-text">DLFaC</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/LearningNotes/LLM-Series/index.html">
 <span class="dropdown-text">LLM Model Series</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <a class="flex-grow-1 no-decor" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
          <h1 class="quarto-secondary-nav-title">The Evolution of Position Encoding in the Transformer</h1>
        </a>     
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="1">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#preliminary" id="toc-preliminary" class="nav-link active" data-scroll-target="#preliminary"><span class="header-section-number">1</span> Preliminary</a>
  <ul>
  <li><a href="#sin-and-cos-function" id="toc-sin-and-cos-function" class="nav-link" data-scroll-target="#sin-and-cos-function"><span class="header-section-number">1.1</span> <span class="math inline">\(\sin\)</span> and <span class="math inline">\(\cos\)</span> function</a></li>
  <li><a href="#rotation-matrix-and-complex-exponential" id="toc-rotation-matrix-and-complex-exponential" class="nav-link" data-scroll-target="#rotation-matrix-and-complex-exponential"><span class="header-section-number">1.2</span> Rotation Matrix and Complex Exponential</a></li>
  <li><a href="#attention-in-transformer" id="toc-attention-in-transformer" class="nav-link" data-scroll-target="#attention-in-transformer"><span class="header-section-number">1.3</span> Attention in Transformer</a></li>
  </ul></li>
  <li><a href="#absolute-positional-encoding" id="toc-absolute-positional-encoding" class="nav-link" data-scroll-target="#absolute-positional-encoding"><span class="header-section-number">2</span> Absolute Positional Encoding</a>
  <ul>
  <li><a href="#learned-position-encoding" id="toc-learned-position-encoding" class="nav-link" data-scroll-target="#learned-position-encoding"><span class="header-section-number">2.1</span> Learned Position Encoding</a></li>
  <li><a href="#d-positional-encoding" id="toc-d-positional-encoding" class="nav-link" data-scroll-target="#d-positional-encoding"><span class="header-section-number">2.2</span> 2D Positional Encoding</a></li>
  </ul></li>
  <li><a href="#relative-positional-encoding" id="toc-relative-positional-encoding" class="nav-link" data-scroll-target="#relative-positional-encoding"><span class="header-section-number">3</span> Relative Positional Encoding</a></li>
  <li><a href="#rope" id="toc-rope" class="nav-link" data-scroll-target="#rope"><span class="header-section-number">4</span> RoPE</a>
  <ul>
  <li><a href="#d-rope" id="toc-d-rope" class="nav-link" data-scroll-target="#d-rope"><span class="header-section-number">4.1</span> 2D-RoPE</a></li>
  <li><a href="#m-rope" id="toc-m-rope" class="nav-link" data-scroll-target="#m-rope"><span class="header-section-number">4.2</span> M-RoPE</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">5</span> Summary</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">The Evolution of Position Encoding in the Transformer</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Transformer</div>
    <div class="quarto-category">Large Language Model</div>
  </div>
  </div>

<div>
  <div class="description">
    In the blog, I will go through different position encoding machnism used in the Transformer-based neural network architecture. We will explore why the position encoding is important and different types of position encoding. In the end, we will extend the position encoding to visual field, such as in the Image and Video to unify the position encoding.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Yuyang Zhang </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">2025-11-25</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">2025-11-26</p>
    </div>
  </div>
    
  </div>
  


</header>

<nav id="TOC-body" role="doc-toc">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#preliminary" id="toc-preliminary"><span class="header-section-number">1</span> Preliminary</a>
  <ul>
  <li><a href="#sin-and-cos-function" id="toc-sin-and-cos-function"><span class="header-section-number">1.1</span> <span class="math inline">\(\sin\)</span> and <span class="math inline">\(\cos\)</span> function</a></li>
  <li><a href="#rotation-matrix-and-complex-exponential" id="toc-rotation-matrix-and-complex-exponential"><span class="header-section-number">1.2</span> Rotation Matrix and Complex Exponential</a></li>
  <li><a href="#attention-in-transformer" id="toc-attention-in-transformer"><span class="header-section-number">1.3</span> Attention in Transformer</a></li>
  </ul></li>
  <li><a href="#absolute-positional-encoding" id="toc-absolute-positional-encoding"><span class="header-section-number">2</span> Absolute Positional Encoding</a>
  <ul>
  <li><a href="#learned-position-encoding" id="toc-learned-position-encoding"><span class="header-section-number">2.1</span> Learned Position Encoding</a></li>
  <li><a href="#d-positional-encoding" id="toc-d-positional-encoding"><span class="header-section-number">2.2</span> 2D Positional Encoding</a></li>
  </ul></li>
  <li><a href="#relative-positional-encoding" id="toc-relative-positional-encoding"><span class="header-section-number">3</span> Relative Positional Encoding</a></li>
  <li><a href="#rope" id="toc-rope"><span class="header-section-number">4</span> RoPE</a>
  <ul>
  <li><a href="#d-rope" id="toc-d-rope"><span class="header-section-number">4.1</span> 2D-RoPE</a></li>
  <li><a href="#m-rope" id="toc-m-rope"><span class="header-section-number">4.2</span> M-RoPE</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary"><span class="header-section-number">5</span> Summary</a></li>
  </ul>
</nav>
<p>After Transformer<span class="citation" data-cites="AttentionAllYou2023vaswani">(<a href="#ref-AttentionAllYou2023vaswani" role="doc-biblioref">Vaswani et al. 2023</a>)</span> was introduced, it become the default components in the Deep Learning. It has several components, one of the most important (and most confused for me) was Position Encoding. However the original position encoding methods has several limit, several different methods was proposed later to improve the perform of position encoding in the transformer. In the article, we will explore different methods of position encoding, and extend to the image and video position encoding. But before that, let’s prepare some mathematic background to help us better understand the concepts.</p>
<section id="preliminary" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Preliminary</h1>
<section id="sin-and-cos-function" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="sin-and-cos-function"><span class="header-section-number">1.1</span> <span class="math inline">\(\sin\)</span> and <span class="math inline">\(\cos\)</span> function</h2>
<p><span class="math inline">\(\sin\)</span> and <span class="math inline">\(\cos\)</span> function are two most basic <strong>periodic functions</strong> that we learned in high school. Both <span class="math inline">\(\sin\)</span> and <span class="math inline">\(\cos\)</span> function are periodic with period <span class="math inline">\(2\pi\)</span>: <span class="math display">\[
\sin(\theta + 2\pi) = \sin \theta , \quad   \cos(\theta + 2\pi) = \cos \theta
\]</span> The phase-shift(addition) formula for the <span class="math inline">\(\sin\)</span> and <span class="math inline">\(\cos\)</span> function is: <span class="math display">\[
\begin{split}
&amp; \sin (\theta + \Delta ) = \sin \theta \cos \Delta + \cos \theta \sin \Delta  \\
&amp; \cos(\theta + \Delta) = \cos\theta\cos\Delta - \sin\theta\sin\Delta
\end{split}
\]</span></p>
<p>We can view those as functions of a real variable <span class="math inline">\(t\)</span>, than, <span class="math inline">\(\sin t\)</span> and <span class="math inline">\(\cos t\)</span> are <strong>smooth oscillating waves</strong>. A general 1D wave can be written as: <span class="math display">\[
A \cos (\omega t + \phi)
\]</span> where: - <span class="math inline">\(A\)</span> is <strong>amplitude</strong>: max / min value - <span class="math inline">\(\omega\)</span> is <strong>frequency</strong>: how many cycles per unit time. - <span class="math inline">\(\phi\)</span> is <strong>phase shift</strong>: horizontal shift</p>
<p>Here is the plot with different value of <span class="math inline">\(\omega\)</span>:</p>
<div id="fig-sin-cos-value-of-diff-omega" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sin-cos-value-of-diff-omega-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="assets/Position-Embedding-sin-cos-omega.png" id="fig-sin-cos-value-of-diff-omega" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-sin-cos-value-of-diff-omega-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1
</figcaption>
</figure>
</div>
<p>As we can see in the <a href="#fig-sin-cos-value-of-diff-omega" class="quarto-xref">Figure&nbsp;1</a>, with larger values of <span class="math inline">\(\omega\)</span>, the <span class="math inline">\(\cos\)</span> function oscillates more rapidly, while smaller values of <span class="math inline">\(\omega\)</span> produce slower, less frequent oscillations.</p>
<div class="sourceCode" id="cb1" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-5"><a href="#cb1-5"></a></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co"># Prepare data</span></span>
<span id="cb1-7"><a href="#cb1-7"></a>t <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">2000</span>)</span>
<span id="cb1-8"><a href="#cb1-8"></a>omegas <span class="op">=</span> [<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">2</span>]</span>
<span id="cb1-9"><a href="#cb1-9"></a></span>
<span id="cb1-10"><a href="#cb1-10"></a>data <span class="op">=</span> []</span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="cf">for</span> w <span class="kw">in</span> omegas:</span>
<span id="cb1-12"><a href="#cb1-12"></a>    data.append(pd.DataFrame({<span class="st">"t"</span>: t, <span class="st">"value"</span>: np.cos(w <span class="op">*</span> t), <span class="st">"omega"</span>: <span class="bu">str</span>(w)}))</span>
<span id="cb1-13"><a href="#cb1-13"></a></span>
<span id="cb1-14"><a href="#cb1-14"></a>df <span class="op">=</span> pd.concat(data)</span>
<span id="cb1-15"><a href="#cb1-15"></a></span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="co"># Plot</span></span>
<span id="cb1-17"><a href="#cb1-17"></a>sns.set_theme(style<span class="op">=</span><span class="st">"whitegrid"</span>)</span>
<span id="cb1-18"><a href="#cb1-18"></a></span>
<span id="cb1-19"><a href="#cb1-19"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb1-20"><a href="#cb1-20"></a>sns.lineplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">"t"</span>, y<span class="op">=</span><span class="st">"value"</span>, hue<span class="op">=</span><span class="st">"omega"</span>)</span>
<span id="cb1-21"><a href="#cb1-21"></a></span>
<span id="cb1-22"><a href="#cb1-22"></a>plt.title(<span class="st">"cos(ω t) for different ω"</span>)</span>
<span id="cb1-23"><a href="#cb1-23"></a>plt.xlabel(<span class="st">"t"</span>)</span>
<span id="cb1-24"><a href="#cb1-24"></a>plt.ylabel(<span class="st">"cos(ω t)"</span>)</span>
<span id="cb1-25"><a href="#cb1-25"></a>plt.tight_layout()</span>
<span id="cb1-26"><a href="#cb1-26"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="rotation-matrix-and-complex-exponential" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="rotation-matrix-and-complex-exponential"><span class="header-section-number">1.2</span> Rotation Matrix and Complex Exponential</h2>
<p>The other usage of the <span class="math inline">\(\sin\)</span> and <span class="math inline">\(\cos\)</span> function is the in the <strong>rotation matrix</strong>. To rotate any 2D vector: <span class="math inline">\(\mathbf{v} = \begin{pmatrix}x  \\y \end{pmatrix}\)</span> by and angle <span class="math inline">\(\theta\)</span> (counterclockwise), we can multiply it be the <strong>rotation matrix</strong>: <span class="math display">\[
R(\theta) =
\begin{pmatrix}
\cos\theta &amp; -\sin\theta \\  
\sin\theta &amp; \cos\theta  \\
\end{pmatrix}
\]</span></p>
<p>So, the new vector <span class="math inline">\(\mathbf{v}'\)</span> be come: <span class="math display">\[
\mathbf{v}' = R(\theta) \mathbf{v} =
\begin{pmatrix}  
x\cos\theta - y\sin\theta \\
x\sin\theta + y\cos\theta  
\end{pmatrix}
\]</span> One of the good property of rotation matrix is that it is the <strong><a href="https://en.wikipedia.org/wiki/Orthogonal_matrix">orthonormal matrix</a></strong>(<span class="math inline">\(R^{\top}R = I\)</span>), which means the length of original vector will not change after rotate: <span class="math display">\[
\|Rv\|^2 = (Rv)^\top (Rv) = v^\top (R^\top R) v = v^\top I v = v^\top v = \|v\|^2
\]</span></p>
<p><img src="assets/Position-Embedding-rotate-matrix.png" class="img-fluid"></p>
<p>On the other hand, a rotation in 2D can also be represented by multiplication be a complex number on the unit circle. The Euler’s Formula is define as: <span class="math display">\[
e^{i\theta} = \cos \theta + i \sin \theta
\]</span> This complex number has: - magnitude 1 - angle <span class="math inline">\(\theta\)</span> By multiplying a complex number by <span class="math inline">\(e^{i \theta}\)</span> produces a rotation. For a point <span class="math inline">\((x, y)\)</span>, it can be represented as a complex number <span class="math inline">\(z = x + iy\)</span>. Rotate it by <span class="math inline">\(\theta\)</span> radians by multiplying <span class="math inline">\(z = e^{i\theta} z\)</span>. Expand using Euler’s formula, we get: <span class="math display">\[
z = (\cos \theta + i \sin \theta)(x + i y) = (x \cos \theta - y \sin \theta) + i(x \sin \theta + y \cos \theta)
\]</span> which is exact the same as the matrix mutiplication.</p>
<div class="sourceCode" id="cb2" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>v <span class="op">=</span> torch.tensor([<span class="fl">1.5</span>, <span class="fl">0.5</span>])  <span class="co"># original vector v = (x, y)</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>theta <span class="op">=</span> torch.tensor(np.pi <span class="op">/</span> <span class="dv">2</span>)  <span class="co"># rotation angle in radians (e.g</span></span>
<span id="cb2-3"><a href="#cb2-3"></a></span>
<span id="cb2-4"><a href="#cb2-4"></a>z <span class="op">=</span> torch.view_as_complex(v.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb2-5"><a href="#cb2-5"></a>rot <span class="op">=</span> torch.exp(<span class="ot">1j</span> <span class="op">*</span> theta)  <span class="co"># e^{iθ} j = \sqrt{-1}.</span></span>
<span id="cb2-6"><a href="#cb2-6"></a></span>
<span id="cb2-7"><a href="#cb2-7"></a>z_rot <span class="op">=</span> rot <span class="op">*</span> z  <span class="co"># rotated complex number</span></span>
<span id="cb2-8"><a href="#cb2-8"></a>v_rot <span class="op">=</span> torch.view_as_real(z_rot).view(<span class="op">-</span><span class="dv">1</span>)  <span class="co"># rotated vector v' = R(θ) v</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We mentioned that a phase shift of <span class="math inline">\(\Delta\)</span> in the previous. One of important observation is that it is equivalent to applying a 2D rotation on the vector <span class="math inline">\(\begin{pmatrix} \cos \theta \\ \sin  \theta\end{pmatrix}\)</span>: <span class="math display">\[
\begin{split}
\begin{pmatrix} \cos(\theta+\Delta) \\ \sin(\theta+\Delta) \end{pmatrix}
&amp; = \begin{pmatrix}
\cos\theta\cos\Delta - \sin\theta\sin\Delta   \\
\sin \theta \cos \Delta + \cos \theta \sin \Delta
\end{pmatrix} \\
&amp; = \begin{pmatrix} \cos\Delta &amp; -\sin\Delta \\ \sin\Delta &amp; \cos\Delta \end{pmatrix} \begin{pmatrix} \cos\theta \\ \sin\theta \end{pmatrix}  \\
&amp; = R(\Delta) \begin{pmatrix} \cos \theta \\ \sin  \theta\end{pmatrix}
\end{split}
\]</span></p>
</section>
<section id="attention-in-transformer" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="attention-in-transformer"><span class="header-section-number">1.3</span> Attention in Transformer</h2>
<p>Attention mechanism is the most important component in the transformer <span class="citation" data-cites="AttentionAllYou2023vaswani">(<a href="#ref-AttentionAllYou2023vaswani" role="doc-biblioref">Vaswani et al. 2023</a>)</span>. It process sequences by employ three learnable weight matrices <span class="math inline">\(W_{Q}, W_{K}, W_{V}  \in \mathbb{R}^{d \times d_{\text{model}}}\)</span>. where <span class="math inline">\(d_{\text{model}}\)</span> represents the dimensionality of the projected subspaces. The matrices transform the input <span class="math inline">\(\mathrm{x}\)</span> into queries, keys, and values respectively: <span class="math display">\[
Q = XW_{Q}, \quad  K = XW_{K}, \quad  V = XW_{V}
\]</span> And the attention matrix is computed using the following formula: <span class="math display">\[
\begin{split}
\text{Attenion}(Q, K) &amp;= \text{softmax} \left( \frac{QK^{\top}}{\sqrt{ d_{\text{model}} }} \right) \\
Z &amp;= \text{Attenion}(Q, K) V
\end{split}
\]</span></p>
<p>As we can see, on</p>
<hr>
<p>We have review some concepts, now we are going to learned different types of the position encodings. The position encoding can be seperated into three different types:</p>
<ul>
<li>Absolute Positional Encoding
<ul>
<li>Learned Positional Encoding</li>
</ul></li>
<li>Relative Positional Encoding</li>
<li>RoPE</li>
</ul>
<p>Let’s dig into one by one.</p>
</section>
</section>
<section id="absolute-positional-encoding" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Absolute Positional Encoding</h1>
<p>The absolute positional encoding was first introduced in the original Transformer paper <span class="citation" data-cites="AttentionAllYou2023vaswani">(<a href="#ref-AttentionAllYou2023vaswani" role="doc-biblioref">Vaswani et al. 2023</a>)</span>. It defined as:</p>
<p><span class="math display">\[
\begin{split}
PE_{(pos, 2i)} &amp;=  \sin \left( \frac{pos}{10,000^{2i / d_{\text{model}}}} \right) \\
PE_{(pos, 2i + 1)} &amp;=  \cos \left( \frac{pos}{10,000^{2i / d_{\text{model}}}} \right) \\
\end{split}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(pos\)</span> is the position id (0, 1, 2..n)</li>
<li><span class="math inline">\(i\)</span> is the half of the dimension.</li>
</ul>
<p>So for the <span class="math inline">\(\mathbf{x}\)</span> is the in the position <span class="math inline">\(p\)</span>, it has position embedding vector: <span class="math display">\[
\mathrm{x}_{p}  = \begin{pmatrix}
\sin \left( \frac{p}{10,000^{\textcolor{red}{0} / d_{\text{model}}}} \right) \\
\cos \left( \frac{p}{10,000^{\textcolor{red}{0} / d_{\text{model}}}} \right) \\
\sin \left( \frac{p}{10,000^{\textcolor{red}{2} / d_{\text{model}}}} \right) \\
\cos \left( \frac{p}{10,000^{\textcolor{red}{2} / d_{\text{model}}}} \right)  \\\
\vdots \\
\sin \left( \frac{p}{10,000^{\textcolor{red}{i / 2} / d_{\text{model}}}} \right) \\
\cos \left( \frac{p}{10,000^{\textcolor{red}{i / 2} / d_{\text{model}}}} \right)
\end{pmatrix}
\]</span></p>
<p>As we can see, each pair of dimensions<span class="math inline">\((2i, 2i+1)\)</span> is one sinusoidal wavelength. The <span class="math inline">\(\omega\)</span> range from 1 to 10,000 with dimension <span class="math inline">\(i\)</span> increase. As in the <a href="#fig-sin-cos-value-of-diff-omega" class="quarto-xref">Figure&nbsp;1</a>, the lower <span class="math inline">\(\omega\)</span> the frequence range less, which mean the wavelength, vice verse. Ok, good, this sound nice and fancy, but why this is useful in the Transformer? We known that with small <span class="math inline">\(\omega\)</span>, we have long wavelength, which mean is change slow, only with huge different <span class="math inline">\(pos\)</span>, we can see the different between position. This is good to capture the global position information. On the other hand, with large <span class="math inline">\(\omega\)</span>, is change rapidly, with little change of the position, we can see the different, this is good for the local structure.</p>
<div class="callout-tldr">
<p><tag style="color:red">TAKEAWAY</tag>: Different frequency capture different level of the structure</p>
<ul>
<li><strong>Low frequency (small <span class="math inline">\(\omega\)</span>)</strong> → long wavelength → changes slowly → captures <em>global</em> structure</li>
<li><strong>High frequency (large <span class="math inline">\(\omega\)</span>)</strong> → short wavelength → changes rapidly → captures <em>local</em> structure</li>
</ul>
</div>
<p>To implement this, it is also easy:</p>
<div class="sourceCode" id="cb3" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">def</span> sinusoidal_positional_encoding(max_len, d_model):</span>
<span id="cb3-2"><a href="#cb3-2"></a>    pe <span class="op">=</span> torch.zeros(max_len, d_model)</span>
<span id="cb3-3"><a href="#cb3-3"></a></span>
<span id="cb3-4"><a href="#cb3-4"></a>    positions <span class="op">=</span> torch.arange(<span class="dv">0</span>, max_len).unsqueeze(<span class="op">-</span><span class="dv">1</span>)  <span class="co">#  shape (max_len, 1)</span></span>
<span id="cb3-5"><a href="#cb3-5"></a>    div_terms <span class="op">=</span> torch.exp(</span>
<span id="cb3-6"><a href="#cb3-6"></a>    torch.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>) <span class="op">*</span> <span class="op">-</span>(np.log(<span class="fl">10000.0</span>) <span class="op">/</span> d_model)</span>
<span id="cb3-7"><a href="#cb3-7"></a>    )  <span class="co"># shape (d_model/2,)</span></span>
<span id="cb3-8"><a href="#cb3-8"></a></span>
<span id="cb3-9"><a href="#cb3-9"></a>    pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(positions <span class="op">*</span> div_terms)  <span class="co"># even indices</span></span>
<span id="cb3-10"><a href="#cb3-10"></a>    pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(positions <span class="op">*</span> div_terms)  <span class="co"># odd indices</span></span>
<span id="cb3-11"><a href="#cb3-11"></a>    <span class="cf">return</span> pe</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>One of the small trick we used in the <code>div_terms</code> <span class="math inline">\(10,000^{-2i / d_{\text{model}}}\)</span> it produce like <span class="math display">\[
\left[ 10000^{0/d_{\text{model}}},\ 10000^{-2/d_{\text{model}}},\ 10000^{-4/d_{\text{model}}},\ \ldots \right]
\]</span> The <span class="math inline">\(a^{x} = e^{x \cdot \log a}\)</span>, so we can express <span class="math inline">\(10,000^{-2i / d_{\text{model}}}\)</span> as <span class="math inline">\(e^{-2i / d_{\text{model}} \cdot 10,000}\)</span>. This is one way to prevent the numerical instablity.</p>
<p>The other good property of this position encoding is that:</p>
<blockquote class="blockquote">
<p>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of P Epos. <cite> Attention Is All You Need, p.6 </cite></p>
</blockquote>
<p>Let’s prove that: We can see, that for <span class="math inline">\(PE_{pos}\)</span> and <span class="math inline">\(PE_{pos + k}\)</span>, we have: <span class="math display">\[
PE_{pos + k} =
\begin{pmatrix}
\sin \left( \frac{pos + k}{10,000^{\textcolor{red}{0} / d_{\text{model}}}} \right) \\
\cos \left( \frac{pos + k}{10,000^{\textcolor{red}{0} / d_{\text{model}}}} \right) \\
\sin \left( \frac{pos + k}{10,000^{\textcolor{red}{2} / d_{\text{model}}}} \right) \\
\cos \left( \frac{pos + k}{10,000^{\textcolor{red}{2} / d_{\text{model}}}} \right)  \\\
\vdots \\
\sin \left( \frac{pos + k}{10,000^{\textcolor{red}{2i} / d_{\text{model}}}} \right) \\
\cos \left( \frac{pos + k}{10,000^{\textcolor{red}{2i} / d_{\text{model}}}} \right)
\end{pmatrix}
\]</span></p>
<p>As we can see, for each pair of the <span class="math inline">\((2i, 2i + 1)\)</span>, we have position encoding <span class="math inline">\(\begin{pmatrix} \sin \left( \frac{pos + k}{10,000^{\textcolor{red}{2i} / d_{\text{model}}}} \right) \\ \cos \left( \frac{pos + k}{10,000^{\textcolor{red}{2i} / d_{\text{model}}}} \right) \end{pmatrix}\)</span>, this is same as the , Stack all pairs together, we get: <span class="math display">\[
PE_{pos+k} = \begin{pmatrix} R_0(k) &amp; &amp; \\ &amp; R_1(k) &amp; \\ &amp; &amp; \ddots \\ \end{pmatrix} PE_{pos}
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="Sinsuiod function in Diffusion Model">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Sinsuiod function in Diffusion Model
</div>
</div>
<div class="callout-body-container callout-body">
<p>This function is also used in the Diffusion Model, to encoding the time <span class="math inline">\(t\)</span>. In the Diffusion Model, we have <span class="math inline">\(t\)</span> to tell network in which we are currently in. For more details about diffusion model, go to visited this <a href="https://yyzhang2025.github.io/posts/Blogs/Diffusion-Models/Diffusion-Model.html">blog</a>.</p>
</div>
</div>
<section id="learned-position-encoding" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="learned-position-encoding"><span class="header-section-number">2.1</span> Learned Position Encoding</h2>
<p>Instead give each position a pre-defined position embedding vector, we can let neural network to learned the information. After Transformer was proposed, several work such as BERT <span class="citation" data-cites="BERTPretrainingDeep2019devlin">(<a href="#ref-BERTPretrainingDeep2019devlin" role="doc-biblioref">Devlin et al. 2019</a>)</span> and GPT. <span class="citation" data-cites="ImprovingLanguageUnderstandingradford">(<a href="#ref-ImprovingLanguageUnderstandingradford" role="doc-biblioref">Radford et al., n.d.</a>)</span> One of the most outstanding is the position encoding in the Vision Transformer <span class="citation" data-cites="ImageWorth16x162021dosovitskiy">(<a href="#ref-ImageWorth16x162021dosovitskiy" role="doc-biblioref">Dosovitskiy et al. 2021</a>)</span>.</p>
<p>Each patch in the image has a position id according to the raser order. Than the index is feed into MLP to project it into the position embedding space:</p>
<p><span class="math display">\[
PE(p) = MLP(p)
\]</span></p>
<p>Compare to the sine,, this is math simpler to implement:</p>
<div class="sourceCode" id="cb4" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="kw">class</span> LearnedPositionEncoding(nn.Module):</span>
<span id="cb4-2"><a href="#cb4-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, max_len, d_model):</span>
<span id="cb4-3"><a href="#cb4-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-4"><a href="#cb4-4"></a>        <span class="va">self</span>.position_embeddings <span class="op">=</span> nn.Embedding(max_len, d_model)</span>
<span id="cb4-5"><a href="#cb4-5"></a></span>
<span id="cb4-6"><a href="#cb4-6"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, positions):</span>
<span id="cb4-7"><a href="#cb4-7"></a>        <span class="cf">return</span> <span class="va">self</span>.position_embeddings(positions)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Just 2 lines of the code.</p>
<p>However, for the image, we known that it has 2D position, the x-axis and y-axis. So, with out flatten the patches, we can use the coordinate to represented the position encoding</p>
</section>
<section id="d-positional-encoding" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="d-positional-encoding"><span class="header-section-number">2.2</span> 2D Positional Encoding</h2>
<p>Some model such as Diffusion Transformer <span class="citation" data-cites="ScalableDiffusionModels2023peebles">(<a href="#ref-ScalableDiffusionModels2023peebles" role="doc-biblioref">Peebles and Xie 2023</a>)</span>, Masked Autoencoder <span class="citation" data-cites="MaskedAutoencodersAre2021he">(<a href="#ref-MaskedAutoencodersAre2021he" role="doc-biblioref">He et al. 2021</a>)</span> are use 2D sincos encoding as the position embedding.</p>
<p>The code implementation is as following:</p>
<div class="sourceCode" id="cb5" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="im">import</span> torch</span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb5-3"><a href="#cb5-3"></a></span>
<span id="cb5-4"><a href="#cb5-4"></a></span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="kw">class</span> PosED(nn.Module):</span>
<span id="cb5-6"><a href="#cb5-6"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim, gird_zie):</span>
<span id="cb5-7"><a href="#cb5-7"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-8"><a href="#cb5-8"></a></span>
<span id="cb5-9"><a href="#cb5-9"></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim</span>
<span id="cb5-10"><a href="#cb5-10"></a>        <span class="va">self</span>.grid_size <span class="op">=</span> grid_size</span>
<span id="cb5-11"><a href="#cb5-11"></a></span>
<span id="cb5-12"><a href="#cb5-12"></a>        <span class="va">self</span>.pos_embed <span class="op">=</span> <span class="va">self</span>.build_2d_sincos_pos_embed(</span>
<span id="cb5-13"><a href="#cb5-13"></a>            <span class="va">self</span>.embed_dim, <span class="va">self</span>.grid_size, cls_token<span class="op">=</span><span class="va">False</span>, extra_tokens<span class="op">=</span><span class="dv">0</span></span>
<span id="cb5-14"><a href="#cb5-14"></a>        )</span>
<span id="cb5-15"><a href="#cb5-15"></a></span>
<span id="cb5-16"><a href="#cb5-16"></a>    <span class="kw">def</span> build_2d_sincos_pos_embed(<span class="va">self</span>, embed_dim, grid_size, cls_token, extra_tokens):</span>
<span id="cb5-17"><a href="#cb5-17"></a>        <span class="co"># --- 1. build 2D grid ---</span></span>
<span id="cb5-18"><a href="#cb5-18"></a>        h <span class="op">=</span> torch.arange(grid_size, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb5-19"><a href="#cb5-19"></a>        w <span class="op">=</span> torch.arange(grid_size, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb5-20"><a href="#cb5-20"></a>        w_grid, h_grid <span class="op">=</span> torch.meshgrid(w, h, indexing<span class="op">=</span><span class="st">"ij"</span>)  <span class="co"># match original numpy ordering: w first</span></span>
<span id="cb5-21"><a href="#cb5-21"></a>        grid <span class="op">=</span> torch.stack([w_grid, h_grid], dim<span class="op">=</span><span class="dv">0</span>)  <span class="co"># (2, H, W)</span></span>
<span id="cb5-22"><a href="#cb5-22"></a></span>
<span id="cb5-23"><a href="#cb5-23"></a>        <span class="co"># reshape to (2, H*W)</span></span>
<span id="cb5-24"><a href="#cb5-24"></a>        grid <span class="op">=</span> grid.reshape(<span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb5-25"><a href="#cb5-25"></a></span>
<span id="cb5-26"><a href="#cb5-26"></a>        <span class="co"># --- 2. build 2D sin-cos embeddings ---</span></span>
<span id="cb5-27"><a href="#cb5-27"></a>        emb_h <span class="op">=</span> <span class="va">self</span>.build_1d_sincos(embed_dim <span class="op">//</span> <span class="dv">2</span>, grid[<span class="dv">0</span>])</span>
<span id="cb5-28"><a href="#cb5-28"></a>        emb_w <span class="op">=</span> <span class="va">self</span>.build_1d_sincos(embed_dim <span class="op">//</span> <span class="dv">2</span>, grid[<span class="dv">1</span>])</span>
<span id="cb5-29"><a href="#cb5-29"></a>        pos_embed <span class="op">=</span> torch.cat([emb_h, emb_w], dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># (H*W, D)</span></span>
<span id="cb5-30"><a href="#cb5-30"></a></span>
<span id="cb5-31"><a href="#cb5-31"></a>        <span class="co"># prepend extra tokens (e.g., cls token)</span></span>
<span id="cb5-32"><a href="#cb5-32"></a>        <span class="cf">if</span> cls_token <span class="kw">and</span> extra_tokens <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb5-33"><a href="#cb5-33"></a>            extra <span class="op">=</span> torch.zeros(extra_tokens, embed_dim)</span>
<span id="cb5-34"><a href="#cb5-34"></a>            pos_embed <span class="op">=</span> torch.cat([extra, pos_embed], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-35"><a href="#cb5-35"></a></span>
<span id="cb5-36"><a href="#cb5-36"></a>        <span class="cf">return</span> pos_embed</span>
<span id="cb5-37"><a href="#cb5-37"></a></span>
<span id="cb5-38"><a href="#cb5-38"></a>    <span class="kw">def</span> build_1d_sincos(<span class="va">self</span>, embed_dim, pos):</span>
<span id="cb5-39"><a href="#cb5-39"></a>        <span class="co">"""</span></span>
<span id="cb5-40"><a href="#cb5-40"></a><span class="co">        embed_dim: D/2</span></span>
<span id="cb5-41"><a href="#cb5-41"></a><span class="co">        pos: (M,)</span></span>
<span id="cb5-42"><a href="#cb5-42"></a><span class="co">        returns: (M, D/2)</span></span>
<span id="cb5-43"><a href="#cb5-43"></a><span class="co">        """</span></span>
<span id="cb5-44"><a href="#cb5-44"></a>        <span class="cf">assert</span> embed_dim <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb5-45"><a href="#cb5-45"></a></span>
<span id="cb5-46"><a href="#cb5-46"></a>        omega <span class="op">=</span> torch.arange(embed_dim <span class="op">//</span> <span class="dv">2</span>, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb5-47"><a href="#cb5-47"></a>        omega <span class="op">=</span> omega <span class="op">/</span> (embed_dim <span class="op">/</span> <span class="fl">2.0</span>)</span>
<span id="cb5-48"><a href="#cb5-48"></a>        omega <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> (<span class="dv">10000</span><span class="op">**</span>omega)</span>
<span id="cb5-49"><a href="#cb5-49"></a></span>
<span id="cb5-50"><a href="#cb5-50"></a>        <span class="co"># outer product</span></span>
<span id="cb5-51"><a href="#cb5-51"></a>        out <span class="op">=</span> pos[:, <span class="va">None</span>] <span class="op">*</span> omega[<span class="va">None</span>, :]  <span class="co"># (M, D/2)</span></span>
<span id="cb5-52"><a href="#cb5-52"></a></span>
<span id="cb5-53"><a href="#cb5-53"></a>        emb <span class="op">=</span> torch.cat([torch.sin(out), torch.cos(out)], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-54"><a href="#cb5-54"></a>        <span class="cf">return</span> emb</span>
<span id="cb5-55"><a href="#cb5-55"></a></span>
<span id="cb5-56"><a href="#cb5-56"></a>    <span class="kw">def</span> forward(<span class="va">self</span>):</span>
<span id="cb5-57"><a href="#cb5-57"></a>        <span class="cf">return</span> <span class="va">self</span>.pos_embed</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For the image 2D position encoding is more natural than the 1D-sequence position encoding. For example, a 256×256 image split into 16×16 patches → 16×16 = 256 patches.</p>
<p>The <em>actual spatial positions</em> of these patches look like this:</p>
<div class="sourceCode" id="cb6" data-code-line-numbers=""><pre class="sourceCode numberSource text number-lines code-with-copy"><code class="sourceCode"><span id="cb6-1"><a href="#cb6-1"></a>(0,0) (0,1) (0,2) ... (0,15)</span>
<span id="cb6-2"><a href="#cb6-2"></a>(1,0) (1,1) (1,2) ... (1,15)</span>
<span id="cb6-3"><a href="#cb6-3"></a>...</span>
<span id="cb6-4"><a href="#cb6-4"></a>(15,0)...          (15,15)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If you use <strong>1-D sinusoidal positional encoding</strong>, the sequence becomes:</p>
<div class="sourceCode" id="cb7" data-code-line-numbers=""><pre class="sourceCode numberSource text number-lines code-with-copy"><code class="sourceCode"><span id="cb7-1"><a href="#cb7-1"></a>token0, token1, token2, ...</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This causes the model to incorrectly assume: • token1 is closer to token2 • token1 is farther from token17</p>
<p>But in the real 2-D image space: • token1 and token2 (left–right neighbors) are close • token1 and token17 (the patch directly below) are also close</p>
</section>
</section>
<section id="relative-positional-encoding" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Relative Positional Encoding</h1>
<p>Absolute Positional Encoding is intuitive, but is this the best choice? Let’s consider a example, one sentence:</p>
<div class="sourceCode" id="cb8" data-code-line-numbers=""><pre class="sourceCode numberSource text number-lines code-with-copy"><code class="sourceCode"><span id="cb8-1"><a href="#cb8-1"></a>This cat is cute.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If the sentence shifts, 1 tokens to the right, become:</p>
<div class="sourceCode" id="cb9" data-code-line-numbers=""><pre class="sourceCode numberSource text number-lines code-with-copy"><code class="sourceCode"><span id="cb9-1"><a href="#cb9-1"></a>Wow, This cat is cute.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The semantics don’t change, but Absolute PE forces the model to re-learn this for every shift, it waste parameters and hurts generralization. On the other hand, the Relative Positional Encoding just encode the Relative position between tokens, if the sentence shift, the position encoding will not change.</p>
<div class="sourceCode" id="cb10" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="im">import</span> torch</span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb10-3"><a href="#cb10-3"></a></span>
<span id="cb10-4"><a href="#cb10-4"></a><span class="kw">class</span> RelativePositionBias(nn.Module):</span>
<span id="cb10-5"><a href="#cb10-5"></a>    <span class="co">"""</span></span>
<span id="cb10-6"><a href="#cb10-6"></a><span class="co">    1D relative position bias (T5 / Swin / DiT style).</span></span>
<span id="cb10-7"><a href="#cb10-7"></a></span>
<span id="cb10-8"><a href="#cb10-8"></a><span class="co">    Returns a [n_heads, L_q, L_k] tensor that you can add to attention logits.</span></span>
<span id="cb10-9"><a href="#cb10-9"></a><span class="co">    """</span></span>
<span id="cb10-10"><a href="#cb10-10"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_heads: <span class="bu">int</span>, max_distance: <span class="bu">int</span>):</span>
<span id="cb10-11"><a href="#cb10-11"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-12"><a href="#cb10-12"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb10-13"><a href="#cb10-13"></a>        <span class="va">self</span>.max_distance <span class="op">=</span> max_distance</span>
<span id="cb10-14"><a href="#cb10-14"></a></span>
<span id="cb10-15"><a href="#cb10-15"></a>        <span class="co"># Distances in [-max_distance+1, max_distance-1] are each given an embedding.</span></span>
<span id="cb10-16"><a href="#cb10-16"></a>        <span class="co"># We store them in [0, 2*max_distance-1) via offset = distance + (max_distance - 1)</span></span>
<span id="cb10-17"><a href="#cb10-17"></a>        <span class="va">self</span>.rel_embeddings <span class="op">=</span> nn.Embedding(<span class="dv">2</span> <span class="op">*</span> max_distance <span class="op">-</span> <span class="dv">1</span>, num_heads)</span>
<span id="cb10-18"><a href="#cb10-18"></a></span>
<span id="cb10-19"><a href="#cb10-19"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, q_len: <span class="bu">int</span>, k_len: <span class="bu">int</span>):</span>
<span id="cb10-20"><a href="#cb10-20"></a>        <span class="co">"""</span></span>
<span id="cb10-21"><a href="#cb10-21"></a><span class="co">        Args:</span></span>
<span id="cb10-22"><a href="#cb10-22"></a><span class="co">            q_len: length of query sequence (L_q)</span></span>
<span id="cb10-23"><a href="#cb10-23"></a><span class="co">            k_len: length of key sequence (L_k)</span></span>
<span id="cb10-24"><a href="#cb10-24"></a></span>
<span id="cb10-25"><a href="#cb10-25"></a><span class="co">        Returns:</span></span>
<span id="cb10-26"><a href="#cb10-26"></a><span class="co">            rel_pos_bias: [n_heads, L_q, L_k]</span></span>
<span id="cb10-27"><a href="#cb10-27"></a><span class="co">        """</span></span>
<span id="cb10-28"><a href="#cb10-28"></a>        <span class="co"># positions: [L_q, L_k]</span></span>
<span id="cb10-29"><a href="#cb10-29"></a>        q_positions <span class="op">=</span> torch.arange(q_len, dtype<span class="op">=</span>torch.<span class="bu">long</span>)[:, <span class="va">None</span>]    <span class="co"># [L_q, 1]</span></span>
<span id="cb10-30"><a href="#cb10-30"></a>        k_positions <span class="op">=</span> torch.arange(k_len, dtype<span class="op">=</span>torch.<span class="bu">long</span>)[<span class="va">None</span>, :]    <span class="co"># [1, L_k]</span></span>
<span id="cb10-31"><a href="#cb10-31"></a>        rel_pos <span class="op">=</span> k_positions <span class="op">-</span> q_positions                             <span class="co"># [L_q, L_k], values in [-(L_q-1), L_k-1]</span></span>
<span id="cb10-32"><a href="#cb10-32"></a></span>
<span id="cb10-33"><a href="#cb10-33"></a>        <span class="co"># clip to [-max_distance+1, max_distance-1]</span></span>
<span id="cb10-34"><a href="#cb10-34"></a>        max_dist <span class="op">=</span> <span class="va">self</span>.max_distance <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb10-35"><a href="#cb10-35"></a>        rel_pos_clipped <span class="op">=</span> torch.clamp(rel_pos, <span class="op">-</span>max_dist, max_dist)</span>
<span id="cb10-36"><a href="#cb10-36"></a></span>
<span id="cb10-37"><a href="#cb10-37"></a>        <span class="co"># shift to [0, 2*max_distance-2]</span></span>
<span id="cb10-38"><a href="#cb10-38"></a>        rel_pos_indices <span class="op">=</span> rel_pos_clipped <span class="op">+</span> max_dist</span>
<span id="cb10-39"><a href="#cb10-39"></a></span>
<span id="cb10-40"><a href="#cb10-40"></a>        <span class="co"># lookup embeddings → [L_q, L_k, n_heads]</span></span>
<span id="cb10-41"><a href="#cb10-41"></a>        values <span class="op">=</span> <span class="va">self</span>.rel_embeddings(rel_pos_indices)  <span class="co"># last dim = num_heads</span></span>
<span id="cb10-42"><a href="#cb10-42"></a></span>
<span id="cb10-43"><a href="#cb10-43"></a>        <span class="co"># permute to [n_heads, L_q, L_k]</span></span>
<span id="cb10-44"><a href="#cb10-44"></a>        <span class="cf">return</span> values.permute(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Some Vision Transformer such as Swin Transformer also use <span class="citation" data-cites="SwinTransformerHierarchical2021liu">(<a href="#ref-SwinTransformerHierarchical2021liu" role="doc-biblioref">Liu et al. 2021</a>)</span></p>
</section>
<section id="rope" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> RoPE</h1>
<p>First proposed in the <span class="citation" data-cites="RoFormerEnhancedTransformer2023su">(<a href="#ref-RoFormerEnhancedTransformer2023su" role="doc-biblioref">Su et al. 2023</a>)</span></p>
<section id="d-rope" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="d-rope"><span class="header-section-number">4.1</span> 2D-RoPE</h2>
</section>
<section id="m-rope" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="m-rope"><span class="header-section-number">4.2</span> M-RoPE</h2>
</section>
</section>
<section id="summary" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Summary</h1>
<table class="table">
<colgroup>
<col style="width: 9%">
<col style="width: 29%">
<col style="width: 39%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Type</strong></th>
<th><strong>Category</strong></th>
<th><strong>Mechanism</strong></th>
<th><strong>Models</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sinusoidal</td>
<td>Absolute</td>
<td>deterministic sin/cos</td>
<td>Original Transformer</td>
</tr>
<tr class="even">
<td>Learned PE</td>
<td>Absolute</td>
<td>learned table</td>
<td>BERT, ViT</td>
</tr>
<tr class="odd">
<td>Shaw RPE</td>
<td>Relative</td>
<td>learned embedding per distance</td>
<td>Transformer-XL</td>
</tr>
<tr class="even">
<td>T5 RPB</td>
<td>Relative bias</td>
<td>learned head-wise bias</td>
<td>T5, Swin, DiT</td>
</tr>
<tr class="odd">
<td><strong>ALiBi</strong></td>
<td><strong>Relative bias (non-learned)</strong></td>
<td><strong>linear distance slope added to logits</strong></td>
<td>GPT-NeoX, Pythia</td>
</tr>
<tr class="even">
<td>RoPE</td>
<td>Relative (implicit)</td>
<td>rotate Q/K by position</td>
<td>GPT-J, LLaMA, DeepSeek</td>
</tr>
</tbody>
</table>



</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-BERTPretrainingDeep2019devlin" class="csl-entry" role="listitem">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. <span>“<span>BERT</span>: <span class="nocase">Pre-training</span> of <span>Deep Bidirectional Transformers</span> for <span>Language Understanding</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1810.04805">https://doi.org/10.48550/arXiv.1810.04805</a>.
</div>
<div id="ref-ImageWorth16x162021dosovitskiy" class="csl-entry" role="listitem">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. <span>“An <span>Image</span> Is <span>Worth</span> 16x16 <span>Words</span>: <span>Transformers</span> for <span>Image Recognition</span> at <span>Scale</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2010.11929">https://doi.org/10.48550/arXiv.2010.11929</a>.
</div>
<div id="ref-MaskedAutoencodersAre2021he" class="csl-entry" role="listitem">
He, Kaiming, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. 2021. <span>“Masked <span>Autoencoders Are Scalable Vision Learners</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2111.06377">https://doi.org/10.48550/arXiv.2111.06377</a>.
</div>
<div id="ref-SwinTransformerHierarchical2021liu" class="csl-entry" role="listitem">
Liu, Ze, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. <span>“Swin <span>Transformer</span>: <span>Hierarchical Vision Transformer</span> Using <span>Shifted Windows</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2103.14030">https://doi.org/10.48550/arXiv.2103.14030</a>.
</div>
<div id="ref-ScalableDiffusionModels2023peebles" class="csl-entry" role="listitem">
Peebles, William, and Saining Xie. 2023. <span>“Scalable <span>Diffusion Models</span> with <span>Transformers</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2212.09748">https://doi.org/10.48550/arXiv.2212.09748</a>.
</div>
<div id="ref-ImprovingLanguageUnderstandingradford" class="csl-entry" role="listitem">
Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. n.d. <span>“Improving <span>Language Understanding</span> by <span>Generative Pre-Training</span>.”</span>
</div>
<div id="ref-RoFormerEnhancedTransformer2023su" class="csl-entry" role="listitem">
Su, Jianlin, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2023. <span>“<span>RoFormer</span>: <span>Enhanced Transformer</span> with <span>Rotary Position Embedding</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2104.09864">https://doi.org/10.48550/arXiv.2104.09864</a>.
</div>
<div id="ref-AttentionAllYou2023vaswani" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. <span>“Attention <span>Is All You Need</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1706.03762">https://doi.org/10.48550/arXiv.1706.03762</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© CC-By Yuyang, 2025</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This page is built with ❤️ and <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




<script src="../../../site_libs/quarto-contrib/line-highlight-1.0.0/line-highlight.js" defer="true"></script>
</body></html>